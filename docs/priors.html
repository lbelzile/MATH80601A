<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayesian modelling - 2&nbsp; Priors</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./introduction.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./priors.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Priors</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/math80601a/" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH80601A-bayesmod.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Bayesics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./priors.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#conjugate-priors" id="toc-conjugate-priors" class="nav-link active" data-scroll-target="#conjugate-priors"><span class="header-section-number">2.1</span> Conjugate priors</a></li>
  <li><a href="#uninformative-priors" id="toc-uninformative-priors" class="nav-link" data-scroll-target="#uninformative-priors"><span class="header-section-number">2.2</span> Uninformative priors</a></li>
  <li><a href="#jeffreys-prior-for-the-normal-distribution" id="toc-jeffreys-prior-for-the-normal-distribution" class="nav-link" data-scroll-target="#jeffreys-prior-for-the-normal-distribution"><span class="header-section-number">2.3</span> Jeffrey’s prior for the normal distribution</a></li>
  <li><a href="#prior-simulation" id="toc-prior-simulation" class="nav-link" data-scroll-target="#prior-simulation"><span class="header-section-number">2.4</span> Prior simulation</a></li>
  <li><a href="#informative-priors" id="toc-informative-priors" class="nav-link" data-scroll-target="#informative-priors"><span class="header-section-number">2.5</span> Informative priors</a></li>
  <li><a href="#priors-for-regression-models" id="toc-priors-for-regression-models" class="nav-link" data-scroll-target="#priors-for-regression-models"><span class="header-section-number">2.6</span> Priors for regression models</a></li>
  <li><a href="#penalized-complexity-priors" id="toc-penalized-complexity-priors" class="nav-link" data-scroll-target="#penalized-complexity-priors"><span class="header-section-number">2.7</span> Penalized complexity priors</a></li>
  <li><a href="#sensitivity-analysis" id="toc-sensitivity-analysis" class="nav-link" data-scroll-target="#sensitivity-analysis"><span class="header-section-number">2.8</span> Sensitivity analysis</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/lbelzile/math80601a/edit/main/priors.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
<h1 class="title display-7"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Priors</span></h1>
</header>

<p>The posterior distribution combines two ingredients: the likelihood and the prior. If the former is a standard ingredient of any likelihood-based inference, prior specification requires some care. The purpose of this chapter is to consider different standard way of constructing prior functions, and to specify the parameters of the latter: we term these hyperparameters.</p>
<section id="conjugate-priors" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="conjugate-priors"><span class="header-section-number">2.1</span> Conjugate priors</h2>
<p>In very simple models, there may exists prior densities that result in a posterior distribution of the same family. We can thus directly extract characteristics of the posterior. Conjugate priors are chosen for computational convenience and because interpretation is convenient, as the parameters of the posterior will often be some weighted average of prior and likelihood component.</p>
<div id="def-conjugate-prior" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 </strong></span>A prior density <span class="math inline">\(p(\boldsymbol{\theta})\)</span> is conjugate for likelihood <span class="math inline">\(L(\boldsymbol{\theta}; \boldsymbol{y})\)</span> if the product <span class="math inline">\(L(\boldsymbol{\theta}; \boldsymbol{y})p(\boldsymbol{\theta})\)</span>, after renormalization, is of the same parametric family as the prior.</p>
<p>Exponential families (including the binomial, Poisson, exponential, Gaussian distributions) admit conjugate priors<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</div>
<div id="exm-conjugatepriors-binom" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Conjugate prior for the binomial model) </strong></span>The binomial log density with <span class="math inline">\(y\)</span> successes out of <span class="math inline">\(n\)</span> trials is proportional to <span class="math display">\[\begin{align*}
y \log(p) + (n-y) \log(1-p) = y\log\left( \frac{p}{1-p}\right) + n \log(1-p)
\end{align*}\]</span> with canonical parameter <span class="math inline">\(\mathrm{logit}(p)\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The binomial distribution is thus an exponential family.</p>
<p>Since the density of the binomial is of the form <span class="math inline">\(p^y(1-p)^{n-y}\)</span>, the beta distribution <span class="math inline">\(\mathsf{Be}(\alpha, \beta)\)</span> with density <span class="math display">\[f(x) \propto x^{\alpha-1} (1-x)^{\beta-1}\]</span> is the conjugate prior.</p>
<p>The beta distribution is also the conjugate prior for the negative binomial, geometric and Bernoulli distributions, since their likelihoods are all proportional to that of the beta. The fact that different sampling schemes that result in proportional likelihood functions give the same inference is called likelihood principle.</p>
</div>
<div id="exm-conjugatepriors-poisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Conjugate prior for the Poisson model) </strong></span>The Poisson distribution with mean <span class="math inline">\(\mu\)</span> has log density proportional to <span class="math inline">\(f(y; \mu) \propto y\log(\mu) -\mu\)</span>, so is an exponential family with natural parameter <span class="math inline">\(\log(\mu)\)</span>. The gamma density, <span class="math display">\[ f(x) \propto \beta^{\alpha}/\Gamma(\alpha)x^{\alpha-1} \exp(-\beta x)\]</span> with shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span> is the conjugate prior for the Poisson. For an <span class="math inline">\(n\)</span>-sample of independent observations <span class="math inline">\(\mathsf{Po}(\mu)\)</span> observations with <span class="math inline">\(\mu \sim \mathsf{Ga}(\alpha, \beta)\)</span>, the posterior is <span class="math inline">\(\mathsf{Ga}(\sum_{i=1}^n y_i + \alpha, \beta + n)\)</span>.</p>
</div>
<p>Knowing the analytic expression for the posterior can be useful for calculations of the marginal likelihood, as <a href="#exm-poisson-negbin">Example&nbsp;<span>2.3</span></a> demonstrates.</p>
<div id="exm-poisson-negbin" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Negative binomial as a Poisson mixture) </strong></span>&nbsp;</p>
<!-- Some canonical distributions have only one parameter which dictates moments: for a Poisson random variable $Y \sim \mathsf{Po}(\lambda)$, both $\mathsf{E}(Y) = \mathsf{Va}(Y) = \lambda$, whereas if we have binomial data with $n$ independent trials, each with probability of success $p$, then $Y \sim \mathsf{Bin}(n, p)$, then the theoretical mean of successes is $\mathsf{E}(Y)=np$ and the variance $\mathsf{Va}(Y) = np(1-p)$. -->
<!-- In generalized linear models, we model the mean as a function of covariate, but data may exhibit overdispersion relative to the theoretical mean. -->
<p>One restriction of the Poisson model is that the restriction on its moments is often unrealistic. The most frequent problem encountered is that of <strong>overdispersion</strong>, meaning that the variability in the counts is larger than that implied by a Poisson distribution.</p>
<p>One common framework for handling overdispersion is to have <span class="math inline">\(Y \mid \Lambda = \lambda \sim \mathsf{Po}(\lambda)\)</span>, where the mean of the Poisson distribution is itself a positive random variable with mean <span class="math inline">\(\mu\)</span>, if <span class="math inline">\(\Lambda\)</span> follows a conjugate gamma distribution with shape <span class="math inline">\(k\mu\)</span> and rate <span class="math inline">\(k&gt;0\)</span>, <span class="math inline">\(\Lambda \sim \mathsf{Ga}(k\mu, k)\)</span>, the posterior <span class="math inline">\(\Lambda \mid Y=y \sim \mathsf{Ga}(k\mu + y, k+1)\)</span>.</p>
<p>Since the joint density of <span class="math inline">\(Y\)</span> and <span class="math inline">\(\Lambda\)</span> can be written <span class="math display">\[
p(y, \lambda) = p(y \mid \lambda)p(\lambda) = p(\lambda \mid y) p(y)
\]</span> we can isolate the marginal density <span class="math display">\[\begin{align*}
p(y) &amp;= \frac{p(y \mid \lambda)p(\lambda)}{p(\lambda \mid y)} \\&amp;= \frac{\frac{\lambda^y\exp(-\lambda)}{\Gamma(y+1)}  \frac{k^{k\mu}\lambda^{k\mu-1}\exp(-k\lambda)}{\Gamma(k\mu)}}{ \frac{(k+1)^{k\mu+y}\lambda^{k\mu+y-1}\exp\{-(k+1)\lambda\}}{\Gamma(k\mu+y)}}\\
&amp;= \frac{\Gamma(k\mu+y)}{\Gamma(k\mu)\Gamma(y+1)}k^{k\mu} (k+1)^{-k\mu-y}\\&amp;= \frac{\Gamma(k\mu+y)}{\Gamma(k\mu)\Gamma(y+1)}\left(1-\frac{1}{k+1}\right)^{k\mu} \left(\frac{1}{k+1}\right)^y
\end{align*}\]</span> and this is the density of a negative binomial distribution with probability of success <span class="math inline">\(1/(k+1)\)</span>. We can thus view the negative binomial as a Poisson mean mixture.</p>
<p>By the laws of iterated expectation and iterative variance, <span class="math display">\[\begin{align*}
\mathsf{E}(Y) &amp;= \mathsf{E}_{\Lambda}\{\mathsf{E}(Y \mid \Lambda\} \\&amp; = \mathsf{E}(\Lambda) = \mu\\
\mathsf{Va}(Y) &amp;= \mathsf{E}_{\Lambda}\{\mathsf{Va}(Y \mid \Lambda)\} + \mathsf{Va}_{\Lambda}\{\mathsf{E}(Y \mid \Lambda)\} \\&amp;= \mathsf{E}(\Lambda) + \mathsf{Va}(\Lambda) \\&amp;= \mu + \mu/k.
\end{align*}\]</span> The marginal distribution of <span class="math inline">\(Y\)</span>, unconditionally, has a variance which exceeds its mean, as <span class="math display">\[\begin{align*}
\mathsf{E}(Y) = \mu, \qquad \mathsf{Va}(Y) = \mu (1+1/k).
\end{align*}\]</span> In a negative binomial regression model, the term <span class="math inline">\(k\)</span> is a dispersion parameter, which is fixed for all observations, whereas <span class="math inline">\(\mu = \exp(\boldsymbol{\beta}\mathbf{X})\)</span> is a function of covariates <span class="math inline">\(\mathbf{X}\)</span>. As <span class="math inline">\(k \to \infty\)</span>, the distribution of <span class="math inline">\(\Lambda\)</span> degenerates to a constant at <span class="math inline">\(\mu\)</span> and we recover the Poisson model.</p>
</div>
<div id="exm-abtest" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (Posterior rates for A/B tests using conjugate Poisson model) </strong></span>Upworthy.com, a US media publisher, revolutionized headlines online advertisement by running systematic A/B tests to compare the different wording of headlines, placement and image and what catches attention the most. The Upworthy Research Archive <span class="citation" data-cites="Matias:2021">(<a href="references.html#ref-Matias:2021" role="doc-biblioref">Matias et al. 2021</a>)</span> contains results for 22743 experiments, with a click through rate of 1.58% on average and a standard deviation of 1.23%. The <code>clickability_test_id</code> gives the unique identifier of the experiment, <code>clicks</code> the number of conversion out of <code>impressions</code>. See <a href="https://tellingstorieswithdata.com/08-hunt.html#ab-testing">Section 8.5</a> of <span class="citation" data-cites="Alexander:2023">Alexander (<a href="references.html#ref-Alexander:2023" role="doc-biblioref">2023</a>)</span> for more details about A/B testing and background information.</p>
<p>Consider an A/B test from November 23st, 2014, that compared four different headlines for a story on Sesame Street workshop with interviews of children whose parents were in jail and visiting them in prisons. The headlines tested were:</p>
<blockquote class="blockquote">
<ol type="1">
<li>Some Don’t Like It When He Sees His Mom. But To Him? Pure Joy. Why Keep Her From Him?</li>
<li>They’re Not In Danger. They’re Right. See True Compassion From The Children Of The Incarcerated.</li>
<li>Kids Have No Place In Jail … But In This Case, They <em>Totally</em> Deserve It.</li>
<li>Going To Jail <em>Should</em> Be The Worst Part Of Their Life. It’s So Not. Not At All.</li>
</ol>
</blockquote>
<p>At first glance, the first and third headlines seem likely to lead to a curiosity gap. The wording of the second is more explicit (and searchable), whereas the first is worded as a question.</p>
<p>We model the conversion rate <span class="math inline">\(\lambda_i\)</span> for each headline separately using a Poisson distribution and compare the posterior distributions for all four choices. Using a conjugate prior and selecting the parameters by moment matching yields approximately <span class="math inline">\(\alpha = 1.64\)</span> and <span class="math inline">\(\beta = 0.01\)</span> for the hyperparameters.</p>
<div class="cell" data-hash="priors_cache/html/tbl-upworthy_cb5910745de87b8528f3b6847dd40abd">
<div class="cell-output-display">
<div id="tbl-upworthy" class="anchored">
<table class="table table-sm table-striped small">
<caption>Table&nbsp;2.1: Number of views, clicks for different headlines for the Upworthy data.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">headline</th>
<th style="text-align: right;">impressions</th>
<th style="text-align: right;">clicks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">H1</td>
<td style="text-align: right;">3060</td>
<td style="text-align: right;">49</td>
</tr>
<tr class="even">
<td style="text-align: left;">H2</td>
<td style="text-align: right;">2982</td>
<td style="text-align: right;">20</td>
</tr>
<tr class="odd">
<td style="text-align: left;">H3</td>
<td style="text-align: right;">3112</td>
<td style="text-align: right;">31</td>
</tr>
<tr class="even">
<td style="text-align: left;">H4</td>
<td style="text-align: right;">3083</td>
<td style="text-align: right;">9</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-hash="priors_cache/html/fig-upworthy_6251c57669da23d76aeb2e17423950af">
<div class="cell-output-display">
<div id="fig-upworthy" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="priors_files/figure-html/fig-upworthy-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.1: Gamma posterior for the Upworthy Sesame street headline.</figcaption>
</figure>
</div>
</div>
</div>
<p>We can visualize the posterior distributions. In this context, the large sample size lead to the dominance of the likelihood contribution <span class="math inline">\(p(Y_i \mid \lambda_i) \sim \mathsf{Po}(n_i\lambda_i)\)</span> relative to the prior. We can see there is virtually no overlap between different rates for headers H1 (preferred) relative to H4 (least favorable). The probability that the conversion rate for Headline 3 is higher than Headline 1 can be approximated by simulating samples from both posteriors and computing the proportion of times one is larger: we get 1.7% for <code>H3</code> relative to <code>H1</code>, indicating a clear preference for the first headline <code>H1</code>.</p>
</div>
<div id="exm-poisson-upworthy-question" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5 (Should you phrase your headline as a question?) </strong></span>We can also consider aggregate records for Upworthy, as <span class="citation" data-cites="Alexander:2023">Alexander (<a href="references.html#ref-Alexander:2023" role="doc-biblioref">2023</a>)</span> did. The <code>upworthy_question</code> database contains a balanced sample of all headlines where at least one of the choices featured a question, with at least one alternative statement. Whether a headline contains a question or not is determined by querying for the question mark. We consider aggregated counts for all such headlines, with the <code>question</code> factor encoding whether there was a question, <code>yes</code> or <code>no</code>. For simplicity, we treat the number of views as fixed, but keep in mind that A/B tests are often sequential experiments with a stopping rule.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>We model first the rates using a Poisson regression; the corresponding frequentist analysis would include an offset to account for differences in views. If <span class="math inline">\(\lambda_{j}\)</span> <span class="math inline">\((j=1, 2)\)</span> are the average rate for each factor level (yes and no), then <span class="math inline">\(\mathsf{E}(Y_{ij}/n_{ij}) = \lambda_j\)</span>. In the frequentist setting, we can fit a simple Poisson generalized linear regression model with an offset term and a binary variable.</p>
<div class="cell" data-hash="priors_cache/html/unnamed-chunk-4_13624caf2cfaf28bdf06708e4d50c08e">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(upworthy_question, <span class="at">package =</span> <span class="st">"hecbayes"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>poismod <span class="ot">&lt;-</span> <span class="fu">glm</span>(</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  clicks <span class="sc">~</span> <span class="fu">offset</span>(<span class="fu">log</span>(impressions)) <span class="sc">+</span> question, </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">poisson</span>(<span class="at">link =</span> <span class="st">"log"</span>),</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> upworthy_question)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(poismod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)  questionno 
-4.51264669  0.07069677 </code></pre>
</div>
</div>
<p>The coefficients represent the difference in log rate (multiplicative effect) relative to the baseline rate, with an increase of 6.3 percent when the headline does not contain a question. A likelihood ratio test can be performed by comparing the deviance of the null model (intercept-only), indicating strong evidence that including question leads to significatively different rates. This is rather unsurprising given the enormous sample sizes.</p>
<p>Consider instead a Bayesian analysis with conjugate prior: we model separately the rates of each group (question or not). Suppose we think apriori that the click-rate is on average 1%, with a standard deviation of 2%, with no difference between questions or not. This would translate, using moment matching, into a gamma prior distribution <span class="math inline">\(p(\lambda_j)\)</span> with rate <span class="math inline">\(\beta = 0.04 = \mathsf{Var}_0/\mathsf{E}_0\)</span> and shape <span class="math inline">\(\alpha = 2.5\)</span> (<span class="math inline">\(j=1, 2\)</span>). If <span class="math inline">\(\lambda_{j}\)</span> is the average rate for each factor level (yes and no), then <span class="math inline">\(\mathsf{E}(Y_{ij}/n_{ij}) = \lambda_j\)</span> so the log likelihood is proportional, as a function of <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>, to <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\lambda}; \boldsymbol{y}, \boldsymbol{n}) \stackrel{\boldsymbol{\lambda}}{\propto} \sum_{i=1}^n \sum_{j=1}^2 y_{ij}\log \lambda_j - \lambda_jn_{ij}
\end{align*}\]</span> and we can recognize that the posterior for <span class="math inline">\(\lambda_i\)</span> is gamma with shape <span class="math inline">\(\alpha + \sum_{i=1}^n y_{ij}\)</span> and rate <span class="math inline">\(\beta + \sum_{i=1}^n n_{ij}.\)</span> For inference, we thus only need to select hyperparameters and calculate the total number of clicks and impressions per group. We can then consider the posterior difference <span class="math inline">\(\lambda_1 - \lambda_2\)</span> or, to mimic the Poisson multiplicative model, of the ratio <span class="math inline">\(\lambda_1/\lambda_2\)</span>. The former suggests very small differences, but one must keep in mind that rates are also small. The ratio, shown in the right-hand panel of <a href="#fig-hist-difference_rates">Figure&nbsp;<span>2.2</span></a>, gives a more easily interpretable portrait that is in line with the frequentist analysis.</p>
<div class="cell" data-hash="priors_cache/html/fig-hist-difference_rates_0a3e1f8a15cfa1dac11a6544ecf57cbd">
<div class="cell-output-display">
<div id="fig-hist-difference_rates" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="priors_files/figure-html/fig-hist-difference_rates-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.2: Histograms of posterior summaries for differences (left) and rates (right) based on 1000 simulations from the independent gamma posteriors.</figcaption>
</figure>
</div>
</div>
</div>
<p>To get an approximation to the posterior mean of the ratio <span class="math inline">\(\lambda_1/\lambda_2\)</span>, it suffices to draw independent observations from their respective posterior, compute the ratio and take the sample mean of those draws. We can see that the sampling distribution of the ratio is nearly symmetrical, so we can expect Wald intervals to perform well should one be interested in building confidence intervals. This is however hardly surprising given the sample size at play.</p>
</div>
<div id="exm-conjugatepriors-normal" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6 (Conjugate prior for Gaussian mean with known variance) </strong></span>Consider an <span class="math inline">\(n\)</span> simple random sample of independent and identically distributed Gaussian variables with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, denoted <span class="math inline">\(Y_i \sim \mathsf{No}(\mu, \sigma^2)\)</span>. We pick a Gaussian prior for the location parameter, <span class="math inline">\(\mu \sim \mathsf{No}(\nu, \tau^2)\)</span> where we assume <span class="math inline">\(\mu, \tau\)</span> are fixed hyperparameter values. For now, we consider only inference for <span class="math inline">\(p(\mu \mid \sigma)\)</span>: discarding any term that is not a function of <span class="math inline">\(\mu\)</span>, the conditional posterior is <span class="math display">\[\begin{align*}
p(\mu \mid \sigma) &amp;\propto \exp\left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_{i}-\mu)^2\right\} \exp\left\{-\frac{1}{2\tau^2}(\mu - \nu)^2\right\}
\\&amp;\propto p(\sigma)\sigma^{-1} \exp\left\{\left(\frac{\sum_{i=1}^n y_{i}}{\sigma^2} + \frac{\nu}{\tau^2}\right)\mu - \left( \frac{n}{2\sigma^2} +\frac{1}{2\tau^2}\right)\mu^2\right\}.
\end{align*}\]</span> The log of the posterior density conditional on <span class="math inline">\(\sigma\)</span> is quadratic in <span class="math inline">\(\mu\)</span>, it must be a Gaussian distribution truncated over the positive half line. This can be seen by completing the square in <span class="math inline">\(\mu\)</span>, or by comparing this expression to the density of <span class="math inline">\(\mathsf{No}(\mu, \sigma^2)\)</span>, <span class="math display">\[\begin{align*}
f(x; \mu, \sigma) \stackrel{\mu}{\propto} \exp\left(-\frac{1}{2 \sigma^2}\mu^2 + \frac{x}{\sigma^2}\mu\right)
\end{align*}\]</span> we can deduce by matching mean and variance that the conditional posterior <span class="math inline">\(p(\mu \mid \sigma)\)</span> is Gaussian with reciprocal variance (precision) <span class="math inline">\(n/\sigma^2 + 1/\tau^2\)</span> and mean <span class="math inline">\((n\overline{y}\tau^2 + \nu \sigma^2)/(n\tau^2 + \sigma^2)\)</span>. The precision is an average of that of the prior and data, but assigns more weight to the latter, which increases linearly with the sample size <span class="math inline">\(n\)</span>. Likewise, the posterior mean is a weighted average of prior and sample mean, with weights proportional to the relative precision.</p>
</div>
<p>The exponential family is quite large; <a href="https://doi.org/10.1.1.157.5540">Fink (1997) <em>A Compendium of Conjugate Priors</em></a> gives multiple examples of conjugate priors and work out parameter values.</p>
<p>In general, unless the sample size is small and we want to add expert opinion, we may wish to pick an <em>uninformative prior</em>, i.e., one that does not impact much the outcome. For conjugate models, one can often show that the relative weight of prior parameters (relative to the random sample likelihood contribution) becomes negligible by <a href="https://en.wikipedia.org/wiki/Conjugate_prior">investigating their relative weights</a>.</p>
</section>
<section id="uninformative-priors" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="uninformative-priors"><span class="header-section-number">2.2</span> Uninformative priors</h2>
<div id="def-properprior" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 (Proper prior) </strong></span>We call a prior function <em>proper</em> if it’s integral is finite over the parameter space; such prior function automatically leads to a valid posterior.</p>
</div>
<p>The best example of prior priors arise from probability density function. We can still employ this rule for improper priors: for example, taking <span class="math inline">\(\alpha, \beta \to 0\)</span> in the beta prior leads to a prior proportional to <span class="math inline">\(x^{-1}(1-x)^{-1}\)</span>, the integral of which diverges on the unit interval <span class="math inline">\([0,1]\)</span>. However, as long as the number of success and the number of failures is larger than 1, meaning <span class="math inline">\(k \geq 1, n-k \geq 1\)</span>, the posterior distribution would be proper, i.e., integrable. To find the posterior, normalizing constants are also superfluous.</p>
<p>Many uninformative priors are flat, or proportional to a uniform on some subset of the real line and therefore improper. It may be superficially tempting to set a uniform prior on a large range to ensure posterior property, but the major problem is that a flat prior may be informative in a different parametrization, as the following example suggests.</p>
<div id="exm-scaleflatprior" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.7 (Transformation of flat prior for scales) </strong></span>Consider the parameter <span class="math inline">\(\log(\tau) \in \mathbb{R}\)</span> and the prior <span class="math inline">\(p( \log \tau) \propto 1\)</span>. If we reparametrize the model in terms of <span class="math inline">\(\tau\)</span>, the new prior (including the Jacobian of the transformation) is <span class="math inline">\(\tau^{-1}\)</span></p>
</div>
<p>Some priors are standard and widely used. In location scale families with location <span class="math inline">\(\nu\)</span> and scale <span class="math inline">\(\tau\)</span>, the density is such that <span class="math display">\[\begin{align*}
f(x; \nu, \tau) =  \frac{1}{\tau} f\left(\frac{x - \nu}{\tau}\right), \qquad \nu \in \mathbb{R}, \tau &gt;0.
\end{align*}\]</span> We thus wish to have a prior so that <span class="math inline">\(p(\tau) = c^{-1}p(\tau/c)\)</span> for any scaling <span class="math inline">\(c&gt;0\)</span>, whence it follows that <span class="math inline">\(p(\tau) \propto \tau^{-1}\)</span>, which is uniform on the log scale.</p>
<p>The priors <span class="math inline">\(p(\nu) \propto 1\)</span> and <span class="math inline">\(p(\tau) \propto \tau^{-1}\)</span> are both improper but lead to location and scale invariance, hence that the result is the same regardless of the units of measurement.</p>
<div class="keyidea" name="Objective and subjective Bayes">
<p>One criticism of the Bayesian approach is the arbitrariness of prior functions. However, the role of the prior is often negligible in large samples (consider for example the posterior of exponential families with conjugate priors). Moreover, the likelihood is also chosen for convenience, and arguably has a bigger influence on the conclusion. Data fitted using a linear regression model seldom follow Gaussian distributions conditionally, in the same way that the linearity is a convenience (and first order approximation).</p>
</div>
<div id="def-jeffreys" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3 (Jeffrey’s prior) </strong></span>In single parameter models, taking a prior function for <span class="math inline">\(\theta\)</span> proportional to the square root of the determinant of the information matrix, <span class="math inline">\(p(\theta) \propto |\imath(\theta)|^{1/2}\)</span> yields a prior that is invariant to reparametrization, so that inferences conducted in different parametrizations are equivalent.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<!--
From notes of M. Jordan for MATH 260.
-->
<p>To see this, consider a bijective transformation <span class="math inline">\(\theta \mapsto \vartheta\)</span>. Under the reparametrized model and suitable regularity conditions<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, the chain rule implies that <span class="math display">\[\begin{align*}
i(\vartheta) &amp;= - \mathsf{E} \left(\frac{\partial^2 \ell(\vartheta)}{\partial^2 \vartheta}\right)
\\&amp;= - \mathsf{E}\left(\frac{\partial^2 \ell(\theta)}{\partial \theta^2}\right) \left( \frac{\mathrm{d} \theta}{\mathrm{d} \vartheta} \right)^2 + \mathsf{E}\left(\frac{\partial \ell(\theta)}{\partial \theta}\right) \frac{\mathrm{d}^2 \theta}{\mathrm{d} \vartheta^2}
\end{align*}\]</span> Since the score has mean zero, <span class="math inline">\(\mathsf{E}\left\{\partial \ell(\theta)/\partial \theta\right\}=0\)</span> and the rightmost term vanishes. We can thus relate the Fisher information in both parametrizations, with <span class="math display">\[\begin{align*}
\imath^{1/2}(\vartheta) = \imath^{1/2}(\theta) \left| \frac{\mathrm{d} \theta}{\mathrm{d} \vartheta} \right|,
\end{align*}\]</span> implying invariance.</p>
<p>Most of the times, Jeffrey’s prior is improper. For the binomial model, it can be viewed as a limiting conjugate beta prior with <span class="math inline">\(\alpha, \beta\to 0\)</span>). Unfortunately, in multiparameter models, the system isn’t invariant to reparametrization if we consider the determinant of the Fisher information.</p>
</div>
<div id="exm-jeffreysbinom" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.8 (Jeffrey’s prior for the binomial distribution) </strong></span>Consider the binomial distribution <span class="math inline">\(f(y; \theta, n) \propto \theta^y(1-\theta)^{n-y}\mathsf{I}_{\theta \in [0,1]}\)</span>. The negative of the second derivative of the log likelihood with respect to <span class="math inline">\(p\)</span> is <span class="math display">\[\jmath(\theta) = - \partial^2 \ell(\theta; y) / \partial \theta^2 = y/\theta^2 + (1-y)/(1-\theta)^2\]</span> and since <span class="math inline">\(\mathsf{E}(Y)=n\theta\)</span>, the Fisher information is <span class="math display">\[\imath(\vartheta) = \mathsf{E}\{\jmath(\theta)\}=n/\theta + n/(1-\theta) = n/\{\theta(1-\theta)\}\]</span> Jeffrey’s prior is thus <span class="math inline">\(p(\theta) \propto \theta^{-1}(1-\theta)^{-1}\)</span>.</p>
</div>
</section>
<div id="exer-jeffreysnormal">
<section id="jeffreys-prior-for-the-normal-distribution" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="jeffreys-prior-for-the-normal-distribution"><span class="header-section-number">2.3</span> Jeffrey’s prior for the normal distribution</h2>
<p>Check that for the Gaussian distribution <span class="math inline">\(\mathsf{No}(\mu, \sigma^2)\)</span>, the Jeffrey’s prior obtained by treating each parameter as fixed in turn, are <span class="math inline">\(p(\mu) \propto 1\)</span> and <span class="math inline">\(p(\sigma) \propto 1/\sigma\)</span>, which also correspond to the default uninformative priors for location-scale families.</p>
</section>
</div>
<div id="exm-jeffreyspoisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.9 (Jeffrey’s prior for the Poisson distribution) </strong></span>The Poisson distribution with <span class="math inline">\(\ell(\lambda) \propto -\lambda + y\log \lambda\)</span>, with second derivative <span class="math inline">\(-\partial^2 \ell(\lambda)/\partial \lambda^2 = y/\lambda^2\)</span>. Since the mean of the Poisson distribution is <span class="math inline">\(\lambda\)</span>, the Fisher information is <span class="math inline">\(\imath(\lambda) = \lambda^{-1}\)</span> and Jeffrey’s prior is <span class="math inline">\(\lambda^{-1/2}\)</span>.</p>
</div>
<section id="prior-simulation" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="prior-simulation"><span class="header-section-number">2.4</span> Prior simulation</h2>
<p>Oftentimes, expert elicitation is difficult and it is hard to grasp what the impacts of the hyperparameters are. One way to see if the priors are reasonable is to sample values from them and generate new observations, resulting in prior predictive draws.</p>
<p>The prior predictive is <span class="math inline">\(\int_{\boldsymbol{\Theta}} p(y \mid \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}\)</span>: we can simulate outcomes from it by first drawing parameter values from the prior, then sampling new observations from the distribution in the likelihood and keeping only the latter.</p>
<div id="exm-bixi-temp" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.10 </strong></span>Consider the daily number of Bixi bike sharing users for 2017–2019 at the Edouard Montpetit station next to HEC: we can consider a simple linear regression with log counts as a function of temperature,<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <span class="math display">\[\log (\texttt{nusers}) \sim \mathsf{No}_{+}\{\beta_0 + \beta_1 (\texttt{temp}-20), \sigma^2\}.\]</span> The <span class="math inline">\(\beta_1\)</span> slope measures units in degree Celsius per log number of person.</p>
<p>The hyperparameters depend of course on the units of the analysis, unless one standardizes response variable and explanatories: it is easier to standardize the temperature so that we consider deviations from, say 20<span class="math inline">\(^{\circ}\)</span>C, which is not far from the observed mean in the sample. After some tuning, the independent priors <span class="math inline">\(\beta_0 \sim \mathsf{No}(\overline{y}, 0.5^2)\)</span>, <span class="math inline">\(\beta_1 \sim \mathsf{No}(0, 0.05^2)\)</span> and <span class="math inline">\(\sigma \sim \mathsf{Exp}(3)\)</span> seem to yield plausible outcomes and relationships.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<div class="cell" data-hash="priors_cache/html/fig-bixi_38c36008d727bca56a2cb0b8fba8fefa">
<div class="cell-output-display">
<div id="fig-bixi" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="priors_files/figure-html/fig-bixi-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.3: Prior draws of the linear regressions with observed data superimposed (left), and draws of observations from the prior predictive distribution (in gray) against observed data (right).</figcaption>
</figure>
</div>
</div>
</div>
<p>We can draw regression lines from the prior, as in the left panel of <a href="#fig-bixi">Figure&nbsp;<span>2.3</span></a>: while some of the negative relationships appear unlikely after seeing the data, the curves all seem to pass somewhere in the cloud of point. By contrast, a silly prior is one that would result in all observations being above or below the regression line, or yield values that are much too large near the endpoints of the explanatory variable. Indeed, given the number of bikes for rental is limited (a docking station has only 20 bikes), it is also sensible to ensure that simulations do not return overly large numbers. The maximum number of daily users in the sample is 68, so priors that return simulations with more than 200 (rougly 5.3 on the log scale) are not that plausible. The prior predictive draws can help establish this and the right panel of <a href="#fig-bixi">Figure&nbsp;<span>2.3</span></a> shows that, expect for the lack of correlation between temperature and number of users, the simulated values from the prior predictive are plausible even if overdispersed.</p>
</div>
</section>
<section id="informative-priors" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="informative-priors"><span class="header-section-number">2.5</span> Informative priors</h2>
<p>One strength of the Bayesian approach is the capability of incorporating expert and domain-based knowledge through priors. Often, these will take the form of moment constraints, so one common way to derive a prior is to perform moment matching to related ellicited quantities with moments of the prior distribution. It may be easier to set priors on a different scale than those of the observations, as <a href="#exm-colestawn">Example&nbsp;<span>2.11</span></a> demonstrates.</p>
<div id="exm-colestawn" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.11 (Gamma quantile difference priors for extreme value distributions) </strong></span>The generalized extreme value distribution arises as the limiting distribution for the maximum of <span class="math inline">\(m\)</span> independent observations from some common distribution <span class="math inline">\(F\)</span>. The <span class="math inline">\(\mathsf{GEV}(\mu, \sigma, \xi)\)</span> distribution is a location-scale with distribution function <span class="math display">\[\begin{align*}
F(x) = \exp\left[ - \left\{1+\xi(x-\mu)/\sigma\right\}^{-1/\xi}_{+}\right]
\end{align*}\]</span> where <span class="math inline">\(x_{+} = \max\{0, x\}\)</span>.</p>
<p>Inverting the distribution function yields the quantile function <span class="math display">\[\begin{align*}
Q(p) \mu + \sigma \frac{(-\log p)^{-\xi}-1}{\xi}
\end{align*}\]</span></p>
<p>In environmental data, we often model annual maximum. Engineering designs are often specified in terms of the <span class="math inline">\(k\)</span>-year return levels, defined as the quantile of the annual maximum exceeded with probability <span class="math inline">\(1/k\)</span> in any given year. Using a <span class="math inline">\(\mathsf{GEV}\)</span> for annual maximum, <span class="citation" data-cites="Coles.Tawn:1996">Coles and Tawn (<a href="references.html#ref-Coles.Tawn:1996" role="doc-biblioref">1996</a>)</span> proposed modelling annual daily rainfall and specifying a prior on the quantile scale <span class="math inline">\(q_1 &lt; q_2 &lt; q_3\)</span> for tail probabilities <span class="math inline">\(p_1&gt; p_2 &gt; p_3\)</span>. To deal with the ordering constraints, gamma priors are imposed on the differences <span class="math inline">\(q_1 - o \sim \mathsf{Ga}(\alpha_1, \beta_1)\)</span>, <span class="math inline">\(q_2 - q_1 \sim \mathsf{Ga}(\alpha_2, \beta_2)\)</span> and <span class="math inline">\(q_3-q_2 \sim \mathsf{Ga}(\alpha_3, \beta_3)\)</span>, where <span class="math inline">\(o\)</span> is the lower bound of the support. The prior is thus of the form</p>
<p><span class="math display">\[\begin{align*}
p(\boldsymbol{q}) \propto q_1^{\alpha_1-1}\exp(-\beta_1 q_1) \prod_{i=2}^3 (q_i-q_{i-1})^{\alpha_i-1} \exp\{\beta_i(q_i-q_{i-1})\}.
\end{align*}\]</span> where <span class="math inline">\(0 \leq q_1 \leq q_2 \leq q_3\)</span>. The fact that these quantities refer to moments or risk estimates which practitioners often must compute as part of regulatory requirements makes it easier to specify sensible values for hyperparameters.</p>
<p>As illustrating example, consider maximum daily cumulated rainfall in Abisko, Sweden. The time series spans from 1913 until December 2014; we compute the 102 yearly maximum, which range from 11mm to 62mm, and fit a generalized extreme value distribution to these.</p>
<p>For the priors, suppose an expert elicits quantiles of the 10, 50 and 100 years return levels; say 30mm, 45mm and 70mm, respectively, for the median and likewise 40mm, 70mm and 120mm for the 90% percentile of the return levels. We can compute the differences and calculate the parameters of the gamma distribution through moment-matching: this gives roughly a shape of <span class="math inline">\(\alpha_1=18.27\)</span> and <span class="math inline">\(\beta_1=0.6\)</span>, etc. <a href="#fig-gev-colestawn-quant-prior">Figure&nbsp;<span>2.4</span></a> shows the transfer from the prior predictive to the posterior distribution. The prior is much more dispersed and concentrated on the tail, which translates in a less peaked posterior than using a weakly informative prior (dotted line): the mode of the latter is slightly to the left and with lower density in the tail.</p>
<div class="cell" data-hash="priors_cache/html/fig-gev-colestawn-quant-prior_becf62ac09acf31301c47634543f84da">
<div class="cell-output-display">
<div id="fig-gev-colestawn-quant-prior" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="priors_files/figure-html/fig-gev-colestawn-quant-prior-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.4: Kernel density estimates of draws from the posterior distribution of 100 year return levels with a Coles–Tawn quantile prior (full line) and from the corresponding prior predictive (dashed). The dotted line gives the posterior distribution for a maximum domain information prior on the shape with improper priors on location and scale.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>What would you do if we you had prior information from different sources? One way to combine these is through a mixture: given <span class="math inline">\(M\)</span> different prior distributions <span class="math inline">\(p_m(\boldsymbol{\theta})\)</span>, we can assign each a positive weight <span class="math inline">\(w_m\)</span> to form a mixture of experts prior through the linear combination <span class="math display">\[ p(\boldsymbol{\theta}) \propto \sum_{m=1}^M w_m p_m(\boldsymbol{\theta})\]</span> <!-- Example of prior elicitation with curve --></p>
<!-- :::{#exm-priors-interest-rates} -->
<!-- Medias often report banking experts forecasts for [interest rates](https://www.cbsnews.com/news/mortgage-interest-rate-forecast-what-experts-predict-for-this-year-and-2024/) or the central bank rates: these could be combined and translated into prior distributions. Note however that, if none of the expert deems a scenario possible and all assign it zero prior probability, it is impossible for the posterior to put any mass there. -->
<!-- ::: -->
</section>
<section id="priors-for-regression-models" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="priors-for-regression-models"><span class="header-section-number">2.6</span> Priors for regression models</h2>
<p>Gaussian components are widespread: not only for linear regression models, but more generally for the specification of random effects that capture group-specific effects, residuals spatial or temporal variability. In the Bayesian paradigm, there is no difference between fixed effects <span class="math inline">\(\boldsymbol{\beta}\)</span> and the random effect parameters: both are random quantities that get assigned priors.</p>
<p>It is generally good advice to center and scale explanatory variables and response vectors so they have approximately mean zero and unit variance, as this facilitates prior specification.</p>
<p>Andrew Gelman uses the following taxonomy for various levels of prior information: uninformative priors are generally flat or uniform priors with <span class="math inline">\(p(\beta) \propto 1\)</span>, vague priors are typically nearly flat even if propor, e.g., <span class="math inline">\(\beta \sim \mathsf{No}(0, 100)\)</span>, weakly informative priors provide little constraints <span class="math inline">\(\beta \sim \mathsf{No}(0, 10)\)</span>), and informative prior are typically application-specific, but constrain the ranges. Uninformative and vague priors are not recommended.</p>
<p>If Gaussian priors are ubiquitous for the mean parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>, priors for the scale are more contentious. <span class="citation" data-cites="Gelman:2006">Gelman (<a href="references.html#ref-Gelman:2006" role="doc-biblioref">2006</a>)</span> recommends a Student-<span class="math inline">\(t\)</span> distribution truncated below at <span class="math inline">\(0\)</span>, with low degrees of freedom.</p>
<p>The rationale for this choice comes from the simple two level model: <span class="math display">\[\begin{align*}
Y_{ij} &amp;\sim \mathsf{No}(\mu + \alpha_j, \sigma^2), \qquad i=1,\ldots, n_j; j = 1, \ldots, J \\
\alpha_j &amp;\sim \mathsf{No}(0, \tau^2_\alpha), \qquad j =1, \ldots, J
\end{align*}\]</span> Given <span class="math inline">\(\alpha, \mu, \sigma\)</span> and the data <span class="math inline">\(\boldsymbol{y}\)</span>, the conditionally conjugate prior is inverse gamma. Standard inference with this parametrization is complicated, because there is strong dependence between parameters.</p>
<p>To reduce this dependence, one can consider an overparametrization in which <span class="math inline">\(\alpha_j = \xi \eta_j\)</span> and <span class="math inline">\(\eta_j \sim \mathsf{No}(0, \tau^2_\eta)\)</span>, where now <span class="math inline">\(\tau_\alpha=|\xi|\tau_{\eta}\)</span> so there is an additional parameter. Consider the likelihood conditional on <span class="math inline">\(\mu, \eta_j\)</span>: we have that <span class="math inline">\((y_{ij} - \mu)/\eta_j \sim \mathsf{No}(\xi, \sigma^2/\eta_j)\)</span> so conditionally conjugate priors for <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\tau_\eta\)</span> are respectively Gaussian an inverse-gamma. This translates into a prior distribution for <span class="math inline">\(\tau_\alpha\)</span> which is that of the absolute value of a noncentral Student-<span class="math inline">\(t\)</span> with location, scale and degrees of freedom <span class="math inline">\(\nu\)</span>. If we set the location to zero, the prior puts high mass at the origin, but is heavy tailed with polynomial decay. We recommend to set degrees of freedom so that the variance is heavy-tailed, e.g., <span class="math inline">\(\nu=3\)</span>. While this prior is not conjugate, it compares favorably to the <span class="math inline">\(\mathsf{IGa}(\epsilon, \epsilon)\)</span> that used to be widespread with <span class="math inline">\(\epsilon&gt;0\)</span> typically set to <span class="math inline">\(0.01\)</span> or <span class="math inline">\(0.001\)</span>, approaching an improper prior. Posterior inference is unfortunately sensitive to the value of <span class="math inline">\(\epsilon\)</span> in hierarchical models when the random effect variance is close to zero, and more so when there are few levels for the groups since the relative weight of the prior relative to that of the likelihood contribution is then large.</p>
<div id="exm-randomeffects" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.12 (Poisson random effect models) </strong></span>We consider data from an experimental study conducted at Tech3Lab on road safety. In <span class="citation" data-cites="Brodeur:2021">Brodeur et al. (<a href="references.html#ref-Brodeur:2021" role="doc-biblioref">2021</a>)</span>, 31 participants were asked to drive in a virtual environment; the number of road violation was measured for different type of distractions (phone notification, phone on speaker, texting and smartwatch). The data are balanced, with each participant exposed to each task exactly once.</p>
<p>We model the data using a Poisson mixed model to measure the number of violations, <code>nviolation</code>, with a fixed effect for <code>task</code>, which captures the type of distraction, and a random effect for participant <code>id</code>. The hierarchical model fitted for individual <span class="math inline">\(i\)</span> and distraction type <span class="math inline">\(j\)</span> is <span class="math display">\[\begin{align*}
Y_{ij} &amp;\sim \mathsf{Po}\{\mu = \exp(\beta_{j} + \alpha_i)\}, \qquad i = 1, \ldots, 31; j = 1, \ldots, 4\\
\beta_j &amp;\sim \mathsf{No}(0, 100) \qquad   j = 1, \ldots, 4\\
\alpha_i &amp;\sim \mathsf{No}(0, \kappa^2); \qquad i=1, \ldots, 31
\kappa \sim \mathsf{St}_{+}(3)
\end{align*}\]</span> so observations are conditionally independent given hyperparameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p>In frequentist statistics, there is a distinction made in mixed-effect models between parameters that are treated as constants, termed fixed effects and corresponding in this example to <span class="math inline">\(\boldsymbol{\beta}\)</span>, and random effects, equivalent to <span class="math inline">\(\boldsymbol{\alpha}\)</span>. There is no such distinction in the Bayesian paradigm, except perhaps for the choice of prior.</p>
<p>We can look at some of posterior distribution of the 31 random effects (here the first five individuals) and the fixed effect parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>, plus the variance of the random effect <span class="math inline">\(\kappa\)</span>: there is strong evidence that the latter is non-zero, suggesting strong heterogeneity between individuals. The distraction which results in the largest number of violation is texting, while the other conditions all seem equally distracting on average (note that there is no control group with no distraction to compare with, so it is hard to draw conclusions).</p>
<div class="cell" data-hash="priors_cache/html/fig-post-dist-poisson-mixed_ffde3e4abd185b194c030066a7e83c10">
<div class="cell-output-display">
<div id="fig-post-dist-poisson-mixed" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="priors_files/figure-html/fig-post-dist-poisson-mixed-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.5: Posterior density plots with 50% credible intervals and median value for the random effects of the first five individuals (left) and the fixed effects and random effect variance (right).</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="penalized-complexity-priors" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="penalized-complexity-priors"><span class="header-section-number">2.7</span> Penalized complexity priors</h2>
<p>Oftentimes, there will be a natural family of prior density to impose on some model component, <span class="math inline">\(p(\boldsymbol{\theta} \mid \zeta)\)</span>, with hyperparameter <span class="math inline">\(\zeta\)</span>. The flexibility of the underlying construction leads itself to overfitting. Penalized complexity priors <span class="citation" data-cites="Simpson:2017">(<a href="references.html#ref-Simpson:2017" role="doc-biblioref">Simpson et al. 2017</a>)</span> aim to palliate this by penalizing models far away from a simple baseline model, which correspond to a fixed value <span class="math inline">\(\zeta_0\)</span>. The prior will favour the simpler parsimonious model the more prior mass one places on <span class="math inline">\(\zeta_0\)</span>, which is in line with Occam’s razor principle.</p>
<p>To construct a penalized-complexity prior, we compute the Kullback–Leibler divergence between the model <span class="math inline">\(p_\zeta \equiv p(\boldsymbol{\theta} \mid \zeta)\)</span> relative to the baseline with <span class="math inline">\(\zeta_0\)</span>, <span class="math inline">\(p_0 \equiv p(\boldsymbol{\theta} \mid \zeta_0)\)</span>; the Kullback–Leibler divergence is</p>
<p><span class="math display">\[
\mathsf{KL}(p_\zeta \Vert\, p_0)=\int p_\zeta \log\left(\frac{p_\zeta}{p_0}\right) \mathrm{d} \boldsymbol{\theta}.
\]</span> The distance between the prior densities is then set to <span class="math inline">\(d(\zeta) = \{2\mathsf{KL}(p_\zeta \mid\mid p_0)\}^{1/2}\)</span>. which is zero at the model with <span class="math inline">\(\zeta_0\)</span>. The PC prior then constructs an exponential prior on the distance scale, which after back-transformation gives <span class="math inline">\(p(\zeta \mid \lambda) = \lambda\exp(-\lambda d(\zeta)) \left| {\partial d(\zeta)}/{\partial \zeta}\right|\)</span>. To choose <span class="math inline">\(\lambda\)</span>, the authors recommend elicitation of a pair <span class="math inline">\((U, \alpha)\)</span> such that <span class="math inline">\(\Pr(\lambda &gt; U)=\alpha\)</span>.</p>
<div id="exm-pcprior-randomeffect" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.13 (Penalized complexity prior for random effects models) </strong></span><span class="citation" data-cites="Simpson:2017">Simpson et al. (<a href="references.html#ref-Simpson:2017" role="doc-biblioref">2017</a>)</span> give the example of a Gaussian prior for random effects <span class="math inline">\(\boldsymbol{\alpha}\)</span>, of the form <span class="math inline">\(\boldsymbol{\alpha} \mid \zeta \sim \mathsf{No}_J(\boldsymbol{0}_J, \zeta^2 \mathbf{I}_J)\)</span> where <span class="math inline">\(\zeta_0=0\)</span> corresponds to the absence of random subject-variability. The penalized complexity prior for the scale <span class="math inline">\(\zeta\)</span> is then an exponential with rate <span class="math inline">\(\lambda\)</span>,<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> with density <span class="math inline">\(p(\zeta \mid \lambda) = \lambda \exp(-\lambda \zeta)\)</span>. Using the recommendation for setting <span class="math inline">\(\lambda\)</span>, we get that <span class="math inline">\(\lambda = -\ln(\alpha/U)\)</span> and this can be directly interpreted in terms of standard deviation of <span class="math inline">\(\zeta\)</span>; simulation from the prior predictive may also be used for calibration.</p>
</div>
<div id="exm-pcprior-arorder" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.14 (Penalized complexity prior for autoregressive model of order 1) </strong></span><span class="citation" data-cites="Sorbye.Holbek.Rue:2017">Sørbye and Rue (<a href="references.html#ref-Sorbye.Holbek.Rue:2017" role="doc-biblioref">2017</a>)</span> derive penalized complexity prior for the Gaussian stationary AR(1) model with autoregressive parameter <span class="math inline">\(\phi \in (-1,1)\)</span>, where <span class="math inline">\(Y_t \mid Y_{t-1}, \phi, \sigma^2 \sim \mathsf{No}(\phi Y_{t-1}, \sigma^2)\)</span>. There are two based models that could be of interest: one with <span class="math inline">\(\phi=0\)</span>, corresponding to lack of autocorrelation, and a static mean <span class="math inline">\(\phi=1\)</span> for no change in time, which is not stationary. For the former, the penalized complexity prior is <span class="math display">\[\begin{align*}
p(\phi \mid \lambda) = \frac{\lambda}{2} \exp\left[-\lambda \left\{-\ln(1-\phi^2)\right\}^{1/2}\right] \frac{|\phi|}{(1-\phi^2)\left\{-\ln(1-\phi^2)\right\}^{1/2}}.
\end{align*}\]</span> One can set <span class="math inline">\(\lambda\)</span> by considering plausible values by relating the parameter to the variance of the one-step ahead forecast error.</p>
</div>
</section>
<section id="sensitivity-analysis" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="sensitivity-analysis"><span class="header-section-number">2.8</span> Sensitivity analysis</h2>
<p>Do priors matter? The answer to that question depends strongly on the model, and how much information the data provides about hyperparameters. While this question is easily answered in conjugate models (the relative weight of hyperparameters relative to data can be derived from the posterior parameters), it is not so simple in hierarchical models, where the interplay between prior distributions is often more intricate. To see the impact, one often has to rely on doing several analyses with different values fr the prior and see the sensitivity of the conclusions to these changes, for example by considering a vague prior or modifying the parameters values (say halving or doubling). If the changes are immaterial, then this provides reassurance that our analyses are robust.</p>
<div id="exm-sensitivity-poisson-mixed" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.15 </strong></span>To check the sensitivity of the conclusion, we revisit the modelling of the <code>smartwatch</code> experiment data using a Poisson regression and compare four priors: a uniform prior truncated to <span class="math inline">\([0, 10]\)</span>, an inverse gamma <span class="math inline">\(\mathsf{IG}(0.01, 0.01)\)</span> prior, a penalized complexity prior such that the 0.95 percentile of the scale is 5, corresponding to <span class="math inline">\(\mathsf{Exp}(0.6)\)</span>. Since each distraction type appears 31 times, there is plenty of information to reliably estimate the dispersion <span class="math inline">\(\kappa\)</span> of the random effects <span class="math inline">\(\alpha\)</span>: the different density plots in <a href="#fig-sensitivity">Figure&nbsp;<span>2.6</span></a> are virtually indistinguishable from one another.</p>
<div class="cell" data-hash="priors_cache/html/fig-sensitivity_7643a668287766185fa445c2259f7ebd">
<div class="cell-output-display">
<div id="fig-sensitivity" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="priors_files/figure-html/fig-sensitivity-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.6: Posterior density of the scale of the random effects with uniform, inverse gamma, penalized complexity and folded Student-t with three degrees of freedom. The circle denotes the median and the bars the 50% and 95% percentile credible intervals.</figcaption>
</figure>
</div>
</div>
</div>
</div>

<!--

Hierarchical linear model with half-t prior

Prior elicitation may require [expert knowledge](https://arxiv.org/abs/2112.01380).


Quantile priors of [Coles and Tawn](http://www.jstor.org/stable/2986068) (using `revdbayes`)


Are my priors reasonable? Use prior predictive distribution to assess the plausibility
comparing prior to posterior standard deviations, e.g., Nott et al. (2020)

Example: simple linear regression slope (height/weight) of Figure 4.5 in McElreath


Improper priors may lead to improper posterior: stick with proper distributions unless you know what you are doing

Penalized complexity prior

Maximum domain information

Sensitivity analysis and asymptotic effect

Consensus of opinion: expert opinion and mixture

[Black--Litterman model](https://hudsonthames.org/bayesian-portfolio-optimisation-the-black-litterman-model/)



-->
<!-- Fit to data and compare results with that of the reference improper prior $p(\phi) \propto (1-\phi^2)^{-1/2}$. 
https://haakonbakkagit.github.io/btopic115.html
-->
<!-- @Zellner:1986 -->
<!-- Zellner's $g$-prior for model selection -->


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Alexander:2023" class="csl-entry" role="listitem">
Alexander, Rohan. 2023. <em>Telling Stories with Data: With Applications in <span>R</span></em>. Boca Raton, FL: CRC Press.
</div>
<div id="ref-Brodeur:2021" class="csl-entry" role="listitem">
Brodeur, Mathieu, Perrine Ruer, Pierre-Majorique Léger, and Sylvain Sénécal. 2021. <span>“Smartwatches Are More Distracting Than Mobile Phones While Driving: Results from an Experimental Study.”</span> <em>Accident Analysis &amp; Prevention</em> 149: 105846. <a href="https://doi.org/10.1016/j.aap.2020.105846">https://doi.org/10.1016/j.aap.2020.105846</a>.
</div>
<div id="ref-Coles.Tawn:1996" class="csl-entry" role="listitem">
Coles, Stuart G., and Jonathan A. Tawn. 1996. <span>“A <span>B</span>ayesian Analysis of Extreme Rainfall Data.”</span> <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 45 (4): 463–78. <a href="https://doi.org/10.2307/2986068">https://doi.org/10.2307/2986068</a>.
</div>
<div id="ref-Gelman:2006" class="csl-entry" role="listitem">
Gelman, Andrew. 2006. <span>“Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by <span>B</span>rowne and <span>D</span>raper).”</span> <em>Bayesian Analysis</em> 1 (3): 515–34. <a href="https://doi.org/10.1214/06-BA117A">https://doi.org/10.1214/06-BA117A</a>.
</div>
<div id="ref-Matias:2021" class="csl-entry" role="listitem">
Matias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles Ebersole. 2021. <span>“The <span>U</span>pworthy <span>R</span>esearch <span>A</span>rchive, a Time Series of 32,487 Experiments in <span>U.S.</span> Media.”</span> <em>Scientific Data</em> 8 (195). <a href="https://doi.org/10.1038/s41597-021-00934-7">https://doi.org/10.1038/s41597-021-00934-7</a>.
</div>
<div id="ref-Simpson:2017" class="csl-entry" role="listitem">
Simpson, Daniel, Håvard Rue, Andrea Riebler, Thiago G. Martins, and Sigrunn H. Sørbye. 2017. <span>“Penalising Model Component Complexity: A Principled, Practical Approach to Constructing Priors.”</span> <em>Statistical Science</em> 32 (1): 1–28. <a href="https://doi.org/10.1214/16-STS576">https://doi.org/10.1214/16-STS576</a>.
</div>
<div id="ref-Sorbye.Holbek.Rue:2017" class="csl-entry" role="listitem">
Sørbye, Sigrunn Holbek, and Håvard Rue. 2017. <span>“Penalised Complexity Priors for Stationary Autoregressive Processes.”</span> <em>Journal of Time Series Analysis</em> 38 (6): 923–35. <a href="https://doi.org/10.1111/jtsa.12242">https://doi.org/10.1111/jtsa.12242</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>A distribution belongs to an exponential family with parameter vector <span class="math inline">\(\boldsymbol{\theta} \in \mathbb{R}^D\)</span> if it can be written as <span class="math display">\[\begin{align*}
f(y; \boldsymbol{\theta}) = \exp\left\{ \sum_{k=1}^K Q_k(\boldsymbol{\theta}) t_k(y) + D(\boldsymbol{\theta})\right\}
\end{align*}\]</span> and in particular, the support does not depend on unknown parameters. If we have an independent and identically distributed sample of observations <span class="math inline">\(y_1, \ldots, y_n\)</span>, the log likelihood is thus of the form <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\theta}) = \sum_{k=1}^K \phi_k(\boldsymbol{\theta}) \sum_{i=1}^n t_k(y_i) + n D(\boldsymbol{\theta}),
\end{align*}\]</span> where the collection <span class="math inline">\(\sum_{i=1}^n t_k(y_i)\)</span> (<span class="math inline">\(k=1, \ldots, K\)</span>) are sufficient statistics and <span class="math inline">\(\phi_k(\boldsymbol{\theta})\)</span> are the canonical parameters. The number of sufficient statistics are the same regardless of the sample size. Exponential families play a prominent role in generalized linear models, in which the natural parameters are modeled as linear function of explanatory variables. A log prior density with parameters <span class="math inline">\(\eta, \nu_1, \ldots, \nu_K\)</span> that is proportional to <span class="math display">\[\begin{align*}
\log p(\boldsymbol{\theta}) \propto \eta D(\boldsymbol{\theta}) + \sum_{k=1}^K Q_k(\boldsymbol{\theta}) \nu_k
\end{align*}\]</span> is conjugate.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The canonical link function for Bernoulli gives rise to logistic regression model.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The stopping rule means that data stops being collected once there is enough evidence to determine if an option is more suitable, or if a predetermined number of views has been reached.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The Fisher information is linear in the sample size for independent and identically distributed data so we can derive the result for <span class="math inline">\(n=1\)</span> without loss of generality.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Using Bartlett’s identity; Fisher consistency can be established using the dominated convergence theorem.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>If counts are Poisson, then the log transform is variance stabilizing.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>One can object to the prior parameters depending on the data, but an alternative would be to model centered data <span class="math inline">\(y-\overline{y}\)</span>, in which case the prior for the intercept parameter <span class="math inline">\(\beta_0\)</span> would be zero.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Possibly truncated above if the support of <span class="math inline">\(\zeta\)</span> has a finite upper bound.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./introduction.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Bayesics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>