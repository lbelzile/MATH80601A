<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.3">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Priors – Bayesian modelling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./montecarlo.html" rel="next">
<link href="./bayesics.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-81b5c3e63835cfde897ecd3d35a35a41.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ad5ef081d17d57ab2b6cc5e45efb5d90.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./priors.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Priors</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/MATH80601A" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH80601A-bayesmod.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./priors.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./montecarlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Monte Carlo methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Metropolis–Hastings algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gibbs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Gibbs sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Computational strategies and diagnostics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regression models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./laplace.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Gaussian approximations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#prior-simulation" id="toc-prior-simulation" class="nav-link active" data-scroll-target="#prior-simulation"><span class="header-section-number">3.1</span> Prior simulation</a></li>
  <li><a href="#conjugate-priors" id="toc-conjugate-priors" class="nav-link" data-scroll-target="#conjugate-priors"><span class="header-section-number">3.2</span> Conjugate priors</a></li>
  <li><a href="#uninformative-priors" id="toc-uninformative-priors" class="nav-link" data-scroll-target="#uninformative-priors"><span class="header-section-number">3.3</span> Uninformative priors</a></li>
  <li><a href="#priors-for-regression-models" id="toc-priors-for-regression-models" class="nav-link" data-scroll-target="#priors-for-regression-models"><span class="header-section-number">3.4</span> Priors for regression models</a></li>
  <li><a href="#informative-priors" id="toc-informative-priors" class="nav-link" data-scroll-target="#informative-priors"><span class="header-section-number">3.5</span> Informative priors</a></li>
  <li><a href="#sensitivity-analysis" id="toc-sensitivity-analysis" class="nav-link" data-scroll-target="#sensitivity-analysis"><span class="header-section-number">3.6</span> Sensitivity analysis</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/lbelzile/MATH80601A/edit/main/priors.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><h1 class="title display-7"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Priors</span></h1></header>

<header id="title-block-header">


</header>


<p>The posterior distribution combines two ingredients: the likelihood and the prior. If the former is a standard ingredient of any likelihood-based inference, prior specification requires some care. The purpose of this chapter is to consider different standard way of constructing prior functions, and to specify the parameters of the latter: we term these hyperparameters.</p>
<p>The posterior is a compromise prior and likelihood: the more informative the prior, the more the posterior resembles it, but in large samples, the effect of the prior is often negligible if there is enough information in the likelihood about all parameters. We can assess the robustness of the prior specification through a sensitivity analysis by comparing the outcomes of the posterior for different priors or different values of the hyperparameters.</p>
<p>Oftentimes, we will specify independent priors in multiparameter models, but the posterior of these will not be independent.</p>
<p>We can use moment matching to get sensible values, or tune via trial-and-error using the prior predictive draws to assess the implausibility of the prior outcomes. One challenge is that even if we have some prior information (e.g., we can obtain sensible prior values for the mean, quantiles or variance of the parameter of interest), these summary statisticss will not typically be enough to fully characterize the prior: many different functions or distributions could encode the same information. This means that different analysts get different inferences. Generally, we will choose the prior for convenience. Priors are controversial because they could be tuned aposteriori to give any answer an analyst might want.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<strong>Learning objectives</strong>:
</div>
</div>
<div class="callout-body-container callout-body">
<p>At the end of the chapter, students should be able to</p>
<ul>
<li>define and distinguish between improper, weak and informative priors.</li>
<li>propose conjugate priors for exponential families.</li>
<li>assess by using the prior predictive distribution the compatibility of the prior with the model.</li>
<li>use moment matching to specify the parameters of a prior distribution.</li>
<li>perform sensitivity analysis by running a model with different priors and assessing changes to the posterior distribution.</li>
</ul>
</div>
</div>
<section id="prior-simulation" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="prior-simulation"><span class="header-section-number">3.1</span> Prior simulation</h2>
<p>Expert elicitation is difficult and it is hard to grasp what the impacts of the hyperparameters are. One way to see if the priors are reasonable is to sample values from them and generate new observations, resulting in prior predictive draws.</p>
<p>The prior predictive is <span class="math inline">\(\int_{\boldsymbol{\Theta}} f(y_{\text{new}}; \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}\)</span>: we can simulate outcomes from it by first drawing parameter values <span class="math inline">\(\boldsymbol{\theta}_0\)</span> from the prior, then sampling new observations from the distribution <span class="math inline">\(f(y_{\text{new}}; \boldsymbol{\theta}_0)\)</span> with those parameters values and keeping only <span class="math inline">\(y_{\text{new}}.\)</span> If there are sensible bounds for the range of the response, we could restrict the prior range and shape until values abide to these.</p>
<p>Working with standardized inputs <span class="math inline">\(x_i \mapsto (x_i - \overline{x})/\mathrm{sd}(\boldsymbol{x})\)</span> is useful. For example, in a simple linear regression (with a sole numerical explanatory), the slope is the correlation between standardized explanatory <span class="math inline">\(\mathrm{X}\)</span> and standardized response <span class="math inline">\(Y\)</span> and the intercept should be mean zero.</p>
<div id="exm-bixi-temp" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1</strong></span> Consider the daily number of Bixi bike sharing users for 2017–2019 at the Edouard Montpetit station next to HEC: we can consider a simple linear regression with log counts as a function of temperature,<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="math display">\[\log (\texttt{nusers}) \sim \mathsf{Gauss}_{+}\{\beta_0 + \beta_1 (\texttt{temp}-20), \sigma^2\}.\]</span> The <span class="math inline">\(\beta_1\)</span> slope measures units in degree Celsius per log number of person.</p>
<p>The hyperparameters depend of course on the units of the analysis, unless one standardizes response variable and explanatories: it is easier to standardize the temperature so that we consider deviations from, say 20<span class="math inline">\(^{\circ}\)</span>C, which is not far from the observed mean in the sample. After some tuning, the independent priors <span class="math inline">\(\beta_0 \sim \mathsf{Gauss}(\overline{y}, 0.5^2),\)</span> <span class="math inline">\(\beta_1 \sim \mathsf{Gauss}(0, 0.05^2)\)</span> and <span class="math inline">\(\sigma \sim \mathsf{Exp}(3)\)</span> seem to yield plausible outcomes and relationships.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-bixi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bixi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="priors_files/figure-html/fig-bixi-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bixi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Prior draws of the linear regressions with observed data superimposed (left), and draws of observations from the prior predictive distribution (in gray) against observed data (right).
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can draw regression lines from the prior, as in the left panel of <a href="#fig-bixi" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>: while some of the negative relationships appear unlikely after seeing the data, the curves all seem to pass somewhere in the cloud of point. By contrast, a silly prior is one that would result in all observations being above or below the regression line, or yield values that are much too large near the endpoints of the explanatory variable. Indeed, given the number of bikes for rental is limited (a docking station has only 20 bikes), it is also sensible to ensure that simulations do not return overly large numbers. The maximum number of daily users in the sample is 68, so priors that return simulations with more than 200 (rougly 5.3 on the log scale) are not that plausible. The prior predictive draws can help establish this and the right panel of <a href="#fig-bixi" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> shows that, expect for the lack of correlation between temperature and number of users, the simulated values from the prior predictive are plausible even if overdispersed.</p>
</div>
</section>
<section id="conjugate-priors" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="conjugate-priors"><span class="header-section-number">3.2</span> Conjugate priors</h2>
<p>In very simple models, there may exists prior densities that result in a posterior distribution of the same family. We can thus directly extract characteristics of the posterior. Conjugate priors are chosen for computational convenience and because interpretation is convenient, as the parameters of the posterior will often be some weighted average of prior and likelihood component.</p>
<div id="def-conjugate-prior" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1 (Conjugate priors)</strong></span> A prior density <span class="math inline">\(p(\boldsymbol{\theta})\)</span> is conjugate for likelihood <span class="math inline">\(L(\boldsymbol{\theta}; \boldsymbol{y})\)</span> if the product <span class="math inline">\(L(\boldsymbol{\theta}; \boldsymbol{y})p(\boldsymbol{\theta}),\)</span> after renormalization, is of the same parametric family as the prior.</p>
<p>Exponential families (see <a href="introduction.html#def-exponential-family" class="quarto-xref">Definition&nbsp;<span>1.13</span></a>, including the binomial, Poisson, exponential, Gaussian distributions) admit conjugate priors.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 32%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>distribution</th>
<th style="text-align: left;">unknown parameter</th>
<th style="text-align: left;">conjugate prior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Y \sim \mathsf{expo}(\lambda)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\lambda\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\lambda \sim \mathsf{gamma}(\alpha, \beta)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Y \sim \mathsf{Poisson}(\mu)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu \sim \mathsf{gamma}(\alpha, \beta)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Y \sim \mathsf{binom}(n, \theta)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\theta\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\theta \sim \mathsf{Be}(\alpha, \beta)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Y \sim \mathsf{Gauss}(\mu, \sigma^2)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu \sim \mathsf{Gauss}(\nu, \omega^2)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Y \sim \mathsf{Gauss}(\mu, \sigma^2)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\sigma\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\sigma^{2} \sim \mathsf{inv. gamma}(\alpha, \beta)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Y \sim \mathsf{Gauss}(\mu, \sigma^2)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu, \sigma\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu \mid \sigma^2 \sim \mathsf{Gauss}(\nu, \omega \sigma^2),\)</span> <span class="math inline">\(\sigma^{2} \sim \mathsf{inv. gamma}(\alpha, \beta)\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="exm-conjugatepriors-binom" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2 (Conjugate prior for the binomial model)</strong></span> Since the density of the binomial is of the form <span class="math inline">\(p^y(1-p)^{n-y}\)</span> and it belongs to an exponential family (<a href="introduction.html#exm-exponential-family-binom" class="quarto-xref">Example&nbsp;<span>1.2</span></a>), the beta distribution <span class="math inline">\(\mathsf{beta}(\alpha, \beta)\)</span> with density <span class="math display">\[f(x) \propto x^{\alpha-1} (1-x)^{\beta-1}\]</span> is the conjugate prior.</p>
<p>The beta distribution is also the conjugate prior for the negative binomial, geometric and Bernoulli distributions, since their likelihoods are all proportional to that of the beta. The fact that different sampling schemes that result in proportional likelihood functions give the same inference is called likelihood principle.</p>
</div>
<div id="exm-conjugatepriors-poisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3 (Conjugate prior for the Poisson model)</strong></span> We saw in <a href="introduction.html#exm-exponential-family-poisson" class="quarto-xref">Example&nbsp;<span>1.3</span></a> that the Poisson distribution is an exponential family. The gamma density, <span class="math display">\[ f(x) \propto \beta^{\alpha}/\Gamma(\alpha)x^{\alpha-1} \exp(-\beta x)\]</span> with shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span> is the conjugate prior for the Poisson. For an <span class="math inline">\(n\)</span>-sample of independent observations <span class="math inline">\(\mathsf{Poisson}(\mu)\)</span> observations with <span class="math inline">\(\mu \sim \mathsf{gamma}(\alpha, \beta),\)</span> the posterior is <span class="math inline">\(\mathsf{gamma}(\sum_{i=1}^n y_i + \alpha, \beta + n).\)</span></p>
</div>
<p>Knowing the analytic expression for the posterior can be useful for calculations of the marginal likelihood, as <a href="introduction.html#exm-poisson-negbin" class="quarto-xref">Example&nbsp;<span>1.13</span></a> demonstrated.</p>
<div id="exm-abtest" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4 (Posterior rates for A/B tests using conjugate Poisson model)</strong></span> Upworthy.com, a US media publisher, revolutionized headlines online advertisement by running systematic A/B tests to compare the different wording of headlines, placement and image and what catches attention the most. The Upworthy Research Archive <span class="citation" data-cites="Matias:2021">(<a href="references.html#ref-Matias:2021" role="doc-biblioref">Matias et al. 2021</a>)</span> contains results for 22743 experiments, with a click through rate of 1.58% on average and a standard deviation of 1.23%. The <code>clickability_test_id</code> gives the unique identifier of the experiment, <code>clicks</code> the number of conversion out of <code>impressions</code>. See <a href="https://tellingstorieswithdata.com/08-hunt.html#ab-testing">Section 8.5</a> of <span class="citation" data-cites="Alexander:2023">Alexander (<a href="references.html#ref-Alexander:2023" role="doc-biblioref">2023</a>)</span> for more details about A/B testing and background information.</p>
<p>Consider an A/B test from November 23st, 2014, that compared four different headlines for a story on Sesame Street workshop with interviews of children whose parents were in jail and visiting them in prisons. The headlines tested were:</p>
<blockquote class="blockquote">
<ol type="1">
<li>Some Don’t Like It When He Sees His Mom. But To Him? Pure Joy. Why Keep Her From Him?</li>
<li>They’re Not In Danger. They’re Right. See True Compassion From The Children Of The Incarcerated.</li>
<li>Kids Have No Place In Jail … But In This Case, They <em>Totally</em> Deserve It.</li>
<li>Going To Jail <em>Should</em> Be The Worst Part Of Their Life. It’s So Not. Not At All.</li>
</ol>
</blockquote>
<p>At first glance, the first and third headlines seem likely to lead to a curiosity gap. The wording of the second is more explicit (and searchable), whereas the first is worded as a question.</p>
<p>We model the conversion rate <span class="math inline">\(\lambda_i\)</span> for each headline separately using a Poisson distribution and compare the posterior distributions for all four choices. Using a conjugate prior and selecting the parameters by moment matching yields approximately <span class="math inline">\(\alpha = 1.65\)</span> and <span class="math inline">\(\beta = 104.44\)</span> for the hyperparameters, setting <span class="math inline">\(\alpha/\beta = 0.0158\)</span> and <span class="math inline">\(\alpha/\beta^2=0.0123^2\)</span> and solving for the two unknown parameters.</p>
<div class="cell">
<div id="tbl-upworthy" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-upworthy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Number of views, clicks for different headlines for the Upworthy data.
</figcaption>
<div aria-describedby="tbl-upworthy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">headline</th>
<th style="text-align: right;">impressions</th>
<th style="text-align: right;">clicks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">H1</td>
<td style="text-align: right;">3060</td>
<td style="text-align: right;">49</td>
</tr>
<tr class="even">
<td style="text-align: left;">H2</td>
<td style="text-align: right;">2982</td>
<td style="text-align: right;">20</td>
</tr>
<tr class="odd">
<td style="text-align: left;">H3</td>
<td style="text-align: right;">3112</td>
<td style="text-align: right;">31</td>
</tr>
<tr class="even">
<td style="text-align: left;">H4</td>
<td style="text-align: right;">3083</td>
<td style="text-align: right;">9</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-upworthy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-upworthy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="priors_files/figure-html/fig-upworthy-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-upworthy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Gamma posterior for conversion rate of the different Upworthy Sesame Street headline.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can visualize the posterior distributions. In this context, the large sample size lead to the dominance of the likelihood contribution <span class="math inline">\(p(Y_i \mid \lambda_i) \sim \mathsf{Poisson}(n_i\lambda_i)\)</span> relative to the prior. We can see there is virtually no overlap between different rates for headers H1 (preferred) relative to H4 (least favorable). The probability that the conversion rate for Headline 3 is higher than Headline 1 can be approximated by simulating samples from both posteriors and computing the proportion of times one is larger: we get 2% for <code>H3</code> relative to <code>H1</code>, indicating a clear preference for the first headline <code>H1</code>.</p>
</div>
<div id="exm-poisson-upworthy-question" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.5 (Should you phrase your headline as a question?)</strong></span> We can also consider aggregate records for Upworthy, as <span class="citation" data-cites="Alexander:2023">Alexander (<a href="references.html#ref-Alexander:2023" role="doc-biblioref">2023</a>)</span> did. The <code>upworthy_question</code> database contains a balanced sample of all headlines where at least one of the choices featured a question, with at least one alternative statement. Whether a headline contains a question or not is determined by querying for the question mark. We consider aggregated counts for all such headlines, with the <code>question</code> factor encoding whether there was a question, <code>yes</code> or <code>no</code>. For simplicity, we treat the number of views as fixed, but keep in mind that A/B tests are often sequential experiments with a stopping rule.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>We model first the rates using a Poisson regression; the corresponding frequentist analysis would include an offset to account for differences in views. If <span class="math inline">\(\lambda_{j}\)</span> <span class="math inline">\((j=1, 2)\)</span> are the average rate for each factor level (yes and no), then <span class="math inline">\(\mathsf{E}(Y_{ij}/n_{ij}) = \lambda_j.\)</span> In the frequentist setting, we can fit a simple Poisson generalized linear regression model with an offset term and a binary variable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(upworthy_question, <span class="at">package =</span> <span class="st">"hecbayes"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>poismod <span class="ot">&lt;-</span> <span class="fu">glm</span>(</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  clicks <span class="sc">~</span> <span class="fu">offset</span>(<span class="fu">log</span>(impressions)) <span class="sc">+</span> question, </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">poisson</span>(<span class="at">link =</span> <span class="st">"log"</span>),</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> upworthy_question)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(poismod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)  questionno 
-4.51264669  0.07069677 </code></pre>
</div>
</div>
<p>The coefficients represent the difference in log rate (multiplicative effect) relative to the baseline rate, with an increase of 6.3 percent when the headline does not contain a question. A likelihood ratio test can be performed by comparing the deviance of the null model (intercept-only), indicating strong evidence that including question leads to significatively different rates. This is rather unsurprising given the enormous sample sizes.</p>
<p>Consider instead a Bayesian analysis with conjugate prior: we model separately the rates of each group (question or not). Suppose we think apriori that the click-rate is on average 1%, with a standard deviation of 2%, with no difference between questions or not. For a <span class="math inline">\(\mathsf{Gamma}(\alpha, \beta)\)</span> prior, this would translate, using moment matching, into a rate of <span class="math inline">\(\beta = 25 = \mathsf{E}_0(\lambda_j)/ \mathsf{Var}_0(\lambda_j)\)</span> and a shape of <span class="math inline">\(\alpha = 0.25\)</span> (<span class="math inline">\(j=1, 2\)</span>). If <span class="math inline">\(\lambda_{j}\)</span> is the average rate for each factor level (yes and no), then <span class="math inline">\(\mathsf{E}(Y_{ij}/n_{ij}) = \lambda_j\)</span> so the log likelihood is proportional, as a function of <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2,\)</span> to <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\lambda}; \boldsymbol{y}, \boldsymbol{n}) \stackrel{\boldsymbol{\lambda}}{\propto} \sum_{i=1}^n \sum_{j=1}^2 y_{ij}\log \lambda_j - \lambda_jn_{ij}
\end{align*}\]</span> and we can recognize that the posterior for <span class="math inline">\(\lambda_i\)</span> is gamma with shape <span class="math inline">\(\alpha + \sum_{i=1}^n y_{ij}\)</span> and rate <span class="math inline">\(\beta + \sum_{i=1}^n n_{ij}.\)</span> For inference, we thus only need to select hyperparameters and calculate the total number of clicks and impressions per group. We can then consider the posterior difference <span class="math inline">\(\lambda_1 - \lambda_2\)</span> or, to mimic the Poisson multiplicative model, of the ratio <span class="math inline">\(\lambda_1/\lambda_2.\)</span> The former suggests very small differences, but one must keep in mind that rates are also small. The ratio, shown in the right-hand panel of <a href="#fig-hist-difference_rates" class="quarto-xref">Figure&nbsp;<span>3.3</span></a>, gives a more easily interpretable portrait that is in line with the frequentist analysis.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-hist-difference_rates" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hist-difference_rates-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="priors_files/figure-html/fig-hist-difference_rates-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hist-difference_rates-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: Histograms of posterior summaries for differences (left) and rates (right) based on 1000 simulations from the independent gamma posteriors.
</figcaption>
</figure>
</div>
</div>
</div>
<p>To get an approximation to the posterior mean of the ratio <span class="math inline">\(\lambda_1/\lambda_2,\)</span> it suffices to draw independent observations from their respective posterior, compute the ratio and take the sample mean of those draws. We can see that the sampling distribution of the ratio is nearly symmetrical, so we can expect Wald intervals to perform well should one be interested in building confidence intervals. This is however hardly surprising given the sample size at play.</p>
</div>
<div id="exm-conjugatepriors-normal" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.6 (Conjugate prior for Gaussian mean with known variance)</strong></span> Consider an <span class="math inline">\(n\)</span> simple random sample of independent and identically distributed Gaussian variables with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma,\)</span> denoted <span class="math inline">\(Y_i \sim \mathsf{Gauss}(\mu, \sigma^2).\)</span> We pick a Gaussian prior for the location parameter, <span class="math inline">\(\mu \sim \mathsf{Gauss}(\nu, \tau^2)\)</span> where we assume <span class="math inline">\(\nu, \tau\)</span> are fixed hyperparameter values. For now, we consider only inference for the conditional marginal posterior <span class="math inline">\(p(\mu \mid \boldsymbol{y}, \sigma)\)</span>: discarding any term that is not a function of <span class="math inline">\(\mu,\)</span> the conditional posterior is <span class="math display">\[\begin{align*}
p(\mu \mid \sigma, \boldsymbol{y}) &amp;\propto \exp\left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_{i}-\mu)^2\right\} \exp\left\{-\frac{1}{2\tau^2}(\mu - \nu)^2\right\}
\\&amp;\propto \exp\left\{\left(\frac{\sum_{i=1}^n y_{i}}{\sigma^2} + \frac{\nu}{\tau^2}\right)\mu - \left( \frac{n}{2\sigma^2} +\frac{1}{2\tau^2}\right)\mu^2\right\}.
\end{align*}\]</span> The log of the posterior density conditional on <span class="math inline">\(\sigma\)</span> is quadratic in <span class="math inline">\(\mu,\)</span> it must be a Gaussian distribution truncated over the positive half line. This can be seen by completing the square in <span class="math inline">\(\mu,\)</span> or by comparing this expression to the density of <span class="math inline">\(\mathsf{Gauss}(\mu, \sigma^2),\)</span> <span class="math display">\[\begin{align*}
f(x; \mu, \sigma) \stackrel{\mu}{\propto} \exp\left(-\frac{1}{2 \sigma^2}\mu^2 + \frac{x}{\sigma^2}\mu\right)
\end{align*}\]</span> we can deduce by matching mean and variance that the conditional posterior <span class="math inline">\(p(\mu \mid \sigma)\)</span> is Gaussian with reciprocal variance (precision) <span class="math inline">\(n/\sigma^2 + 1/\tau^2\)</span> and mean <span class="math inline">\((n\overline{y}\tau^2 + \nu \sigma^2)/(n\tau^2 + \sigma^2).\)</span> The precision is an average of that of the prior and data, but assigns more weight to the latter, which increases linearly with the sample size <span class="math inline">\(n.\)</span> Likewise, the posterior mean is a weighted average of prior and sample mean, with weights proportional to the relative precision.</p>
</div>
<p>The exponential family is quite large; <a href="https://www.johndcook.com/CompendiumOfConjugatePriors.pdf">Fink (1997) <em>A Compendium of Conjugate Priors</em></a> gives multiple examples of conjugate priors and work out parameter values.</p>
<p>In general, unless the sample size is small and we want to add expert opinion, we may wish to pick an <em>uninformative prior</em>, i.e., one that does not impact much the outcome. For conjugate models, one can often show that the relative weight of prior parameters (relative to the random sample likelihood contribution) becomes negligible by <a href="https://en.wikipedia.org/wiki/Conjugate_prior">investigating their relative weights</a>.</p>
</section>
<section id="uninformative-priors" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="uninformative-priors"><span class="header-section-number">3.3</span> Uninformative priors</h2>
<div id="def-properprior" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2 (Proper prior)</strong></span> We call a prior function <em>proper</em> if it’s integral is finite over the parameter space; such prior function automatically leads to a valid posterior. A prior over <span class="math inline">\(\boldsymbol{\Theta}\)</span> is <strong>improper</strong> if <span class="math inline">\(\int_{\boldsymbol{\Theta}} p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} = \infty.\)</span></p>
</div>
<p>The best example of proper priors arise from probability density function. We can still employ this rule for improper priors: for example, taking <span class="math inline">\(\alpha, \beta \to 0\)</span> in the beta prior leads to a prior proportional to <span class="math inline">\(x^{-1}(1-x)^{-1},\)</span> the integral of which diverges on the unit interval <span class="math inline">\([0,1].\)</span> However, as long as the number of success and the number of failures is larger than 1, meaning <span class="math inline">\(k \geq 1, n-k \geq 1,\)</span> the posterior distribution would be proper, i.e., integrable. To find the posterior, normalizing constants are also superfluous.</p>
<p>Many uninformative priors are flat, or proportional to a uniform on some subset of the real line and therefore improper. It may be superficially tempting to set a uniform prior on a large range to ensure posterior property, but the major problem is that a flat prior may be informative in a different parametrization, as the following example suggests.</p>
<p><span class="citation" data-cites="Gelman:2013">Gelman et al. (<a href="references.html#ref-Gelman:2013" role="doc-biblioref">2013</a>)</span> uses the following taxonomy for various levels of prior information:</p>
<ul>
<li>uninformative priors are generally flat or uniform priors with <span class="math inline">\(p(\beta) \propto 1.\)</span></li>
<li>vague priors are typically nearly flat even if proper, e.g., <span class="math inline">\(\beta \sim \mathsf{Gauss}(0, 100),\)</span></li>
<li>weakly informative priors provide little constraints <span class="math inline">\(\beta \sim \mathsf{Gauss}(0, 10),\)</span> and</li>
<li>informative prior are typically application-specific, but constrain the ranges.</li>
</ul>
<p>Uninformative and vague priors are generally not recommended unless they are known to give valid posterior inference and the amount of information from the likelihood is high.</p>
<div id="exm-scaleflatprior" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.7 (Transformation of flat prior for scales)</strong></span> Consider the parameter <span class="math inline">\(\log(\tau) \in \mathbb{R}\)</span> and the prior <span class="math inline">\(p( \log \tau) \propto 1.\)</span> If we reparametrize the model in terms of <span class="math inline">\(\tau,\)</span> the new prior (including the Jacobian of the transformation) is <span class="math inline">\(\tau^{-1}\)</span></p>
</div>
<p>Some priors are standard and widely used. In location scale families with location <span class="math inline">\(\nu\)</span> and scale <span class="math inline">\(\tau,\)</span> the density is such that <span class="math display">\[\begin{align*}
f(x; \nu, \tau) =  \frac{1}{\tau} f\left(\frac{x - \nu}{\tau}\right), \qquad \nu \in \mathbb{R}, \tau &gt;0.
\end{align*}\]</span> We thus wish to have a prior so that <span class="math inline">\(p(\tau) = c^{-1}p(\tau/c)\)</span> for any scaling <span class="math inline">\(c&gt;0,\)</span> whence it follows that <span class="math inline">\(p(\tau) \propto \tau^{-1},\)</span> which is uniform on the log scale.</p>
<p>The priors <span class="math inline">\(p(\nu) \propto 1\)</span> and <span class="math inline">\(p(\tau) \propto \tau^{-1}\)</span> are both improper but lead to location and scale invariance, hence that the result is the same regardless of the units of measurement.</p>
<div class="keyidea" name="Objective and subjective Bayes">
<p>One criticism of the Bayesian approach is the arbitrariness of prior functions. However, the role of the prior is often negligible in large samples (consider for example the posterior of exponential families with conjugate priors). Moreover, the likelihood is also chosen for convenience, and arguably has a bigger influence on the conclusion. Data fitted using a linear regression model seldom follow Gaussian distributions conditionally, in the same way that the linearity is a convenience (and first order approximation).</p>
</div>
<div id="def-jeffreys" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.3 (Jeffrey’s prior)</strong></span> In single parameter models, taking a prior function for <span class="math inline">\(\theta\)</span> proportional to the square root of the determinant of the information matrix, <span class="math inline">\(p(\theta) \propto |\imath(\theta)|^{1/2}\)</span> yields a prior that is invariant to reparametrization, so that inferences conducted in different parametrizations are equivalent.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<!--
From notes of M. Jordan for MATH 260.
-->
<p>To see this, consider a bijective transformation <span class="math inline">\(\theta \mapsto \vartheta.\)</span> Under the reparametrized model and suitable regularity conditions<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, the chain rule implies that <span class="math display">\[\begin{align*}
i(\vartheta) &amp;= - \mathsf{E} \left(\frac{\partial^2 \ell(\vartheta)}{\partial^2 \vartheta}\right)
\\&amp;= - \mathsf{E}\left(\frac{\partial^2 \ell(\theta)}{\partial \theta^2}\right) \left( \frac{\mathrm{d} \theta}{\mathrm{d} \vartheta} \right)^2 + \mathsf{E}\left(\frac{\partial \ell(\theta)}{\partial \theta}\right) \frac{\mathrm{d}^2 \theta}{\mathrm{d} \vartheta^2}
\end{align*}\]</span> Since the score has mean zero, <span class="math inline">\(\mathsf{E}\left\{\partial \ell(\theta)/\partial \theta\right\}=0\)</span> and the rightmost term vanishes. We can thus relate the Fisher information in both parametrizations, with <span class="math display">\[\begin{align*}
\imath^{1/2}(\vartheta) = \imath^{1/2}(\theta) \left| \frac{\mathrm{d} \theta}{\mathrm{d} \vartheta} \right|,
\end{align*}\]</span> implying invariance.</p>
<p>In multiparameter models, the system isn’t invariant to reparametrization if we consider the determinant of the Fisher information.</p>
</div>
<div id="exm-jeffreysbinom" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.8 (Jeffrey’s prior for the binomial distribution)</strong></span> Consider the binomial distribution <span class="math inline">\(\mathsf{Bin}(1, \theta)\)</span> with density <span class="math inline">\(f(y; \theta) \propto  \theta^y(1-\theta)^{1-y}\mathbf{1}_{\theta \in [0,1]}.\)</span> The negative of the second derivative of the log likelihood with respect to <span class="math inline">\(p\)</span> is <span class="math display">\[\jmath(\theta) = - \partial^2 \ell(\theta; y) / \partial \theta^2 = y/\theta^2 + (1-y)/(1-\theta)^2\]</span> and since <span class="math inline">\(\mathsf{E}(Y)=\theta,\)</span> the Fisher information is <span class="math display">\[\imath(\vartheta) = \mathsf{E}\{\jmath(\theta)\}=1/\theta + 1/(1-\theta) = 1/\{\theta(1-\theta)\}\]</span> Jeffrey’s prior is thus <span class="math inline">\(p(\theta) \propto \theta^{-1/2}(1-\theta)^{-1/2},\)</span> a conjugate Beta prior <span class="math inline">\(\mathsf{beta}(0.5,0.5).\)</span></p>
</div>
<div id="exr-jeffreysnormal" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 3.1 (Jeffrey’s prior for the normal distribution)</strong></span> Check that for the Gaussian distribution <span class="math inline">\(\mathsf{Gauss}(\mu, \sigma^2),\)</span> the Jeffrey’s prior obtained by treating each parameter as fixed in turn, are <span class="math inline">\(p(\mu) \propto 1\)</span> and <span class="math inline">\(p(\sigma) \propto 1/\sigma,\)</span> which also correspond to the default uninformative priors for location-scale families.</p>
</div>
<div id="exm-jeffreyspoisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.9 (Jeffrey’s prior for the Poisson distribution)</strong></span> The Poisson distribution with <span class="math inline">\(\ell(\lambda) \propto -\lambda + y\log \lambda,\)</span> with second derivative <span class="math inline">\(-\partial^2 \ell(\lambda)/\partial \lambda^2 = y/\lambda^2.\)</span> Since the mean of the Poisson distribution is <span class="math inline">\(\lambda,\)</span> the Fisher information is <span class="math inline">\(\imath(\lambda) = \lambda^{-1}\)</span> and Jeffrey’s prior is <span class="math inline">\(\lambda^{-1/2}.\)</span></p>
</div>
</section>
<section id="priors-for-regression-models" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="priors-for-regression-models"><span class="header-section-number">3.4</span> Priors for regression models</h2>
<p>Regression models often feature Gaussian priors on the mean coefficients <span class="math inline">\(\boldsymbol{\beta},\)</span> typically chosen to be vague with large variance. Below are some alternatives, many of which aim to enforce shrinkage towards zero, or sparsity.</p>
<div id="prp-Zellner-g-prior" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3.1 (Zellner’s <span class="math inline">\(g\)</span> prior)</strong></span> Consider an ordinary linear regression model for <span class="math inline">\(\boldsymbol{Y} \sim \mathsf{Gauss}_n(\beta_0\mathbf{1}_n + \mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}_n),\)</span> with intercept <span class="math inline">\(\beta_0\)</span> and mean coefficient vector <span class="math inline">\(\boldsymbol{\beta} = (\beta_1, \ldots, \beta_p)^\top\)</span> associated to the model matrix <span class="math inline">\(\mathbf{X}.\)</span> <span class="citation" data-cites="Zellner:1986">Zellner (<a href="references.html#ref-Zellner:1986" role="doc-biblioref">1986</a>)</span>’s <span class="math inline">\(g\)</span> prior consists in letting <span class="math inline">\(\boldsymbol{\beta} \sim \mathsf{Gauss}_p\{\boldsymbol{0}_p, g \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\},\)</span> where <span class="math inline">\(g&gt;0\)</span> is a constant.</p>
<p>The ordinary least square estimator of the mean coefficients satisfies under regularity conditions on the model matrix <span class="math inline">\(\widehat{\boldsymbol{\beta}} \sim \mathsf{Gauss}_p\{\boldsymbol{\beta}, \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\}\)</span> for Gaussian data, whence we get the closed-form conditional distributions <span class="math display">\[\begin{align*}
\beta_0 \mid \sigma^2, \boldsymbol{Y} &amp;\sim \mathsf{Gauss}(\overline{y}, \sigma^2/n)\\
\boldsymbol{\beta} \mid \beta_0, \sigma^2, \boldsymbol{Y} &amp; \sim \mathsf{Gauss}_p\left\{\frac{g}{g+1} \widehat{\boldsymbol{\beta}}, \frac{g}{g+1}\sigma^2 (\mathbf{X}^\top\mathbf{X})^{-1}\right\}
\end{align*}\]</span> where <span class="math inline">\(\overline{y} = \boldsymbol{y}^\top\mathbf{1}_n/n\)</span> is the sample mean of the observed response vector. We can interpret <span class="math inline">\(g&gt;0\)</span> as a prior weight, with the posterior conditional mean giving weight of <span class="math inline">\(n/g\)</span> to “phantom (prior) observations” with mean zero, relative to the <span class="math inline">\(n\)</span> observations in the observed sample: the ratio <span class="math inline">\(g/(g+1)\)</span> is called <strong>shrinkage factor</strong>.</p>
<p>By virtue of <a href="introduction.html#prp-conditional-gaussian" class="quarto-xref">Proposition&nbsp;<span>1.6</span></a>, the prior is also closed under conditioning, which is useful for model comparison using Bayes factors. Consider a partition <span class="math inline">\(\boldsymbol{\beta} = (\boldsymbol{\beta}_1^\top, \boldsymbol{\beta}_2^\top)^\top\)</span> of the mean coefficients and similarly the block of columns from the model matrix, say <span class="math inline">\(\mathbf{X} = [\mathbf{X}_1\; \mathbf{X}_2]\)</span> for blocks of size <span class="math inline">\(k\)</span> and <span class="math inline">\(p-k.\)</span> If we remove <span class="math inline">\(p-k\)</span> regressors from the model setting <span class="math inline">\(\boldsymbol{\beta}_2=0,\)</span> then the conditional is <span class="math display">\[\boldsymbol{\beta}_{1} \mid \boldsymbol{\beta}_2=\boldsymbol{0}_{p-k} \sim \mathsf{Gauss}_k\{\boldsymbol{0}_k, g\sigma^2 (\mathbf{X}_1^\top\mathbf{X}_1)^{-1}\},\]</span> which is the <span class="math inline">\(g\)</span> prior for the submodel in which we omit the columns corresponding to <span class="math inline">\(\mathbf{X}_2.\)</span></p>
</div>
</section>
<section id="informative-priors" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="informative-priors"><span class="header-section-number">3.5</span> Informative priors</h2>
<p>One strength of the Bayesian approach is the capability of incorporating expert and domain-based knowledge through priors. Often, these will take the form of moment constraints, so one common way to derive a prior is to perform moment matching to related elicited quantities with moments of the prior distribution. It may be easier to set priors on a different scale than those of the observations, as <a href="#exm-colestawn" class="quarto-xref">Example&nbsp;<span>3.10</span></a> demonstrates.</p>
<div id="exm-colestawn" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.10 (Gamma quantile difference priors for extreme value distributions)</strong></span> The generalized extreme value distribution arises as the limiting distribution for the maximum of <span class="math inline">\(m\)</span> independent observations from some common distribution <span class="math inline">\(F.\)</span> The <span class="math inline">\(\mathsf{GEV}(\mu, \sigma, \xi)\)</span> distribution is a location-scale with distribution function <span class="math display">\[\begin{align*}
F(x) = \exp\left[ - \left\{1+\xi(x-\mu)/\sigma\right\}^{-1/\xi}_{+}\right]
\end{align*}\]</span> where <span class="math inline">\(x_{+} = \max\{0, x\}.\)</span></p>
<p>Inverting the distribution function yields the quantile function <span class="math display">\[\begin{align*}
Q(p) \mu + \sigma \frac{(-\log p)^{-\xi}-1}{\xi}
\end{align*}\]</span></p>
<p>In environmental data, we often model annual maximum. Engineering designs are often specified in terms of the <span class="math inline">\(k\)</span>-year return levels, defined as the quantile of the annual maximum exceeded with probability <span class="math inline">\(1/k\)</span> in any given year. Using a <span class="math inline">\(\mathsf{GEV}\)</span> for annual maximum, <span class="citation" data-cites="Coles.Tawn:1996">Coles and Tawn (<a href="references.html#ref-Coles.Tawn:1996" role="doc-biblioref">1996</a>)</span> proposed modelling annual daily rainfall and specifying a prior on the quantile scale <span class="math inline">\(q_1 &lt; q_2 &lt; q_3\)</span> for tail probabilities <span class="math inline">\(p_1&gt; p_2 &gt; p_3.\)</span> To deal with the ordering constraints, gamma priors are imposed on the differences</p>
<ul>
<li><span class="math inline">\(q_1 - o \sim \mathsf{gamma}(\alpha_1, \beta_1),\)</span></li>
<li><span class="math inline">\(q_2 - q_1 \sim \mathsf{gamma}(\alpha_2, \beta_2)\)</span> and</li>
<li><span class="math inline">\(q_3-q_2 \sim \mathsf{gamma}(\alpha_3, \beta_3),\)</span></li>
</ul>
<p>where <span class="math inline">\(o\)</span> is the lower bound of the support. The prior is thus of the form <span class="math display">\[\begin{align*}
p(\boldsymbol{q}) \propto q_1^{\alpha_1-1}\exp(-\beta_1 q_1) \prod_{i=2}^3 (q_i-q_{i-1})^{\alpha_i-1} \exp\{\beta_i(q_i-q_{i-1})\}.
\end{align*}\]</span> where <span class="math inline">\(0 \leq q_1 \leq q_2 \leq q_3.\)</span> The fact that these quantities refer to moments or risk estimates which practitioners often must compute as part of regulatory requirements makes it easier to specify sensible values for hyperparameters.</p>
</div>
<div id="exm-martins" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.11 (Priors in extreme value theory)</strong></span> The generalized extreme value distribution obtained as the limit of maximum of blocks of size <span class="math inline">\(m\)</span> when suitably normalizes is a location-scale family with a shape parameter <span class="math inline">\(\xi \in \mathbb{R}\)</span>. The latter describes the heavyness of the tail, with negative values corresponding to approximation by bounded upper tail distributions (such as the beta), <span class="math inline">\(\xi=0\)</span> to exponential tail decay and <span class="math inline">\(\xi&gt;0\)</span> to polynomial tails, with finite moments of order <span class="math inline">\(1/\xi\)</span>. For example, the Cauchy or Student-<span class="math inline">\(t\)</span> distribution with one degree of freedom has infinite first moment and <span class="math inline">\(\xi=1\)</span>.</p>
<p>In practice, the maximum likelihood estimators do not exist if <span class="math inline">\(\xi &lt; -1\)</span> as the model is nonregular <span class="citation" data-cites="Smith:1985">(<a href="references.html#ref-Smith:1985" role="doc-biblioref">Smith 1985</a>)</span>, and the cumulant of order <span class="math inline">\(k\)</span> exists only if <span class="math inline">\(\xi &gt; -1/k\)</span>; the Fisher information matrix exists only when <span class="math inline">\(\xi &gt; -1/2\)</span>. Thus, informative priors that restrict the range of the shape, may be useful as in environmental applications the shapes would be in the vicinity of zero. <span class="citation" data-cites="Martins.Stedinger:2000">Martins and Stedinger (<a href="references.html#ref-Martins.Stedinger:2000" role="doc-biblioref">2000</a>)</span> proposed a prior of the form <span class="math display">\[\begin{align*}
p(\xi) =\frac{(0.5+\xi)^{p-1}(0.5-\xi)^{q-1}}{\mathrm{beta}(p,q)}, \qquad \xi \in [-0.5, 0.5]
\end{align*}\]</span> a shifted <span class="math inline">\(\mathsf{beta}(p,q)\)</span> prior.</p>
<p>On the contrary, the <strong>maximal data information</strong> (MDI) prior <span class="citation" data-cites="Zellner:1971">(<a href="references.html#ref-Zellner:1971" role="doc-biblioref">Zellner 1971</a>)</span> is defined in terms of entropy, <span class="math display">\[p(\boldsymbol{\theta}) = \exp \mathsf{E}\{\log f(Y \mid \boldsymbol{\theta})\}.\]</span> It is an objective prior that reflects little about the parameter and leads to inferences that have good frequentist property.</p>
<p>For the generalized Pareto distribution, <span class="math inline">\(p(\xi) \propto \exp(-\xi)\)</span>. In this particular case, however, it is improper without modification since <span class="math inline">\(\lim_{\xi \to -\infty} \exp(-\xi) = \infty\)</span>, and the prior density increases without bound as <span class="math inline">\(\xi\)</span> becomes smaller.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-mdiprior" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mdiprior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="priors_files/figure-html/fig-mdiprior-1.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mdiprior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: Unscaled maximum data information (MDI) prior density.
</figcaption>
</figure>
</div>
</div>
</div>
<p>If we restrict the range of the MDI prior <span class="math inline">\(p(\xi)\)</span> to <span class="math inline">\(\xi \geq -1\)</span>, then <span class="math inline">\(p(\xi + 1) \sim \mathsf{expo}(1)\)</span> and the resulting posterior is proper <span class="citation" data-cites="Northrop.Attalides:2016">(<a href="references.html#ref-Northrop.Attalides:2016" role="doc-biblioref">Northrop and Attalides 2016, Zhang.Shaby:2023</a>)</span>. While being “objective”, it is perhaps not much suitable as it puts mass towards lower values of the shape, an undesirable feature.</p>
</div>
<p>What would you do if we you had prior information from different sources? One way to combine these is through a mixture: given <span class="math inline">\(M\)</span> different prior distributions <span class="math inline">\(p_m(\boldsymbol{\theta}),\)</span> we can assign each a positive weight <span class="math inline">\(w_m\)</span> to form a mixture of experts prior through the linear combination <span class="math display">\[ p(\boldsymbol{\theta}) \propto \sum_{m=1}^M w_m p_m(\boldsymbol{\theta})\]</span> <!-- Example of prior elicitation with curve --></p>
<!-- :::{#exm-priors-interest-rates} -->
<!-- Medias often report banking experts forecasts for [interest rates](https://www.cbsnews.com/news/mortgage-interest-rate-forecast-what-experts-predict-for-this-year-and-2024/) or the central bank rates: these could be combined and translated into prior distributions. Note however that, if none of the expert deems a scenario possible and all assign it zero prior probability, it is impossible for the posterior to put any mass there. -->
<!-- ::: -->
<div id="prp-penalized-complexity" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3.2 (Penalized complexity priors)</strong></span> Oftentimes, there will be a natural family of prior density to impose on some model component, <span class="math inline">\(p(\boldsymbol{\theta} \mid \zeta),\)</span> with hyperparameter <span class="math inline">\(\zeta.\)</span> The flexibility of the underlying construction leads itself to overfitting. Penalized complexity priors <span class="citation" data-cites="Simpson:2017">(<a href="references.html#ref-Simpson:2017" role="doc-biblioref">Simpson et al. 2017</a>)</span> aim to palliate this by penalizing models far away from a simple baseline model, which correspond to a fixed value <span class="math inline">\(\zeta_0.\)</span> The prior will favour the simpler parsimonious model the more prior mass one places on <span class="math inline">\(\zeta_0,\)</span> which is in line with Occam’s razor principle.</p>
<p>To construct a penalized-complexity prior, we compute the Kullback–Leibler divergence <span class="citation" data-cites="Simpson:2017">(<a href="references.html#ref-Simpson:2017" role="doc-biblioref">Simpson et al. 2017</a>)</span> or the Wasserstein distance <span class="citation" data-cites="Bolin:2023">(<a href="references.html#ref-Bolin:2023" role="doc-biblioref">Bolin, Simas, and Xiong 2023</a>)</span> between the model <span class="math inline">\(p_\zeta \equiv p(\boldsymbol{\theta} \mid \zeta)\)</span> relative to the baseline with <span class="math inline">\(\zeta_0,\)</span> <span class="math inline">\(p_0 \equiv p(\boldsymbol{\theta} \mid
\zeta_0);\)</span> the distance between the prior densities is then set to <span class="math inline">\(d(\zeta) = \{2\mathsf{KL}(p_\zeta \mid\mid p_0)\}^{1/2},\)</span> where the Kullback–Leibler divergence is <span class="math display">\[\begin{align*}
\mathsf{KL}(p_\zeta \Vert\, p_0)=\int p_\zeta \log\left(\frac{p_\zeta}{p_0}\right) \mathrm{d} \boldsymbol{\theta}.
\end{align*}\]</span> The divergence is zero at the model with <span class="math inline">\(\zeta_0.\)</span> The PC prior then constructs an exponential prior on the distance scale, which after back-transformation gives <span class="math inline">\(p(\zeta \mid \lambda) = \lambda\exp(-\lambda d(\zeta)) \left| {\partial d(\zeta)}/{\partial \zeta}\right|.\)</span> To choose <span class="math inline">\(\lambda,\)</span> the authors recommend elicitation of a pair <span class="math inline">\((Q_\zeta, \alpha)\)</span>, where <span class="math inline">\(Q_\zeta\)</span> is the quantile at level <span class="math inline">\(1-\alpha\)</span>, such that <span class="math inline">\(\Pr(\lambda &gt; Q_\zeta)=\alpha.\)</span></p>
<p>The construction of Wasserstein complexity priors <span class="citation" data-cites="Bolin:2023">(<a href="references.html#ref-Bolin:2023" role="doc-biblioref">Bolin, Simas, and Xiong 2023</a>)</span> is more involved, but those priors are also parametrization-invariant and well-defined even when the Kullback–Leibler divergence limit does not exist.</p>
</div>
<div id="exm-pcprior-randomeffect" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.12 (Penalized complexity prior for random effects models)</strong></span> <span class="citation" data-cites="Bolin:2023">Bolin, Simas, and Xiong (<a href="references.html#ref-Bolin:2023" role="doc-biblioref">2023</a>)</span> consider a Gaussian prior for independent and identically random effects <span class="math inline">\(\boldsymbol{\alpha},\)</span> of the form <span class="math inline">\(\alpha_j \mid \zeta \sim \mathsf{Gauss}(0, \zeta^2)\)</span> where <span class="math inline">\(\zeta_0=0\)</span> corresponds to the absence of random subject-variability. The penalized complexity prior for the scale <span class="math inline">\(\zeta\)</span> is then an exponential with rate <span class="math inline">\(\lambda,\)</span> with density <span class="math display">\[p(\zeta \mid \lambda) = \lambda \exp(-\lambda \zeta).\]</span></p>
<p>We can elicit a high quantile <span class="math inline">\(Q_\zeta\)</span> at tail probability <span class="math inline">\(\alpha\)</span> for the standard deviation parameter <span class="math inline">\(\zeta\)</span> and set <span class="math inline">\(\lambda = -\ln(\alpha/Q_\zeta)\)</span>.</p>
</div>
<div id="exm-pcprior-arorder" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.13 (Penalized complexity prior for autoregressive model of order 1)</strong></span> <span class="citation" data-cites="Sorbye.Holbek.Rue:2017">Sørbye and Rue (<a href="references.html#ref-Sorbye.Holbek.Rue:2017" role="doc-biblioref">2017</a>)</span> derive penalized complexity prior for the Gaussian stationary AR(1) model with autoregressive parameter <span class="math inline">\(\phi \in (-1,1),\)</span> where <span class="math inline">\(Y_t \mid Y_{t-1}, \phi, \sigma^2 \sim \mathsf{Gauss}(\phi Y_{t-1}, \sigma^2).\)</span> There are two based models that could be of interest: one with <span class="math inline">\(\phi=0,\)</span> corresponding to a memoryless model with no autocorrelation, and a static mean <span class="math inline">\(\phi=1\)</span> for no change in time; note that the latter is not stationary. For the former <span class="math inline">\((\phi=0)\)</span>, the penalized complexity prior is <span class="math display">\[\begin{align*}
p(\phi \mid \lambda) = \frac{\lambda}{2} \exp\left[-\lambda \left\{-\ln(1-\phi^2)\right\}^{1/2}\right] \frac{|\phi|}{(1-\phi^2)\left\{-\ln(1-\phi^2)\right\}^{1/2}}.
\end{align*}\]</span> One can set <span class="math inline">\(\lambda\)</span> by considering plausible values by relating the parameter to the variance of the one-step ahead forecast error.</p>
</div>
<div id="rem-raneff" class="proof remark">
<p><span class="proof-title"><em>Remark 3.1</em> (Variance parameters in hierarchical models). </span>Gaussian components are widespread: not only for linear regression models, but more generally for the specification of random effects that capture group-specific effects, residuals spatial or temporal variability. In the Bayesian paradigm, there is no difference between fixed effects <span class="math inline">\(\boldsymbol{\beta}\)</span> and the random effect parameters: both are random quantities that get assigned priors, but we will treat these priors differently.</p>
<p>The reason why we would like to use a penalized complexity prior for a random effect, say <span class="math inline">\(\alpha_j \sim \mathsf{Gauss}(0, \zeta^2),\)</span> is because we don’t know a prior if there is variability between groups. The inverse gamma prior for <span class="math inline">\(\zeta,\)</span> <span class="math inline">\(\zeta \sim \mathsf{InvGamma}(\epsilon, \epsilon)\)</span> does not have a mode at zero unless it is improper with <span class="math inline">\(\epsilon \to 0.\)</span> Generally, we want our prior for the variance to have significant probability density at the null <span class="math inline">\(\zeta=0.\)</span> The penalized complexity prior is not the only sensible choice. Posterior inference is unfortunately sensitive to the value of <span class="math inline">\(\epsilon\)</span> in hierarchical models when the random effect variance is close to zero, and more so when there are few levels for the groups since the relative weight of the prior relative to that of the likelihood contribution is then large.</p>
</div>
<div id="exm-random-effect-variance" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.14 (Student-t prior for variance components)</strong></span> <span class="citation" data-cites="Gelman:2006">Gelman (<a href="references.html#ref-Gelman:2006" role="doc-biblioref">2006</a>)</span> recommends a Student-<span class="math inline">\(t\)</span> distribution truncated below at <span class="math inline">\(0,\)</span> with low degrees of freedom. The rationale for this choice comes from the simple two level model with <span class="math inline">\(n_j\)</span> independent in each group <span class="math inline">\(j=1, \ldots, J\)</span>: for observation <span class="math inline">\(i\)</span> in group <span class="math inline">\(j,\)</span> <span class="math display">\[\begin{align*}
Y_{ij} &amp;\sim \mathsf{Gauss}(\mu + \alpha_j, \sigma^2),\\
\alpha_j &amp;\sim \mathsf{Gauss}(0, \tau^2_\alpha),
\end{align*}\]</span> The conditionally conjugate prior <span class="math inline">\(p(\tau \mid \boldsymbol{\alpha}, \mu, \sigma)\)</span> is inverse gamma. Standard inference with this parametrization is however complicated, because there is strong dependence between parameters.</p>
<p>To reduce this dependence, one can add a parameter, taking <span class="math inline">\(\alpha_j = \xi \eta_j\)</span> and <span class="math inline">\(\tau_\alpha=|\xi|\tau_{\eta}\)</span>; the model is now overparametrized. Suppose <span class="math inline">\(\eta_j \sim \mathsf{Gauss}(0, \tau^2_\eta)\)</span> and consider the likelihood conditional on <span class="math inline">\(\mu, \eta_j\)</span>: we have that <span class="math inline">\((y_{ij} - \mu)/\eta_j \sim \mathsf{Gauss}(\xi, \sigma^2/\eta_j)\)</span> so conditionally conjugate priors for <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\tau_\eta\)</span> are respectively Gaussian and inverse gamma. This translates into a prior distribution for <span class="math inline">\(\tau_\alpha\)</span> which is that of the absolute value of a noncentral Student-<span class="math inline">\(t\)</span> with location, scale and degrees of freedom <span class="math inline">\(\nu.\)</span> If we set the location to zero, the prior puts high mass at the origin, but is heavy tailed with polynomial decay. We recommend to set degrees of freedom so that the variance is heavy-tailed, e.g., <span class="math inline">\(\nu=3.\)</span> While this prior is not conjugate, it compares favorably to the <span class="math inline">\(\mathsf{inv. gamma}(\epsilon, \epsilon).\)</span></p>
</div>
<div id="exm-randomeffects" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.15 (Poisson random effect models)</strong></span> We consider data from an experimental study conducted at Tech3Lab on road safety. In <span class="citation" data-cites="Brodeur:2021">Brodeur et al. (<a href="references.html#ref-Brodeur:2021" role="doc-biblioref">2021</a>)</span>, 31 participants were asked to drive in a virtual environment; the number of road violation was measured for different type of distractions (phone notification, phone on speaker, texting and smartwatch). The data are balanced, with each participant exposed to each task exactly once.</p>
<p>We model the data using a Poisson mixed model to measure the number of violations, <code>nviolation</code>, with a fixed effect for <code>task</code>, which captures the type of distraction, and a random effect for participant <code>id</code>. The hierarchical model fitted for individual <span class="math inline">\(i\)</span> <span class="math inline">\((i=1, \ldots, 34)\)</span> and distraction type <span class="math inline">\(j\)</span> <span class="math inline">\((j=1, \ldots, 4)\)</span> is <span class="math display">\[\begin{align*}
Y_{ij} &amp;\sim \mathsf{Poisson}\{\mu = \exp(\beta_{j} + \alpha_i)\},\\
\beta_j &amp;\sim \mathsf{Gauss}(0, 100), \\
\alpha_i &amp;\sim \mathsf{Gauss}(0, \kappa^2), \\
\kappa &amp;\sim \mathsf{Student}_{+}(0,1,3).
\end{align*}\]</span> so observations are conditionally independent given hyperparameters <span class="math inline">\(\boldsymbol{\alpha}\)</span> and <span class="math inline">\(\boldsymbol{\beta}.\)</span></p>
<p>In frequentist statistics, there is a distinction made in mixed-effect models between parameters that are treated as constants, termed fixed effects and corresponding in this example to <span class="math inline">\(\boldsymbol{\beta},\)</span> and random effects, equivalent to <span class="math inline">\(\boldsymbol{\alpha}.\)</span> There is no such distinction in the Bayesian paradigm, except perhaps for the choice of prior.</p>
<p>We can look at some of posterior distribution of the 31 random effects (here the first five individuals) and the fixed effect parameters <span class="math inline">\(\boldsymbol{\beta},\)</span> plus the variance of the random effect <span class="math inline">\(\kappa\)</span>: there is strong evidence that the latter is non-zero, suggesting strong heterogeneity between individuals. The distraction which results in the largest number of violation is texting, while the other conditions all seem equally distracting on average (note that there is no control group with no distraction to compare with, so it is hard to draw conclusions).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-post-dist-poisson-mixed" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post-dist-poisson-mixed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="priors_files/figure-html/fig-post-dist-poisson-mixed-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post-dist-poisson-mixed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Posterior density plots with 50% credible intervals and median value for the random effects of the first five individuals (left) and the fixed effects and random effect variance (right).
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="sensitivity-analysis" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="sensitivity-analysis"><span class="header-section-number">3.6</span> Sensitivity analysis</h2>
<p>Do priors matter? The answer to that question depends strongly on the model, and how much information the data provides about hyperparameters. While this question is easily answered in conjugate models (the relative weight of hyperparameters relative to data can be derived from the posterior parameters), it is not so simple in hierarchical models, where the interplay between prior distributions is often more intricate. To see the impact, one often has to rely on doing several analyses with different values fr the prior and see the sensitivity of the conclusions to these changes, for example by considering a vague prior or modifying the parameters values (say halving or doubling). If the changes are immaterial, then this provides reassurance that our analyses are robust.</p>
<div id="exm-sensitivity-poisson-mixed" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.16</strong></span> To check the sensitivity of the conclusion, we revisit the modelling of the <code>smartwatch</code> experiment data using a Poisson regression and compare four priors: a uniform prior truncated to <span class="math inline">\([0, 10],\)</span> an inverse gamma <span class="math inline">\(\mathsf{InvGamma}(0.01, 0.01)\)</span> prior, a penalized complexity prior such that the 0.95 percentile of the scale is 5, corresponding to <span class="math inline">\(\mathsf{Exp}(0.6).\)</span> Since each distraction type appears 31 times, there is plenty of information to reliably estimate the dispersion <span class="math inline">\(\kappa\)</span> of the random effects <span class="math inline">\(\alpha\)</span>: the different density plots in <a href="#fig-sensitivity" class="quarto-xref">Figure&nbsp;<span>3.6</span></a> are virtually indistinguishable from one another. This is perhaps unsurprising given the large number of replicates, and the significant variability between groups.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-sensitivity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sensitivity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="priors_files/figure-html/fig-sensitivity-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sensitivity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: Posterior density of the scale of the random effects with uniform, inverse gamma, penalized complexity and folded Student-t with three degrees of freedom. The circle denotes the median and the bars the 50% and 95% percentile credible intervals.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-rainfall-abisko" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.17 (Extreme rainfall in Abisko, Sweden)</strong></span> As illustrating example, consider maximum daily cumulated rainfall in Abisko, Sweden. The time series spans from 1913 until December 2014; we compute the 102 yearly maximum, which range from 11mm to 62mm, and fit a generalized extreme value distribution to these.</p>
<p>For the priors, suppose an expert elicits quantiles of the 10, 50 and 100 years return levels; say 30mm, 45mm and 70mm, respectively, for the median and likewise 40mm, 70mm and 120mm for the 90% percentile of the return levels. We can compute the differences and calculate the parameters of the gamma distribution through moment-matching: this gives roughly a shape of <span class="math inline">\(\alpha_1=18.27\)</span> and <span class="math inline">\(\beta_1=0.6,\)</span> etc. <a href="#fig-gev-colestawn-quant-prior" class="quarto-xref">Figure&nbsp;<span>3.7</span></a> shows the transfer from the prior predictive to the posterior distribution. The prior is much more dispersed and concentrated on the tail, which translates in a less peaked posterior than using a weakly informative prior (dotted line): the mode of the latter is slightly to the left and with lower density in the tail.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-gev-colestawn-quant-prior" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gev-colestawn-quant-prior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="priors_files/figure-html/fig-gev-colestawn-quant-prior-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gev-colestawn-quant-prior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.7: Kernel density estimates of draws from the posterior distribution of 100 year return levels with a Coles–Tawn quantile prior (full line) and from the corresponding prior predictive (dashed). The dotted line gives the posterior distribution for a maximum domain information prior on the shape with improper priors on location and scale.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<strong>Summary</strong>:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Priors are distributions for the parameters. In multi-parameter models, they can be specified through a joint distribution or assumed independent apriori (which does not translate into independence a posteriori).</li>
<li>Priors are not invariant to reparametrization, except when they are constructed with this property (e.g., Jeffrey’s prior or penalized-complexity priors).</li>
<li>Improper priors may lead to improper posterior.</li>
<li>Priors that restrict the domain of <span class="math inline">\(\boldsymbol{\theta}\)</span> will also restrict the posterior. These are useful to avoid regions that are implausible or impossible.</li>
<li>Physical knowledge of the system can be helpful to specify sensible values of the prior through moment matching.</li>
<li>Conjugate priors facilitate derivations, but are mostly chosen for convenience.</li>
<li>Generally, the prior has constant weight <span class="math inline">\(\mathrm{O}(1)\)</span>, relative to <span class="math inline">\(\mathrm{O}(n)\)</span> for the likelihood. The posterior is thus dominated in most circumstances by the likelihood.</li>
<li>We can compute the prior to posterior gain by comparing their density (if the prior is proper).</li>
<li>For many (conjugate) priors, we can view some function of the parameter as given a prior number of observations (in Gaussian models, binomial, gamma, etc.)</li>
<li>Informative priors can be used to specify expert knowledge about the system. This will impact the posterior, but often in a sensible manner, thereby regularizing or improving posterior inference.</li>
</ul>
</div>
</div>
<!--

Hierarchical linear model with half-t prior

Prior elicitation may require [expert knowledge](https://arxiv.org/abs/2112.01380).


Quantile priors of [Coles and Tawn](http://www.jstor.org/stable/2986068) (using `revdbayes`)


Are my priors reasonable? Use prior predictive distribution to assess the plausibility
comparing prior to posterior standard deviations, e.g., Nott et al. (2020)

Example: simple linear regression slope (height/weight) of Figure 4.5 in McElreath


Improper priors may lead to improper posterior: stick with proper distributions unless you know what you are doing

Penalized complexity prior

Maximum domain information

Sensitivity analysis and asymptotic effect

Consensus of opinion: expert opinion and mixture

[Black--Litterman model](https://hudsonthames.org/bayesian-portfolio-optimisation-the-black-litterman-model/)



-->
<!-- Fit to data and compare results with that of the reference improper prior $p(\phi) \propto (1-\phi^2)^{-1/2}.$ 
https://haakonbakkagit.github.io/btopic115.html
-->
<!-- @Zellner:1986 -->
<!-- Zellner's $g$-prior for model selection -->


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Alexander:2023" class="csl-entry" role="listitem">
Alexander, Rohan. 2023. <em>Telling Stories with Data: With Applications in <span>R</span></em>. Boca Raton, FL: CRC Press.
</div>
<div id="ref-Bolin:2023" class="csl-entry" role="listitem">
Bolin, David, Alexandre B. Simas, and Zhen Xiong. 2023. <span>“<span>W</span>asserstein Complexity Penalization Priors: A New Class of Penalizing Complexity Priors.”</span> <em>arXiv e-Prints</em>, arXiv:2312.04481. <a href="https://doi.org/10.48550/arXiv.2312.04481">https://doi.org/10.48550/arXiv.2312.04481</a>.
</div>
<div id="ref-Brodeur:2021" class="csl-entry" role="listitem">
Brodeur, Mathieu, Perrine Ruer, Pierre-Majorique Léger, and Sylvain Sénécal. 2021. <span>“Smartwatches Are More Distracting Than Mobile Phones While Driving: Results from an Experimental Study.”</span> <em>Accident Analysis &amp; Prevention</em> 149: 105846. <a href="https://doi.org/10.1016/j.aap.2020.105846">https://doi.org/10.1016/j.aap.2020.105846</a>.
</div>
<div id="ref-Coles.Tawn:1996" class="csl-entry" role="listitem">
Coles, Stuart G., and Jonathan A. Tawn. 1996. <span>“A <span>B</span>ayesian Analysis of Extreme Rainfall Data.”</span> <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 45 (4): 463–78. <a href="https://doi.org/10.2307/2986068">https://doi.org/10.2307/2986068</a>.
</div>
<div id="ref-Gelman:2006" class="csl-entry" role="listitem">
Gelman, Andrew. 2006. <span>“Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by <span>B</span>rowne and <span>D</span>raper).”</span> <em>Bayesian Analysis</em> 1 (3): 515–34. <a href="https://doi.org/10.1214/06-ba117a">https://doi.org/10.1214/06-ba117a</a>.
</div>
<div id="ref-Gelman:2013" class="csl-entry" role="listitem">
Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. 3rd ed. New York: Chapman; Hall/CRC. <a href="https://doi.org/10.1201/b16018">https://doi.org/10.1201/b16018</a>.
</div>
<div id="ref-Martins.Stedinger:2000" class="csl-entry" role="listitem">
Martins, Eduardo S., and Jery R. Stedinger. 2000. <span>“Generalized Maximum-Likelihood Generalized Extreme-Value Quantile Estimators for Hydrologic Data.”</span> <em>Water Resources Research</em> 36 (3): 737–44. <a href="https://doi.org/10.1029/1999WR900330">https://doi.org/10.1029/1999WR900330</a>.
</div>
<div id="ref-Matias:2021" class="csl-entry" role="listitem">
Matias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles Ebersole. 2021. <span>“The <span>U</span>pworthy <span>R</span>esearch <span>A</span>rchive, a Time Series of 32,487 Experiments in <span>U.S.</span> Media.”</span> <em>Scientific Data</em> 8 (195). <a href="https://doi.org/10.1038/s41597-021-00934-7">https://doi.org/10.1038/s41597-021-00934-7</a>.
</div>
<div id="ref-Northrop.Attalides:2016" class="csl-entry" role="listitem">
Northrop, Paul J., and Nicolas Attalides. 2016. <span>“Posterior Propriety in <span>B</span>ayesian Extreme Value Analyses Using Reference Priors.”</span> <em>Statistica Sinica</em> 26 (2): 721–43. <a href="https://doi.org/10.5705/ss.2014.034">https://doi.org/10.5705/ss.2014.034</a>.
</div>
<div id="ref-Simpson:2017" class="csl-entry" role="listitem">
Simpson, Daniel, Håvard Rue, Andrea Riebler, Thiago G. Martins, and Sigrunn H. Sørbye. 2017. <span>“Penalising Model Component Complexity: A Principled, Practical Approach to Constructing Priors.”</span> <em>Statistical Science</em> 32 (1): 1–28. <a href="https://doi.org/10.1214/16-sts576">https://doi.org/10.1214/16-sts576</a>.
</div>
<div id="ref-Smith:1985" class="csl-entry" role="listitem">
Smith, Richard L. 1985. <span>“Maximum Likelihood Estimation in a Class of Nonregular Cases.”</span> <em>Biometrika</em> 72 (1): 67–90. <a href="https://doi.org/10.1093/biomet/72.1.67">https://doi.org/10.1093/biomet/72.1.67</a>.
</div>
<div id="ref-Sorbye.Holbek.Rue:2017" class="csl-entry" role="listitem">
Sørbye, Sigrunn Holbek, and Håvard Rue. 2017. <span>“Penalised Complexity Priors for Stationary Autoregressive Processes.”</span> <em>Journal of Time Series Analysis</em> 38 (6): 923–35. <a href="https://doi.org/10.1111/jtsa.12242">https://doi.org/10.1111/jtsa.12242</a>.
</div>
<div id="ref-Zellner:1971" class="csl-entry" role="listitem">
Zellner, Arnold. 1971. <em>An Introduction to <span>B</span>ayesian Inference in Econometrics</em>. Wiley.
</div>
<div id="ref-Zellner:1986" class="csl-entry" role="listitem">
———. 1986. <span>“On Assessing Prior Distributions and <span>B</span>ayesian Regression Analysis with <span class="math inline">\(g\)</span>-Prior Distributions.”</span> In <em><span>B</span>ayesian Inference and Decision Techniques: Essays in Honor of <span>B</span>runo de <span>F</span>inetti</em>, 233–43. North-Holland/Elsevier.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>If counts are Poisson, then the log transform is variance stabilizing.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>One can object to the prior parameters depending on the data, but an alternative would be to model centered data <span class="math inline">\(y-\overline{y},\)</span> in which case the prior for the intercept parameter <span class="math inline">\(\beta_0\)</span> would be zero.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The stopping rule means that data stops being collected once there is enough evidence to determine if an option is more suitable, or if a predetermined number of views has been reached.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The Fisher information is linear in the sample size for independent and identically distributed data so we can derive the result for <span class="math inline">\(n=1\)</span> without loss of generality.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Using Bartlett’s identity; Fisher consistency can be established using the dominated convergence theorem.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lbelzile\.github\.io\/MATH80601A\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./bayesics.html" class="pagination-link" aria-label="Bayesics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./montecarlo.html" class="pagination-link" aria-label="Monte Carlo methods">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Monte Carlo methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2024-2025, Léo Belzile</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/lbelzile/MATH80601A/edit/main/priors.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>