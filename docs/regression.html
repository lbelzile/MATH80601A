<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.3">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>8&nbsp; Regression models – Bayesian modelling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./laplace.html" rel="next">
<link href="./workflow.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-81b5c3e63835cfde897ecd3d35a35a41.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ad5ef081d17d57ab2b6cc5e45efb5d90.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regression.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regression models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/MATH80601A" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH80601A-bayesmod.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./montecarlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Monte Carlo methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Metropolis–Hastings algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gibbs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Gibbs sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Computational strategies and diagnostics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regression models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./laplace.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Deterministic approximations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#shrinkage-priors" id="toc-shrinkage-priors" class="nav-link active" data-scroll-target="#shrinkage-priors"><span class="header-section-number">8.1</span> Shrinkage priors</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/lbelzile/MATH80601A/edit/main/regression.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><h1 class="title display-7"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regression models</span></h1></header>

<header id="title-block-header">


</header>


<p>This chapter is dedicated to the study of regression models from a Bayesian standpoint. Starting with Gaussian data, we investigate the link between frequentist approaches to regularization and shrinkage priors. We also look at hierarchical models with mixed effects and variable selection using reversible jump MCMC and conditional Bayes factor.</p>
<p>Throughout, we consider regression models with model (or design) matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> with centered inputs, so <span class="math inline">\(\mathbf{1}_n^\top\mathbf{X}=\mathbf{0}_p.\)</span> We are interested in the associated vector of regression coefficients <span class="math inline">\(\boldsymbol{\beta} = (\beta_1, \ldots, \beta_p)^\top\)</span> which describe the mean and act as weights for each covariate vector. In the ordinary linear regression model <span class="math display">\[\begin{align*}
\boldsymbol{Y} \mid \mathbf{X}, \boldsymbol{\beta}, \omega \sim \mathsf{Gauss}_n(\beta_0\mathbf{1}_n + \mathbf{X}\boldsymbol{\beta}, \omega^{-1}\mathbf{I}_n),
\end{align*}\]</span> so that observations are independent and homoscedastic. Inference is performed conditional on the observed covariate vectors <span class="math inline">\(\mathbf{X}_i\)</span>; we omit this dependence hereafter, but note that this can be relaxed. The intercept <span class="math inline">\(\beta_0\)</span>, which is added to capture the mean response and make it mean-zero, receives special treatment and is typically assigned an improper prior. We largely follow the exposition of <span class="citation" data-cites="Villani:2023">Villani (<a href="references.html#ref-Villani:2023" role="doc-biblioref">2023</a>)</span>.</p>
<!--

:::{#prp-conjugate-bayesian-linmod}

## Bayesian linear model

Consider a linear regression model with observation-specific mean $\mu_i = \mathbf{x}_i\boldsymbol{\beta}$ $(i=1,\ldots, n)$ with $\mathbf{x}_i$ the $i$th row of the $n \times p$ model matrix $\mathbf{X}.$

Concatenating records, $\boldsymbol{Y} \sim \mathsf{Gauss}_n(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{Q}_y^{-1}),$ for a known precision matrix $\mathbf{Q}_y,$ typically $\mathbf{I}_n.$ To construct a conjugate joint prior for $p(\boldsymbol{\beta}, \sigma^2),$ we consider the sequential formulation
\begin{align*}
\boldsymbol{\beta} \mid \sigma^2 \sim \mathsf{Gauss}_p(\boldsymbol{\nu}_\beta, \sigma^2 \mathbf{Q}^{-1}_\beta), \qquad \sigma^2 \sim \mathsf{inv. gamma}(\alpha,\beta)
\end{align*}
where $\mathsf{inv. gamma}$ denotes the inverse gamma distribution^[This simply means that the precision $\sigma^{-2},$ the reciprocal of the variance, has a gamma distribution with shape $\alpha$ and rate $\beta.$]

<!--
Writing the log likelihood in exponential family form,
\begin{align*}
\ell(\mu, \sigma^2; \boldsymbol{y}) \propto -n \log(\sigma) - \frac{\boldsymbol{y}^\top\boldsymbol{y}}{2\sigma^2} + \frac{\mu}{\sigma^2}\boldsymbol{y}^\top \boldsymbol{1}_n - \frac{n}{2}\frac{\mu^2}{\sigma^2}
\end{align*}

-->
<p>The joint posterior is Gaussian-inverse gamma and can be factorized <span class="math display">\[\begin{align*}
p(\boldsymbol{\beta}, \sigma^2 \mid y) = p(\sigma^2 \mid y) p(\boldsymbol{\beta} \mid \sigma^2, y)
\end{align*}\]</span> where <span class="math inline">\(p(\sigma^2 \mid y) \sim \mathsf{inv. gamma}(\alpha^*, \beta^*)\)</span> and <span class="math display">\[\begin{align*}
\boldsymbol{\beta} \mid \sigma^2, y &amp; \sim \mathsf{Gauss}_p(\mathbf{M}\boldsymbol{m}, \sigma^2\mathbf{M})
\\
\alpha^* &amp;= \alpha + n/2,\\
\beta^* &amp;=\beta + 0.5 \boldsymbol{\nu}_\beta^\top \mathbf{Q}_\beta\boldsymbol{\nu}_\beta + \boldsymbol{y}^\top\boldsymbol{y} - \boldsymbol{m}^\top\mathbf{M}\boldsymbol{m}, \\
\boldsymbol{m} &amp;= \mathbf{Q}_\beta \boldsymbol{\nu}_\beta + \mathbf{X}^\top \mathbf{Q}_y\boldsymbol{y}\\
\mathbf{M} &amp;= (\mathbf{Q}_\beta + \mathbf{X}^\top\mathbf{Q}_y\mathbf{X})^{-1};
\end{align*}\]</span> the latter can be evaluated efficiently using Shermann–Morrisson–Woodbury identity. Given the conditionally conjugate priors, we can easily sample from the posterior using Gibbs sampling.</p>
<p>:::</p>
<p>–&gt;</p>
<div id="prp-gaussian-ols" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 8.1 (Gaussian ordinary linear regression with conjugate priors)</strong></span> The conjugate prior for the Gaussian regression model for the mean and precision parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\omega\)</span>, respectively, is a Gaussian-gamma and is defined hierarchically as <span class="math display">\[\begin{align*}
\boldsymbol{\beta} \mid \omega &amp;\sim \mathsf{Gauss}\left\{\boldsymbol{\mu}_0, (
\omega\boldsymbol{\Omega}_0)^{-1}\right\} \\
\omega &amp;\sim \mathsf{gamma}(\nu_0/2,\tau_0/2).
\end{align*}\]</span> Using properties of the Gaussian distribution, the sampling distribution of the ordinary least squares estimator is <span class="math inline">\(\widehat{\boldsymbol{\beta}} \sim \mathsf{Gauss}_p\{\boldsymbol{\beta}, (\omega\mathbf{X}^\top\mathbf{X})^{-1}\}\)</span>.</p>
<p>The conditional and marginal posterior distributions for the mean coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> and for the precision <span class="math inline">\(\omega\)</span> are <span class="math display">\[\begin{align*}
\boldsymbol{\beta} \mid \omega, \boldsymbol{y} &amp;\sim \mathsf{Gauss}_p\left\{\boldsymbol{\mu}_n, (\omega\boldsymbol{\Omega}_n)^{-1}\right\}  \\
\omega \mid  \boldsymbol{y} &amp;\sim \mathsf{gamma}\left\{(\nu_0 + n)/2,  \tau^2_n/2\right\}, \\
\boldsymbol{\beta} \mid  \boldsymbol{y} &amp;\sim \mathsf{Student}_p(\boldsymbol{\mu}_n,  \tau_n/(\nu_0+n) \times \mathbf{\Omega}_n^{-1}, \nu_0 + n)
\end{align*}\]</span> where <span class="math display">\[\begin{align*}
\boldsymbol{\Omega}_n &amp;= \mathbf{X}^\top\mathbf{X} + \boldsymbol{\Omega}_0\\
\boldsymbol{\mu}_n &amp;= \boldsymbol{\Omega}_n^{-1}(\mathbf{X}^\top\mathbf{X}\widehat{\boldsymbol{\beta}} + \boldsymbol{\Omega}_0\boldsymbol{\mu}_0) = \boldsymbol{\Omega}_n^{-1}(\mathbf{X}^\top\boldsymbol{y} + \boldsymbol{\Omega}_0\boldsymbol{\mu}_0)\\
\tau_n &amp;= \tau_0 + (\boldsymbol{y} - \mathbf{X}\widehat{\boldsymbol{\beta}})^\top(\boldsymbol{y} - \mathbf{X}\widehat{\boldsymbol{\beta}}) + (\boldsymbol{\mu}_n - \widehat{\boldsymbol{\beta}})^\top \mathbf{X}^\top\mathbf{X}(\boldsymbol{\mu}_n - \widehat{\boldsymbol{\beta}}) \\&amp; \quad + (\boldsymbol{\mu}_n-\boldsymbol{\mu}_0)^\top\boldsymbol{\Omega}_0(\boldsymbol{\mu}_n-\boldsymbol{\mu}_0)
\end{align*}\]</span></p>
</div>
<div id="prp-quadratic-forms" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 8.2 (Decomposition of quadratic forms)</strong></span> For quadratic forms (in <span class="math inline">\(\boldsymbol{x}\)</span>) with <span class="math display">\[\begin{align*}
&amp; (\boldsymbol{x} - \boldsymbol{a})^\top \mathbf{A}(\boldsymbol{x} - \boldsymbol{a}) + (\boldsymbol{x} - \boldsymbol{b})^\top \mathbf{B}(\boldsymbol{x} - \boldsymbol{b}) \\\quad &amp;=
(\boldsymbol{x} - \boldsymbol{c})^\top \mathbf{C}(\boldsymbol{x} - \boldsymbol{c}) + (\boldsymbol{c}-\boldsymbol{a})^\top\mathbf{A}(\boldsymbol{c}-\boldsymbol{a}) + (\boldsymbol{c}-\boldsymbol{b})^\top\mathbf{B}(\boldsymbol{c}-\boldsymbol{b})\\
&amp;\stackrel{\boldsymbol{x}}{\propto} (\boldsymbol{x} - \boldsymbol{c})^\top \mathbf{C}(\boldsymbol{x} - \boldsymbol{c})
\end{align*}\]</span> where <span class="math inline">\(\mathbf{C} = \mathbf{A} + \mathbf{B}\)</span> and <span class="math inline">\(\boldsymbol{c}= \mathbf{C}^{-1}(\mathbf{A}\boldsymbol{a} + \mathbf{B}\boldsymbol{b})\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The improper prior <span class="math inline">\(p(\boldsymbol{\beta}, \sigma^2) \propto \sigma^{-2}\)</span> can be viewed as a special case of the conjugate prior when the variance of the Gaussian is infinite and the <span class="math inline">\(\mathsf{gamma}(a,b)\)</span> when both <span class="math inline">\(a, b \to 0.\)</span></p>
<p>Write the posterior as <span class="math display">\[\begin{align*}
p(\boldsymbol{\beta}, \omega \mid \boldsymbol{y}) &amp;\propto p(\boldsymbol{y} \mid \boldsymbol{\beta}, \omega) p(\omega)
\\&amp; \propto  \omega^{n/2} \exp\left\{-\frac{\omega}{2}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\right\}\\&amp; \times |\omega\boldsymbol{\Omega}_0|^{1/2}\exp \left\{ -\frac{\omega}{2} (\boldsymbol{\beta}-\boldsymbol{\mu}_0)^\top\boldsymbol{\Omega}_0(\boldsymbol{\beta}-\boldsymbol{\mu}_0)\right\} \\&amp; \times \omega^{\nu_0/2-1}\exp\left(-\tau_0\omega/2\right).
\end{align*}\]</span> We rewrite the first quadratic form in <span class="math inline">\(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}\)</span> using the orthogonal decomposition <span class="math display">\[\begin{align*}
(\boldsymbol{y}-\mathbf{X}\widehat{\boldsymbol{\beta}}) + (\mathbf{X}\widehat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta})
\end{align*}\]</span> since <span class="math inline">\((\boldsymbol{y}-\mathbf{X}\widehat{\boldsymbol{\beta}})^\top (\mathbf{X}\widehat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta}) = 0.\)</span> We can pull together terms and separate the conditional posterior <span class="math inline">\(p(\boldsymbol{\beta} \mid \boldsymbol{y}, \omega)\)</span> and <span class="math inline">\(p(\omega \mid \boldsymbol{y})\)</span> as <span class="math display">\[\begin{align*}
p(\boldsymbol{\beta}, \omega \mid \boldsymbol{y}) &amp;\propto \omega^{(n+p+\nu_0)/2 -1} \exp\left[-\frac{\omega}{2}\left\{\tau_0 + (\boldsymbol{y}-\mathbf{X}\widehat{\boldsymbol{\beta}})^\top(\boldsymbol{y}-\mathbf{X}\widehat{\boldsymbol{\beta}})\right\}\right]
\\&amp; \times \exp \left[-\frac{\omega}{2}\left\{(\widehat{\boldsymbol{\beta}} - \boldsymbol{\beta})^\top\mathbf{X}^\top\mathbf{X}(\widehat{\boldsymbol{\beta}} - \boldsymbol{\beta})+ (\boldsymbol{\beta}-\boldsymbol{\mu}_0)^\top\boldsymbol{\Omega}_0(\boldsymbol{\beta}-\boldsymbol{\mu}_0)\right\}\right]
\end{align*}\]</span> and using <a href="#prp-quadratic-forms" class="quarto-xref">Proposition&nbsp;<span>8.2</span></a> for the terms in the exponent with <span class="math inline">\(\boldsymbol{a} = \widehat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\mathbf{A}=\mathbf{X}^\top\mathbf{X}\)</span>, <span class="math inline">\(\boldsymbol{b} = \boldsymbol{\mu}_0\)</span> and <span class="math inline">\(\mathbf{B}=\boldsymbol{\Omega}_0\)</span>, we find <span class="math display">\[\begin{align*}
  p(\boldsymbol{\beta}, \omega \mid \boldsymbol{y}) &amp; \propto
   \omega^{(n+\nu_0)/2 -1} \exp\left(-\frac{\omega\tau_n}{2}\right)
   \\&amp; \times \omega^{p/2}\exp\left\{-\frac{1}{2}(\boldsymbol{\beta} - \boldsymbol{\mu}_n)^\top(\omega\mathbf{\Omega}_n)(\boldsymbol{\beta} - \boldsymbol{\mu}_n)\right\}
\end{align*}\]</span> whence the decomposition of the posterior as a Gaussian conditional on the precision, and a gamma for the latter. The marginal of <span class="math inline">\(\boldsymbol{\beta}\)</span> is obtained by regrouping all terms that depend on <span class="math inline">\(\omega\)</span> and integrating over the latter, recognizing the integral as an unnormalized gamma density, and thus <span class="math display">\[\begin{align*}
p(\boldsymbol{\beta}, \mid \boldsymbol{y}) &amp; \stackrel{\boldsymbol{\beta}}{\propto} \int_0^\infty \omega^{(\nu_0 + n + p)/2 -1}\exp\left\{- \omega \frac{\tau_n +(\boldsymbol{\beta} - \boldsymbol{\mu}_n)^\top\mathbf{\Omega}_n(\boldsymbol{\beta} - \boldsymbol{\mu}_n)}{2}\right\} \mathrm{d} \omega
\\&amp;\stackrel{\boldsymbol{\beta}}{\propto} \left\{\frac{\tau_n +(\boldsymbol{\beta} - \boldsymbol{\mu}_n)^\top\mathbf{\Omega}_n(\boldsymbol{\beta} - \boldsymbol{\mu}_n)}{2}\right\}^{-(\nu_0 + n + p)/2}
\\&amp; \stackrel{\boldsymbol{\beta}}{\propto} \left\{1 + \frac{(\boldsymbol{\beta} - \boldsymbol{\mu}_n)^\top\frac{\nu_0 + n}{\tau_n}\mathbf{\Omega}_n(\boldsymbol{\beta} - \boldsymbol{\mu}_n)}{\nu_0 + n}\right\}^{-(\nu_0 + n + p)/2}
\end{align*}\]</span> so must be a Student-<span class="math inline">\(t\)</span> distribution with location <span class="math inline">\(\boldsymbol{\mu}_n\)</span>, scale matrix <span class="math inline">\(\tau_n/(\nu_0+n) \times \mathbf{\Omega}_n^{-1}\)</span> and <span class="math inline">\(\nu_0+n\)</span> degrees of freedom.</p>
</div>
<p>The choice of prior precision <span class="math inline">\(\boldsymbol{\Omega}_0\)</span> is left to the user, but typically the components of the vector <span class="math inline">\(\boldsymbol{\beta}\)</span> are left apriori independent, with <span class="math inline">\(\boldsymbol{\Omega}_0 \propto \lambda\mathbf{I}_n\)</span>.</p>
<div id="exm-regression" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.1</strong></span> Study 4 of <span class="citation" data-cites="Lin:2024">Lin et al. (<a href="references.html#ref-Lin:2024" role="doc-biblioref">2024</a>)</span> focuses on cultural appropriation using a fictional scenario focusing on soul food recipe cookbook from Chef Dax. The chef who was either African-American or not, and the authors manipulated the description of the way he obtained the recipes (either by peeking without permission in kitchens, by asking permission or without them mentioning altogether for the control category). Authors postulated that the perception of appropriation would vary by political ideology (liberal or conservative). The study results in a 3 by 2 by 2 three-way between-subject ANOVA.</p>
<p>We use the multivariate regression model to draw samples from <span class="math inline">\(\boldsymbol{\beta}\)</span> and use these to reconstruct the subgroup sample means for each of the 12 categories. Then, we consider the difference in perception of cultural appropriation for participants when Chef Dax is black (no cultural appropriation) versus when he is not, separately for liberals and conservatives, by pooling data across all different descriptions. <a href="#fig-contrasts" class="quarto-xref">Figure&nbsp;<span>8.1</span></a> shows the posterior distribution of those contrasts, which indicate that on average liberal perceive cultural appropriation more strongly (with nearly 2 points more), than conservatives (0.7 points on average).</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(LKUK24_S4, <span class="at">package =</span> <span class="st">"hecedsm"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">contrasts =</span> <span class="fu">c</span>(<span class="st">"contr.sum"</span>, <span class="st">"contr.poly"</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(appropriation <span class="sc">~</span> politideo <span class="sc">*</span> chefdax <span class="sc">*</span> brandaction,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> LKUK24_S4)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Model matrix, response and dimensions</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> LKUK24_S4<span class="sc">$</span>appropriation</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(model)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Priors</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># We set zero for contrasts and a small value</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># for the global mean (since response is on [1,7] Likert scale)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># with lower values indicating more cultural appropriation)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>mu_0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">2.5</span>, <span class="fu">rep</span>(<span class="dv">0</span>, p<span class="dv">-1</span>))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>Omega_0 <span class="ot">&lt;-</span> <span class="fu">diag</span>(p)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior precision of 0.25 (variance of 4)</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>nu_0 <span class="ot">&lt;-</span> <span class="fl">0.25</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>tau_0 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>Omega_n_inv <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">crossprod</span>(X) <span class="sc">+</span> Omega_0)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>mu_n <span class="ot">&lt;-</span> Omega_n_inv <span class="sc">%*%</span> (<span class="fu">crossprod</span>(X, y) <span class="sc">+</span> <span class="fu">crossprod</span>(Omega_0, mu_0))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>tau_n <span class="ot">&lt;-</span> tau_0 <span class="sc">+</span> <span class="fu">sum</span>(<span class="fu">resid</span>(model)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">c</span>(<span class="fu">crossprod</span>(X <span class="sc">%*%</span> (mu_n<span class="sc">-</span><span class="fu">coef</span>(model)))) <span class="sc">+</span> <span class="fu">sum</span>((mu_n<span class="sc">-</span>mu_0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior draws from the model</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>omega <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="at">n =</span> <span class="fl">1e3</span>L, </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>                <span class="at">shape =</span> (nu_0 <span class="sc">+</span> n)<span class="sc">/</span><span class="dv">2</span>, </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>                tau_n<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="at">n =</span> <span class="fl">1e3</span>L<span class="sc">*</span>p), <span class="at">ncol =</span> p) <span class="sc">%*%</span> </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">chol</span>(Omega_n_inv)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">sweep</span>(beta, <span class="dv">1</span>, <span class="fu">sqrt</span>(omega), <span class="st">"/"</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">sweep</span>(beta, <span class="dv">2</span>, mu_n, <span class="st">"+"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior quartiles for beta</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>beta_qu <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(beta, <span class="dv">2</span>, quantile, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.25</span>,<span class="fl">0.5</span>,<span class="fl">0.75</span>)))</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard dev. for beta (from Student-t)</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>beta_se <span class="ot">&lt;-</span> <span class="fu">sqrt</span>((nu_0 <span class="sc">+</span> n)<span class="sc">/</span>(nu_0 <span class="sc">+</span> n <span class="sc">-</span> <span class="dv">2</span>) <span class="sc">*</span><span class="fu">diag</span>(tau_n<span class="sc">/</span>(nu_0 <span class="sc">+</span> n) <span class="sc">*</span> Omega_n_inv))</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> TruncatedNormal<span class="sc">::</span><span class="fu">rtmvt</span>(</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fl">1e3</span>L,</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>  <span class="at">mu =</span> mu_n, </span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>  <span class="at">sigma =</span> tau_n<span class="sc">/</span>(nu_0 <span class="sc">+</span> n) <span class="sc">*</span> Omega_n_inv, </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>  <span class="at">df =</span> nu_0 <span class="sc">+</span> n)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>dfg <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">politideo =</span><span class="fu">c</span>(<span class="st">"conservative"</span>, <span class="st">"liberal"</span>),</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>            <span class="at">chefdax =</span> <span class="fu">c</span>(<span class="st">"not black"</span>, <span class="st">"black"</span>),</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>            <span class="at">brandaction =</span> <span class="fu">c</span>(<span class="st">"peeking"</span>,<span class="st">"permission"</span>, <span class="st">"control"</span>))</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>mm <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>( <span class="sc">~</span> politideo <span class="sc">*</span> chefdax <span class="sc">*</span> brandaction,</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> dfg)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Subgroup means for each of the 12 categories</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">tcrossprod</span>(beta, mm)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Contrast weights, averaging over brandaction</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">0</span>), <span class="at">length.out =</span> <span class="fu">nrow</span>(dfg))<span class="sc">/</span><span class="dv">3</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="at">length.out =</span> <span class="fu">nrow</span>(dfg))<span class="sc">/</span><span class="dv">3</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior distribution of contrasts</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>tc <span class="ot">&lt;-</span> mu <span class="sc">%*%</span> <span class="fu">cbind</span>(w1, w2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The coefficients and standard errors from the linear regression are very nearly similar to the posterior mean and standard deviations for <span class="math inline">\(\boldsymbol{\beta}\)</span> from the marginal Student-<span class="math inline">\(t,\)</span> owing to the large sample size and uninformative priors.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-contrasts" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-contrasts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regression_files/figure-html/fig-contrasts-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-contrasts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: Difference in appropriation rating for black vs non-black Chef Dax, average accross different levels of brand action.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>In multivariate regression, it sometimes is useful to specify correlated coefficients (e.g., for random effects). This leads to the necessity to set a prior on a covariance matrix.</p>
<div id="prp-Wishart-distribution" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 8.3 (Wishart distribution)</strong></span> Let <span class="math inline">\(\boldsymbol{Q}\)</span> by a random <span class="math inline">\(p \times p\)</span> symmetric positive definite matrix with Wishart distribution, denoted <span class="math inline">\(\mathbf{Q} \sim \mathsf{Wishart}_p(\nu, \mathbf{S})\)</span> for <span class="math inline">\(\nu&gt;0\)</span> degrees of freedom and scale <span class="math inline">\(\mathbf{S}\)</span>. It’s density is proportional to <span class="math display">\[\begin{align*}
f(\boldsymbol{Q}) \stackrel{\boldsymbol{Q}}{\propto} |\boldsymbol{Q}|^{(\nu-p-1)/2}\exp\{-\mathrm{tr}(\mathbf{S}^{-1}\boldsymbol{Q})/2\}, \qquad \nu &gt; p-1.
\end{align*}\]</span> where <span class="math inline">\(|\cdot|\)</span> denotes the determinant of the matrix and <span class="math inline">\(\mathrm{tr}\)</span> the trace operator. The Wishart also arises from considering <span class="math inline">\(n \geq p\)</span> independent and identically distributed mean zero Gaussian vectors <span class="math inline">\(\boldsymbol{Y}_i \sim \mathsf{Gauss}_p(\boldsymbol{0}_p, \mathbf{S})\)</span>, where <span class="math display">\[\begin{align*}
\sum_{i=1}^{\nu} \boldsymbol{Y}_i\boldsymbol{Y}_i^\top \sim \mathsf{Wishart}_p(\nu, \mathbf{S}).
\end{align*}\]</span> For prior elicitation, <span class="math inline">\(\nu\)</span> is thus a prior sample size, whereas we can specify <span class="math inline">\(\mathbf{S}\)</span> using the fact that the mean of the Wishart is <span class="math inline">\(\nu \mathbf{S}\)</span>; taking an identity matrix is standard. For more mathematical properties, consult Chapter 8 of <span class="citation" data-cites="Eaton:2007">Eaton (<a href="references.html#ref-Eaton:2007" role="doc-biblioref">2007</a>)</span>.</p>
</div>
<div id="def-inverse-wishart" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.1 (Inverse Wishart)</strong></span> Analogous to the relationship between gamma prior for the precision and inverse gamma for the variance, we can also similarly consider a prior for the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma} = \boldsymbol{Q}^{-1}\)</span>. Applying the change of variable formula, we get Jacobian <span class="math inline">\(|\boldsymbol{\Sigma}|^{p+1}\)</span>, and so <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is inverse Wishart <span class="math inline">\(\mathsf{inv. Wishart}(\nu, \mathbf{S}^{-1}),\)</span> with density proportional to <span class="math display">\[\begin{align*}
p(\boldsymbol{\Sigma}) \propto |\boldsymbol{\Sigma}|^{-(\nu+p+1)/2} \exp\left\{-\frac{1}{2} \mathrm{tr}\left(\boldsymbol{S}^{-1}\boldsymbol{\Sigma}^{-1}\right)\right\}
\end{align*}\]</span> with expectation <span class="math inline">\(\mathbf{S}^{-1}(\nu-p-1)\)</span> for <span class="math inline">\(\nu &gt; p+1.\)</span></p>
</div>
<div id="prp-conjugate-prior" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 8.4 (Wishart as conjugate prior in Gaussian model)</strong></span> Consider <span class="math inline">\(\boldsymbol{\mu} \sim \mathsf{Gauss}_p(\boldsymbol{\mu}_0, \boldsymbol{Q}^{-1})\)</span> and <span class="math inline">\(\boldsymbol{Q} \sim \mathsf{Wishart}_p(\nu, \mathbf{S})\)</span> for <span class="math inline">\(\nu \geq p\)</span>. Then, the conditional density of <span class="math inline">\(\boldsymbol{Q} \mid \boldsymbol{\mu}, \boldsymbol{\mu}_0\)</span> is proportional to <span class="math display">\[\begin{align*}
|\boldsymbol{Q}|^{1/2} \exp \left\{ -\frac{1}{2} (\boldsymbol{\mu}-\boldsymbol{\mu}_0)^\top\boldsymbol{Q}(\boldsymbol{\mu}-\boldsymbol{\mu}_0)\right\} |\boldsymbol{Q}|^{(\nu-p-1)/2}\exp\{-\mathrm{tr}(\mathbf{S}^{-1}\boldsymbol{Q})/2\}
\end{align*}\]</span> and thus <span class="math inline">\(\mathsf{Wishart}_p\{\nu + 1/2, \boldsymbol{S} + (\boldsymbol{\mu}-\boldsymbol{\mu}_0)(\boldsymbol{\mu}-\boldsymbol{\mu}_0)^\top\}.\)</span> To see this, note that a <span class="math inline">\(1 \times 1\)</span> matrix is equal to it’s trace, and the trace operator is invariant to cyclic of it’s argument, meaning that <span class="math display">\[\begin{align*}
(\boldsymbol{\mu}-\boldsymbol{\mu}_0)^\top\boldsymbol{Q}(\boldsymbol{\mu}-\boldsymbol{\mu}_0) = \mathrm{tr}\left\{ \boldsymbol{Q}(\boldsymbol{\mu}-\boldsymbol{\mu}_0)(\boldsymbol{\mu}-\boldsymbol{\mu}_0)^\top\right\}.
\end{align*}\]</span> We then combine elements to get the parameters. This extends naturally to the case of <span class="math inline">\(n\)</span> independent observations, for example linear regression model.</p>
</div>
<div id="prp-prior-variance" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 8.5 (Priors for variance matrices)</strong></span> The marginal precision for the Wishart variate are gamma distributed with the same degrees of freedom <span class="math inline">\(\nu\)</span>. The problem with using this prior is that it has a single parameter governing all scale parameters (i.e., the marginal variance) and the absolute value of the correlation and marginal variance parameters are negatively related <span class="citation" data-cites="Gelman:2013">(<a href="references.html#ref-Gelman:2013" role="doc-biblioref">Gelman et al. 2013</a>)</span>, as seen in <a href="#fig-draws-Wishart" class="quarto-xref">Figure&nbsp;<span>8.2</span></a>. Large variance thus correspond to small correlations shrunk towards zero when the degrees of freedom increase. There exists alternative distributions, such as the scaled-inverse Wishart, that add redundant scale parameters to decouple <span class="math inline">\(\mathrm{diag}(\boldsymbol{\xi}) \times \boldsymbol{\Sigma} \times \mathrm{diag}(\boldsymbol{\xi})\)</span>, but we refrain from pursuing these here.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-draws-Wishart" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-draws-Wishart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regression_files/figure-html/fig-draws-Wishart-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-draws-Wishart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.2: Prior draws from a bivariate inverse Wishart with identity scale matrix and <span class="math inline">\(\nu \in \{3, 20\}\)</span> degrees of freedom.
</figcaption>
</figure>
</div>
</div>
</div>
<p>A better alternative is to specify different prior for each marginal scale <span class="math inline">\(\sigma_j\)</span> and a prior on the correlation matrix <span class="math inline">\(\mathbf{R}.\)</span> For the latter, the onion peel or LKJ prior, named after the authors of <span class="citation" data-cites="Lewandowski:2009">Lewandowski, Kurowicka, and Joe (<a href="references.html#ref-Lewandowski:2009" role="doc-biblioref">2009</a>)</span>, is <span class="math inline">\(p(\mathbf{R}) \propto |\mathbf{R}|^{\eta-1}\)</span> for a scale <span class="math inline">\(\eta&gt;0.\)</span> The case <span class="math inline">\(\eta=1\)</span> leads to uniform over the space of correlation matrices, and <span class="math inline">\(\eta&gt;1\)</span> favours the identity matrix.</p>
</div>
<section id="shrinkage-priors" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="shrinkage-priors"><span class="header-section-number">8.1</span> Shrinkage priors</h2>
<p>In contexts where the number of regressors <span class="math inline">\(p\)</span> is considerable relative to the sample size <span class="math inline">\(n\)</span>, it may be useful to constrain the parameter vector if we assume that the signal is sparse, with a large proportion of coefficients that should be zero. This is notably important when the ratio <span class="math inline">\(p/n \to c\)</span> for <span class="math inline">\(c &gt; 0,\)</span> meaning that the number of coefficients and covariates increases proportional to the sample size. Shrinkage priors can regularize and typically consist of distributions that have a mode at zero, and another that allows for larger signals.</p>
<p>The scale parameters and hyperparameters can be estimated jointly with the model, and uncertainty diagnostics follow naturally. We assume that coefficients <span class="math inline">\(\beta_j\)</span> are independent apriori, although it is possible to specify group structures (e.g., for handling coefficients associated to a common categorical covariate with <span class="math inline">\(K\)</span> levels, represented by <span class="math inline">\(K-1\)</span> columns of dummy group indicators).</p>
<div id="prp-spike-slab" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 8.6 (Spike-and-slab prior)</strong></span> The spike-and-slab prior is a two-component mixture that assigns a positive probability to zero via a point mass or a vary narrow distribution centered at the origin (the spike) and the balance to the slab, a diffuse distribution.</p>
<p>The spike-and-slab prior was originally proposed by <span class="citation" data-cites="Mitchell.Beauchamp:1988">Mitchell and Beauchamp (<a href="references.html#ref-Mitchell.Beauchamp:1988" role="doc-biblioref">1988</a>)</span> with a uniform on a large interval and a point mass at zero. The term was also used in <span class="citation" data-cites="George.McCulloch:1993">George and McCulloch (<a href="references.html#ref-George.McCulloch:1993" role="doc-biblioref">1993</a>)</span>, which replaced the prior by a mixture of Gaussians, one of which diffuse and the other with near infinite precision and centered at the origin. The latter is known under the vocable stochastic search variable selection prior. Letting <span class="math inline">\(\gamma_j \in [0,1]\)</span> denote the probability of the slab or inclusion of the variable, the independent priors for the regression coefficients are <span class="math display">\[\begin{align*}
\beta_j \mid \gamma_j, c_j^2,\phi^2_j \sim (1-\gamma_j) \mathsf{Gauss}(0, \phi^2_j) + \gamma_j \mathsf{Gauss}(0, \sigma_j^2\phi^2)
\end{align*}\]</span> where <span class="math inline">\(\sigma^2_j\)</span> is very nearly zero. We set a Jeffrey’s prior for <span class="math inline">\(\gamma_j \sim \mathsf{beta}(0.5, 0.5)\)</span> and typically <span class="math inline">\(\phi_j^2=0.001\)</span> or a small number if using a Gaussian. The construction allows for variable augmentation with mixture indicators and Gibbs sampling, although convergence isn’t trivial.</p>
</div>
<div id="prp-horseshoe" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 8.7 (Horseshoe prior)</strong></span> The horseshoe prior of <span class="citation" data-cites="Carvalho.Polson.Scott:2010">Carvalho, Polson, and Scott (<a href="references.html#ref-Carvalho.Polson.Scott:2010" role="doc-biblioref">2010</a>)</span> is a hierarchical prior of the form <span class="math display">\[\begin{align*}
\beta_j \mid \sigma^2_j \sim \mathsf{Gauss}(0, \sigma^2_j), \quad \sigma^2_j \mid \lambda \sim \mathsf{Student}_{+}(0, \lambda, 1), \quad \lambda \sim \mathsf{Student}_{+}(0, \omega, 1)
\end{align*}\]</span> where <span class="math inline">\(\mathsf{Student}_{+}(0, a, 1)\)</span> denotes a half-Cauchy distribution with scale <span class="math inline">\(a&gt;0,\)</span> truncated on <span class="math inline">\(\mathbb{R}_{+}.\)</span></p>
<p>This prior has no explicit density, but is continuous and can be simulated. It is useful to consider the behaviour of the random variance <span class="math inline">\(\sigma^2_j\)</span> term, which leads to an unconditional scale mixture of Gaussian for <span class="math inline">\(\beta_j\)</span>. More useful to understanding is looking at <span class="math inline">\(\kappa = 1 - 1/(1+\sigma^2)\)</span>, which gives a weight in <span class="math inline">\([0,1].\)</span> We can see what happens to the shrunk components close to zero by looking at the density of <span class="math inline">\(\kappa \to 0\)</span>, and similarly at the large signals when <span class="math inline">\(\kappa \to 1.\)</span> The Cauchy prior does not shrink towards zero, and lets large signals pass, whereas the Bayesian LASSO double exponential shrinkage leads to attenuation of strong signals. The horseshoe prior name comes from the shape of the prior, which leads to a shrinkage analog to <span class="math inline">\(\mathsf{beta}(1/2, 1/2)\)</span> and thus penalizes, forcing components to be either large or small.</p>
<p><a href="#fig-weights-shrinkage" class="quarto-xref">Figure&nbsp;<span>8.3</span></a> shows the weighting implied by the mixture density for a Cauchy prior on the variance, the double exponential of the Laplace from Bayesian LASSO and the horseshoe.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-weights-shrinkage" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weights-shrinkage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regression_files/figure-html/fig-weights-shrinkage-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weights-shrinkage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.3: Density of penalization weights <span class="math inline">\(\kappa\)</span> of spike (near zero) and slab (near one) for three shrinkage priors.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>In the Bayesian paradigm, regularization is achieved via priors that have mass at or towards zero, pushing coefficients of the regression model towards zero unless there is strong evidence from the likelihood against this. We however want to allow non-zero coefficients, typically by setting coefficient-specific parameters with a heavy tailed distribution to prevent overshrinking. Most if not all parameter can be viewed as scale mixtures of Gaussian.</p>
<p>We make a distinction between <strong>global</strong> shrinkage priors those that consider a common shrinkage parameter for all regression coefficients, to be compared with <strong>local</strong> scale mixtures that have coefficient-specific parameters.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-shrinkage" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-shrinkage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regression_files/figure-html/fig-shrinkage-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-shrinkage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.4: Marginal density for a regression coefficient <span class="math inline">\(\beta\)</span> with horseshoe prior (full), Laplace (dashed) and a Student-<span class="math inline">\(t\)</span> (thick dotted). The plot on the right shows the tail behaviour. The density of the horseshoe is unbounded at the origin. Inspired from Figure 1 of <span class="citation" data-cites="Carvalho.Polson.Scott:2010">Carvalho, Polson, and Scott (<a href="references.html#ref-Carvalho.Polson.Scott:2010" role="doc-biblioref">2010</a>)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>While the horseshoe prior guarantees that large coefficients are not regularized, this feature of the shrinkage prior is harmful in certain instances, for example separation of variables for logistic regression. Markov chain Monte Carlo simulations are hampered by these parameters whose posterior mean does not exist, leading to poor mixing. Some very weak regularization for these big components can thus help. <span class="citation" data-cites="Piironen.Vehtari:2017">Piironen and Vehtari (<a href="references.html#ref-Piironen.Vehtari:2017" role="doc-biblioref">2017</a>)</span> proposed the regularized horseshoe, nicknamed Finnish horseshoe, where <span class="math display">\[\begin{align*}
\beta_j \mid \lambda, \tau_j, c^2 &amp;\sim \mathsf{Gauss}\left(0, \lambda\frac{c^2\tau_j^2}{c^2 + \tau^2_j\lambda^2}\right), \\
\tau_j &amp;\sim \mathsf{Student}_{+}(0, 1, 1)\\
c^2 \mid s^2, \nu &amp;\sim \mathsf{inv. gamma}(\nu/2, \nu s^2/2).
\end{align*}\]</span> When <span class="math inline">\(\tau^2\lambda^2_j\)</span> is much greater than <span class="math inline">\(c^2\)</span>, this amounts to having a Student slab with <span class="math inline">\(\nu\)</span> degrees of freedom for large coefficients; taking a small value of <span class="math inline">\(\nu\)</span> allows for large, but not extreme components, and the authors use <span class="math inline">\(s^2=2, \nu=4.\)</span> The above specification does not specify the prior for the global scale <span class="math inline">\(\lambda\)</span>, for which <span class="citation" data-cites="Piironen.Vehtari:2017">Piironen and Vehtari (<a href="references.html#ref-Piironen.Vehtari:2017" role="doc-biblioref">2017</a>)</span> recommend using an empirical Bayes prior, with <span class="math display">\[\lambda \sim \mathsf{Student}_{+}\left\{0, \frac{p_0}{(p-p_0)}\frac{\sigma}{n^{1/2}}, 1\right\},\]</span> where <span class="math inline">\(p_0\)</span> is a prior guess for the number of non-zero components out of <span class="math inline">\(p,\)</span> <span class="math inline">\(n\)</span> is the sample size and <span class="math inline">\(\sigma\)</span> is some level of the noise.</p>
<div id="exm-case-study-shrinkage" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.2 (Comparison of shrinkage priors)</strong></span> We revisit the <code>diabetes</code> data from the <strong>R</strong> package <code>lars</code>, which was used in <span class="citation" data-cites="Park.Casella:2009">(<a href="references.html#ref-Park.Casella:2009" role="doc-biblioref"><strong>Park.Casella:2009?</strong></a>)</span> to illustrate the Bayesian LASSO. We consider three methods: the default Gaussian prior, which gives a ridge penalty, the Bayesian LASSO and finally the horseshoe. Models are fitted using the <code>bayesreg</code> package.</p>
<p><a href="#fig-shrinkage-dens-comparison" class="quarto-xref">Figure&nbsp;<span>8.5</span></a> shows the ordered coefficients for each method. We can see that the ridge has the widest intervals of all methods, providing some shrinkage only for large values of <span class="math inline">\(\beta\)</span>. The horseshoe has typically narrower intervals, with more mass in a neighborhood of zero for smaller coefficients, and asymmetric intervals.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-shrinkage-dens-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-shrinkage-dens-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regression_files/figure-html/fig-shrinkage-dens-comparison-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-shrinkage-dens-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.5: Density estimates for regression coefficients with Gaussian (ridge), double exponential (Laplace) and horseshoe priors for the <code>diabetes</code> data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>One aspect worth mentioning is that the horseshoe prior impacts strongly the geometry and leads to slower mixing: the effective sample size fraction relative to the number of samples ranges from 15% to 79%, compared to 60% to 100% for the Bayesian LASSO and near-independent draws with the conjugate ridge.</p>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Carvalho.Polson.Scott:2010" class="csl-entry" role="listitem">
Carvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010. <span>“The Horseshoe Estimator for Sparse Signals.”</span> <em>Biometrika</em> 97 (2): 465–80. <a href="https://doi.org/10.1093/biomet/asq017">https://doi.org/10.1093/biomet/asq017</a>.
</div>
<div id="ref-Eaton:2007" class="csl-entry" role="listitem">
Eaton, Morris L. 2007. <em>Multivariate Statistics: A Vector Space Approach</em>. Institute for Mathematical Statistics. <a href="https://doi.org/10.1214/lnms/1196285102">https://doi.org/10.1214/lnms/1196285102</a>.
</div>
<div id="ref-Gelman:2013" class="csl-entry" role="listitem">
Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. 3rd ed. New York: Chapman; Hall/CRC. <a href="https://doi.org/10.1201/b16018">https://doi.org/10.1201/b16018</a>.
</div>
<div id="ref-George.McCulloch:1993" class="csl-entry" role="listitem">
George, Edward I., and Robert E. McCulloch. 1993. <span>“Variable Selection via <span>G</span>ibbs Sampling.”</span> <em>Journal of the American Statistical Association</em> 88 (423): 881–89. <a href="https://doi.org/10.1080/01621459.1993.10476353">https://doi.org/10.1080/01621459.1993.10476353</a>.
</div>
<div id="ref-Lewandowski:2009" class="csl-entry" role="listitem">
Lewandowski, Daniel, Dorota Kurowicka, and Harry Joe. 2009. <span>“Generating Random Correlation Matrices Based on Vines and Extended Onion Method.”</span> <em>Journal of Multivariate Analysis</em> 100 (9): 1989–2001. <a href="https://doi.org/10.1016/j.jmva.2009.04.008">https://doi.org/10.1016/j.jmva.2009.04.008</a>.
</div>
<div id="ref-Lin:2024" class="csl-entry" role="listitem">
Lin, Jason D, Nicole You Jeung Kim, Esther Uduehi, and Anat Keinan. 2024. <span>“Culture for Sale: Unpacking Consumer Perceptions of Cultural Appropriation.”</span> <em>Journal of Consumer Research</em>. <a href="https://doi.org/10.1093/jcr/ucad076">https://doi.org/10.1093/jcr/ucad076</a>.
</div>
<div id="ref-Mitchell.Beauchamp:1988" class="csl-entry" role="listitem">
Mitchell, T. J., and J. J. Beauchamp. 1988. <span>“Bayesian Variable Selection in Linear Regression.”</span> <em>Journal of the American Statistical Association</em> 83 (404): 1023–32. <a href="https://doi.org/10.1080/01621459.1988.10478694">https://doi.org/10.1080/01621459.1988.10478694</a>.
</div>
<div id="ref-Piironen.Vehtari:2017" class="csl-entry" role="listitem">
Piironen, Juho, and Aki Vehtari. 2017. <span>“Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors.”</span> <em>Electronic Journal of Statistics</em> 11 (2): 5018–51. <a href="https://doi.org/10.1214/17-ejs1337si">https://doi.org/10.1214/17-ejs1337si</a>.
</div>
<div id="ref-Villani:2023" class="csl-entry" role="listitem">
Villani, Mattias. 2023. <span>“Bayesian Learning: A Gentle Introduction.”</span> <a href="https://mattiasvillani.com/BayesianLearningBook/">https://mattiasvillani.com/BayesianLearningBook/</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lbelzile\.github\.io\/MATH80601A\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./workflow.html" class="pagination-link" aria-label="Computational strategies and diagnostics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Computational strategies and diagnostics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./laplace.html" class="pagination-link" aria-label="Deterministic approximations">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Deterministic approximations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2024-2025, Léo Belzile</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/lbelzile/MATH80601A/edit/main/regression.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>