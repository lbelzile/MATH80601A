<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.3">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Introduction – Bayesian modelling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./bayesics.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-81b5c3e63835cfde897ecd3d35a35a41.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ad5ef081d17d57ab2b6cc5e45efb5d90.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./introduction.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/MATH80601A" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH80601A-bayesmod.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./montecarlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Monte Carlo methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Metropolis–Hastings algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gibbs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Gibbs sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Computational strategies and diagnostics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regression models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./laplace.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Deterministic approximations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#random-vectors" id="toc-random-vectors" class="nav-link active" data-scroll-target="#random-vectors"><span class="header-section-number">1.1</span> Random vectors</a>
  <ul class="collapse">
  <li><a href="#common-distributions" id="toc-common-distributions" class="nav-link" data-scroll-target="#common-distributions"><span class="header-section-number">1.1.1</span> Common distributions</a></li>
  <li><a href="#marginal-and-conditional-distributions" id="toc-marginal-and-conditional-distributions" class="nav-link" data-scroll-target="#marginal-and-conditional-distributions"><span class="header-section-number">1.1.2</span> Marginal and conditional distributions</a></li>
  </ul></li>
  <li><a href="#expectations" id="toc-expectations" class="nav-link" data-scroll-target="#expectations"><span class="header-section-number">1.2</span> Expectations</a></li>
  <li><a href="#likelihood" id="toc-likelihood" class="nav-link" data-scroll-target="#likelihood"><span class="header-section-number">1.3</span> Likelihood</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/lbelzile/MATH80601A/edit/main/introduction.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><h1 class="title display-7"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></h1></header>

<header id="title-block-header">


</header>


<p>This section review basic concepts in probability theory that will be used throughout the course. The overview begins with basic statistical concepts, random variables, their distribution and density, moments and likelihood derivations.</p>
<section id="random-vectors" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="random-vectors"><span class="header-section-number">1.1</span> Random vectors</h2>
<p>We begin with a characterization of random vectors and their marginal, conditional and joint distributions. A good reference for this material is Chapter 3 of <span class="citation" data-cites="McNeil.Frey.Embrechts:2005">McNeil, Frey, and Embrechts (<a href="references.html#ref-McNeil.Frey.Embrechts:2005" role="doc-biblioref">2005</a>)</span>, and Appendix A of <span class="citation" data-cites="Held.Bove:2020">Held and Bové (<a href="references.html#ref-Held.Bove:2020" role="doc-biblioref">2020</a>)</span>.</p>
<div id="def-cdf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.1 (Density and distribution function)</strong></span> Let <span class="math inline">\(\boldsymbol{X}\)</span> denote a <span class="math inline">\(d\)</span>-dimensional vector with real entries in <span class="math inline">\(\mathbb{R}^d.\)</span> The distribution function of <span class="math inline">\(\boldsymbol{X}\)</span> is <span class="math display">\[\begin{align*}
F_{\boldsymbol{X}}(\boldsymbol{x}) = \Pr(\boldsymbol{X} \leq \boldsymbol{x}) = \Pr(X_1 \leq x_1, \ldots, X_d \leq x_d).
\end{align*}\]</span></p>
<p>If the distribution of <span class="math inline">\(\boldsymbol{X}\)</span> is absolutely continuous, we may write <span class="math display">\[\begin{align*}
F_{\boldsymbol{X}}(\boldsymbol{x}) = \int_{-\infty}^{x_d} \cdots \int_{-\infty}^{x_1} f_{\boldsymbol{X}}(z_1, \ldots, z_d) \mathrm{d} z_1 \cdots \mathrm{d} z_d,
\end{align*}\]</span> where <span class="math inline">\(f_{\boldsymbol{X}}(\boldsymbol{x})\)</span> is the joint <strong>density function</strong>. The density function can be obtained as the derivative of the distribution function with respect to all of it’s arguments.</p>
<p>We use the same notation for the mass function in the discrete case where <span class="math inline">\(f_{\boldsymbol{X}}(\boldsymbol{x}) = \Pr(X_1 = x_1, \ldots, X_d = x_d),\)</span> where the integral is understood to mean a summation over all values lower or equal to <span class="math inline">\(\boldsymbol{x}\)</span> in the support. In the discrete case, <span class="math inline">\(0 \leq f_{\boldsymbol{X}}(\boldsymbol{x}) \leq 1\)</span> is a probability and the total probability over all points in the support sum to one, meaning <span class="math inline">\(\sum_{\boldsymbol{x} \in \mathsf{supp}(\boldsymbol{X})} f_{\boldsymbol{X}}(\boldsymbol{x}) = 1.\)</span></p>
</div>
<section id="common-distributions" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="common-distributions"><span class="header-section-number">1.1.1</span> Common distributions</h3>
<div id="def-gamma" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.2 (Gamma, chi-square and exponential distributions)</strong></span> A random variable follows a gamma distribution with shape <span class="math inline">\(\alpha&gt;0\)</span> and rate <span class="math inline">\(\beta&gt;0,\)</span> denoted <span class="math inline">\(Y \sim \mathsf{gamma}(\alpha, \beta),\)</span> if it’s density is <span class="math display">\[\begin{align*}
f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}\exp(-\beta x), \qquad x \in (0, \infty),
\end{align*}\]</span> where <span class="math inline">\(\Gamma(\alpha)\coloneqq\int_0^\infty t^{\alpha-1}\exp(-t)\mathrm{d} t\)</span> is the gamma function.</p>
<p>If <span class="math inline">\(\alpha=1,\)</span> the density simplifies to <span class="math inline">\(\beta \exp(-\beta x)\)</span> and we recover the <strong>exponential distribution</strong>, denote <span class="math inline">\(\mathsf{expo}(\beta).\)</span> The case <span class="math inline">\(\mathsf{gamma}(\nu/2, 1/2)\)</span> corresponds to the chi-square distribution <span class="math inline">\(\chi^2_\nu.\)</span></p>
<p>The mean and variance of a gamma are <span class="math inline">\(\mathsf{E}(Y)=\alpha/\beta\)</span> and <span class="math inline">\(\mathsf{Va}(Y)=\alpha/\beta^2\)</span>.</p>
</div>
<div id="def-beta" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.3 (Beta and uniform distribution)</strong></span> The beta distribution <span class="math inline">\(\mathsf{beta}(\alpha_1, \alpha_2)\)</span> is a distribution supported on the unit interval <span class="math inline">\([0,1]\)</span> with shape parameters <span class="math inline">\(\alpha_1&gt;0\)</span> and <span class="math inline">\(\alpha_2&gt;0.\)</span> It’s density is <span class="math display">\[\begin{align*}
f(x) = \frac{\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)}x^{\alpha_1-1}(1-x)^{1-\alpha_2}, \qquad x \in [0,1].
\end{align*}\]</span> The case <span class="math inline">\(\mathsf{beta}(1,1),\)</span> also denoted <span class="math inline">\(\mathsf{unif}(0,1),\)</span> corresponds to a standard uniform distribution. The beta distribution <span class="math inline">\(Y \sim \mathsf{beta}(\alpha, \beta)\)</span> has expectation <span class="math inline">\(\mathsf{E}(Y)=\alpha/(\alpha+\beta)\)</span> and variance <span class="math inline">\(\mathsf{Va}(Y)=\alpha\beta/\{(\alpha+\beta)^2(\alpha+\beta+1)\}\)</span>.</p>
</div>
<p>The beta distribution is commonly used to model proportions, and can be generalized to the multivariate setting as follows.</p>
<div id="def-dirichlet-dist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.4 (Dirichlet distribution)</strong></span> Let <span class="math inline">\(\boldsymbol{\alpha} \in (0, \infty)^d\)</span> denote shape parameters and consider a random vector of size <span class="math inline">\(d\)</span> with positive components on the simplex <span class="math display">\[\mathbb{S}_{d-1}: \{ 0 \leq x_j \leq 1; j=1, \ldots, d: x_1 + \cdots + x_d=1\}.\]</span> The density of a <strong>Dirichlet</strong> random vector, denoted <span class="math inline">\(\boldsymbol{Y} \sim \mathsf{Dirichlet}(\boldsymbol{\alpha}),\)</span> is <span class="math display">\[\begin{align*}
f(\boldsymbol{x}) = \frac{\prod_{j=1}^{d-1}\Gamma(\alpha_j)}{\Gamma(\alpha_1 + \cdots + \alpha_d)}\prod_{j=1}^{d} x_j^{\alpha_j-1}, \qquad \boldsymbol{x} \in \mathbb{S}_{d-1}
\end{align*}\]</span></p>
<p>Due to the linear dependence, the <span class="math inline">\(d\)</span>th component <span class="math inline">\(x_d = 1- x_1 - \cdots - x_{d-1}\)</span> is fully determined.</p>
</div>
<div id="def-binomial" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.5 (Binomial distribution)</strong></span> The density of the binomial distribution, denoted <span class="math inline">\(Y \sim \mathsf{binom}(n, p),\)</span> is <span class="math display">\[\begin{align*}
f(x) = \mathsf{Pr}(Y=x) = \binom{m}{x}p^x (1-p)^{m-x}, \quad x=0, 1, \ldots, n.
\end{align*}\]</span></p>
<p>If <span class="math inline">\(n=1,\)</span> we recover the Bernoulli distribution with density <span class="math inline">\(f(x) = p^{y}(1-p)^{1-y}.\)</span> The binomial distribution is closed under convolution, meaning that the number the number of successes <span class="math inline">\(Y\)</span> out of <span class="math inline">\(n\)</span> Bernoulli trials is binomial</p>
</div>
<div id="def-multinom-dist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.6 (Multinomial distribution)</strong></span> If there are more than two outcomes, say <span class="math inline">\(d,\)</span> we can generalize this mass function. Suppose that <span class="math inline">\(\boldsymbol{Y}=(Y_1, \ldots, Y_d)\)</span> denotes the number of realizations of each of the <span class="math inline">\(d\)</span> outcomes based on <span class="math inline">\(n\)</span> trials, so that <span class="math inline">\(0 \leq Y_j \leq n (j=1, \ldots, d)\)</span> and <span class="math inline">\(Y_1 + \cdots + Y_d=n.\)</span> The joint density of the multinomial vector <span class="math inline">\(\boldsymbol{Y} \sim \mathsf{multinom}(\boldsymbol{p})\)</span> with probability vector <span class="math inline">\(\boldsymbol{p} \in \mathbb{S}_{d-1}\)</span> is <span class="math display">\[\begin{align*}
f(\boldsymbol{x}) = \frac{n!}{\prod_{j=1}^d x_j!} \prod_{j=1}^d p_j^{x_j}, \qquad \boldsymbol{y}/n \in \mathbb{S}_{d-1},
\end{align*}\]</span> where <span class="math inline">\(x! = \Gamma(x+1)\)</span> denotes the factorial function.</p>
</div>
<div id="def-poissondist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.7 (Poisson distribution)</strong></span> If the probability of success <span class="math inline">\(p\)</span> of a Bernoulli event is small in the sense that <span class="math inline">\(np \to \lambda\)</span> when the number of trials <span class="math inline">\(n\)</span> increases, then the number of success follows approximately a Poisson distribution with mass function <span class="math display">\[\begin{align*}
f(x)=\mathsf{Pr}(Y=x) = \frac{\exp(-\lambda)\lambda^y}{\Gamma(y+1)}, \quad x=0, 1, 2, \ldots
\end{align*}\]</span> where <span class="math inline">\(\Gamma(\cdot)\)</span> denotes the gamma function. The parameter <span class="math inline">\(\lambda\)</span> of the Poisson distribution is both the expectation and the variance of the distribution, meaning <span class="math inline">\(\mathsf{E}(Y)=\mathsf{Va}(Y)=\lambda.\)</span> We denote the distribution as <span class="math inline">\(Y \sim \mathsf{Poisson}(\lambda).\)</span></p>
</div>
<div id="def-gaussian" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.8 (Gaussian distribution)</strong></span> Consider a <span class="math inline">\(d\)</span> dimensional vector <span class="math inline">\(\boldsymbol{Y} \sim \mathsf{Gauss}_d(\boldsymbol{\mu}, \boldsymbol{Q}^{-1})\)</span> with density <span class="math display">\[\begin{align*}
f(\boldsymbol{x}) = (2\pi)^{-d/2} |\boldsymbol{Q}|^{1/2} \exp \left\{ - \frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu})^\top \boldsymbol{Q}(\boldsymbol{x}-\boldsymbol{\mu})\right\}, \qquad \boldsymbol{x} \in \mathbb{R}^d
\end{align*}\]</span></p>
<p>The mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span> is the vector of expectation of individual observations, whereas <span class="math inline">\(\boldsymbol{Q}^{-1}\equiv \boldsymbol{\Sigma}\)</span> is the <span class="math inline">\(d \times d\)</span> covariance matrix of <span class="math inline">\(\boldsymbol{Y}\)</span> and <span class="math inline">\(\boldsymbol{Q},\)</span> the canonical parameter, is called the precision matrix.</p>
<p>In the univariate case, the density of <span class="math inline">\(\mathsf{Gauss}(\mu, \sigma^2)\)</span> reduces to <span class="math display">\[\begin{align*}
f(x) = (2\pi\sigma^2)^{-1/2} \exp \left\{ - \frac{(x-\mu)^2}{2\sigma^2}\right\}, \qquad x \in \mathbb{R}.
\end{align*}\]</span> Although the terminology “normal” is frequent, we will stick to Gaussian in these course notes.</p>
</div>
<div id="def-student-dist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.9 (Student-<span class="math inline">\(t\)</span> distribution)</strong></span> The name “Student” comes from the pseudonym used by William Gosset in <span class="citation" data-cites="Student:1908">Gosset (<a href="references.html#ref-Student:1908" role="doc-biblioref">1908</a>)</span>, who introduced the asymptotic distribution of the <span class="math inline">\(t\)</span>-statistic. The density of the Student-<span class="math inline">\(t\)</span> univariate distribution with <span class="math inline">\(\nu\)</span> degrees of freedom, location <span class="math inline">\(\mu\)</span> and scale <span class="math inline">\(\sigma,\)</span> denoted <span class="math inline">\(\mathsf{Student}(\mu, \sigma, \nu),\)</span> is <span class="math display">\[\begin{align*}
f(y; \mu, \sigma, \nu) = \frac{\Gamma \left( \frac{\nu+1}{2}\right)}{\sigma\Gamma\left(\frac{\nu}{2}\right)
\sqrt{\nu\pi}}\left(1+\frac{1}{\nu}\left(\frac{y-\mu}{\sigma}\right)^2\right)^{-\frac{\nu+1}{2}}.
\end{align*}\]</span> We also write <span class="math inline">\(\mathsf{Student}_{+}\)</span> to denote the truncated distribution on the positive half-line, <span class="math inline">\([0, \infty).\)</span></p>
<p>The density of the random vector <span class="math inline">\(\boldsymbol{Y} \sim \mathsf{Student}_d(\boldsymbol{\mu}, \boldsymbol{Q}^{-1}, \nu),\)</span> with location vector <span class="math inline">\(\boldsymbol{\mu},\)</span> scale matrix <span class="math inline">\(\boldsymbol{Q}^{-1}\)</span> and <span class="math inline">\(\nu\)</span> degrees of freedom is <span class="math display">\[\begin{align*}
f(\boldsymbol{x}) = \frac{\Gamma \left( \frac{\nu+d}{2}\right)|\boldsymbol{Q}|^{1/2}}{\Gamma\left(\frac{\nu}{2}\right)
(\nu\pi)^{d/2}}\left(1+\frac{(\boldsymbol{x}-\boldsymbol{\mu})^\top\boldsymbol{Q}(\boldsymbol{x}-\boldsymbol{\mu})}{\nu} \right)^{-\frac{\nu+d}{2}}, \qquad \boldsymbol{x} \in \mathbb{R}^d
\end{align*}\]</span> The Student distribution is a location-scale family and an elliptical distribution. The distribution has polynomial tails, is symmetric around <span class="math inline">\(\boldsymbol{\mu}\)</span> and is unimodal. As <span class="math inline">\(\nu \to \infty,\)</span> the Student distribution converges to a normal distribution. It has heavier tails than the normal distribution and only the first <span class="math inline">\(\nu-1\)</span> moments of the distribution exist. The case <span class="math inline">\(\nu=1\)</span> is termed Cauchy distribution.</p>
</div>
<div id="def-weibull" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.10 (Weibull distribution)</strong></span> The distribution function of a Weibull random variable with scale <span class="math inline">\(\lambda&gt;0\)</span> and shape <span class="math inline">\(\alpha&gt;0\)</span> is <span class="math display">\[\begin{align*}
F(x; \lambda, \alpha) &amp;= 1 - \exp\left\{-(x/\lambda)^\alpha\right\}, \qquad x \geq 0,
\end{align*}\]</span> while the corresponding density is <span class="math display">\[\begin{align*}
f(x; \lambda, \alpha) &amp;= \frac{\alpha}{\lambda^\alpha} x^{\alpha-1}\exp\left\{-(x/\lambda)^\alpha\right\}, \qquad x \geq 0.
\end{align*}\]</span></p>
<p>The quantile function, the inverse of the distribution function, is <span class="math inline">\(Q(p) = \lambda\{-\log(1-p)\}^{1/\alpha}.\)</span> The Weibull distribution includes the exponential as special case when <span class="math inline">\(\alpha=1.\)</span> The expected value of <span class="math inline">\(Y \sim \mathsf{Weibull}(\lambda, \alpha)\)</span> is <span class="math inline">\(\mathsf{E}(Y) = \lambda \Gamma(1+1/\alpha).\)</span></p>
</div>
<div id="def-gen-pareto" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.11 (Generalized Pareto distribution)</strong></span> The generalized Pareto distribution with scale <span class="math inline">\(\tau&gt;0\)</span> and shape <span class="math inline">\(\xi \in \mathbb{R}\)</span> has distribution and density functions equal to, respectively <span class="math display">\[\begin{align*}
F(x) &amp;=
\begin{cases}
1 - \left(1+\frac{\xi}{\tau}x\right)_{+}^{-1/\xi} &amp; \xi \neq 0 \\
1 - \exp(-x/\tau) &amp; \xi=0
\end{cases}, \quad x \geq 0; \\
f(x) &amp;=
\begin{cases}
\tau^{-1}\left(1+\frac{\xi}{\tau}x\right)_{+}^{-1/\xi-1} &amp; \xi \neq 0 \\
\tau^{-1}\exp(-x/\tau) &amp; \xi=0
\end{cases}\quad x \geq 0;
\end{align*}\]</span> with <span class="math inline">\(x_{+} = \max\{x, 0\}.\)</span> The case <span class="math inline">\(\xi=0\)</span> corresponding to the exponential distribution with rate <span class="math inline">\(\tau^{-1}\)</span>. The distribution is used to model excesses over a large threshold <span class="math inline">\(u\)</span>, as extreme value theory dictates that, under broad conditions, <span class="math inline">\(Y-u \mid Y &gt; u \sim \mathsf{gen. Pareto}(\tau_u, \xi)\)</span> as <span class="math inline">\(u\)</span> tends to the endpoint of the support of <span class="math inline">\(Y\)</span>, regardless of the underlying distribution of <span class="math inline">\(Y\)</span>. See <a href="bayesics.html#exm-loss-extremes" class="quarto-xref">Example&nbsp;<span>2.6</span></a> for an application of this model.</p>
</div>
<div id="def-location-scale" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.12 (Location and scale distribution)</strong></span> A random variable <span class="math inline">\(Y\)</span> is said to belong to a location scale family with location parameter <span class="math inline">\(b\)</span> and scale <span class="math inline">\(a&gt;0\)</span> if it is equal in distribution to a location and scale transformation of a standard variable <span class="math inline">\(X\)</span> with location zero and unit scale, denoted <span class="math inline">\(Y {=}_d\, aX + b\)</span> and meaning, <span class="math display">\[\Pr(Y \leq y) = \Pr(aX + b \leq y).\]</span> If the density exists, then <span class="math inline">\(f_Y(y) = a^{-1}f_X\{(y-b)/a\}.\)</span></p>
<p>We can extend this definition to the multivariate setting for location vector <span class="math inline">\(\boldsymbol{b} \in \mathbb{R}^d\)</span> and positive definite scale matrix <span class="math inline">\(\mathbf{A},\)</span> such that <span class="math display">\[\Pr(\boldsymbol{Y} \leq \boldsymbol{y}) = \Pr(\mathbf{A}\boldsymbol{X} + \boldsymbol{b} \leq \boldsymbol{y}).\]</span></p>
</div>
<div id="def-exponential-family" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.13 (Exponential family)</strong></span> A univariate distribution is an exponential family if it’s density or mass function can be written for all <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span> and <span class="math inline">\(y \in \mathbb{R}\)</span> as <span class="math display">\[\begin{align*}
f(y; \boldsymbol{\theta}) = \exp\left\{ \sum_{k=1}^K Q_k(\boldsymbol{\theta}) t_k(y) + D(\boldsymbol{\theta}) + h(y)\right\},
\end{align*}\]</span> where functions <span class="math inline">\(Q_1(\cdot), \ldots, Q_K(\cdot)\)</span> and <span class="math inline">\(D(\cdot)\)</span> depend only on <span class="math inline">\(\boldsymbol{\theta}\)</span> and not on the data, and conversely <span class="math inline">\(t_1(\cdot), \ldots, t_K(\cdot)\)</span> and <span class="math inline">\(h(\cdot)\)</span> do not depend on the vector of parameters <span class="math inline">\(\boldsymbol{\theta}.\)</span></p>
<p>The support of <span class="math inline">\(f\)</span> must not depend on <span class="math inline">\(\boldsymbol{\theta}.\)</span> The transformed parameters <span class="math inline">\(Q_k(\boldsymbol{\theta})\)</span> <span class="math inline">\((k=1, \ldots, K)\)</span> are termed canonical parameters.</p>
</div>
<p>If we have an independent and identically distributed sample of observations <span class="math inline">\(y_1, \ldots, y_n\)</span>, the log likelihood is thus of the form <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\theta}) = \sum_{k=1}^K \phi_k(\boldsymbol{\theta}) \sum_{i=1}^n t_k(y_i) + n D(\boldsymbol{\theta}),
\end{align*}\]</span> where the collection <span class="math inline">\(\sum_{i=1}^n t_k(y_i)\)</span> (<span class="math inline">\(k=1, \ldots, K\)</span>) are sufficient statistics and <span class="math inline">\(\phi_k(\boldsymbol{\theta})\)</span> are the canonical parameters.</p>
<p>We term <strong>conjugate family</strong> families of distribution on <span class="math inline">\(\boldsymbol{\Theta}\)</span> with parameters <span class="math inline">\(\boldsymbol{\chi}, \gamma\)</span> if their density is proportional to <span class="math display">\[\begin{align*}
\exp\left\{ \sum_{k=1}^K Q_k(\boldsymbol{\theta}) \chi_k + \gamma D(\boldsymbol{\theta})\right\}
\end{align*}\]</span> A log prior density with parameters <span class="math inline">\(\eta, \nu_1, \ldots, \nu_K\)</span> that is proportional to <span class="math display">\[\begin{align*}
\log p(\boldsymbol{\theta}) \propto \eta D(\boldsymbol{\theta}) + \sum_{k=1}^K Q_k(\boldsymbol{\theta}) \nu_k
\end{align*}\]</span> is conjugate.</p>
<!-- p.131 of Barndorff-Nielsen  -->
<p>Exponential families play a crucial role due to the fact that the vector of sufficient statistics <span class="math inline">\(\boldsymbol{t}\)</span> for a random sample allows for data compression. They feature prominently in generalized linear models. <!-- We will encounter exponential families when discussing conjugate priors. --></p>
<div id="exm-exponential-family-gaussian" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Gaussian as exponential family)</strong></span> We can rewrite the density of <span class="math inline">\(\mathsf{Gauss}(\mu, \sigma^2)\)</span> as <span class="math display">\[\begin{align*}
f(y; \mu, \sigma^2) = (2\pi)^{-1/2}\exp\left\{ \frac{-y^2 + 2y\mu - \mu^2}{2\sigma^2}-\log \sigma\right\},
\end{align*}\]</span> so taking <span class="math inline">\(Q_1(\mu, \sigma^2)=\mu/\sigma^2\)</span> and <span class="math inline">\(Q_2(\mu, \sigma^2)=1/\sigma^2\)</span> and <span class="math inline">\(t_1(y) = y\)</span> and <span class="math inline">\(t_2(y) = -y^2/2.\)</span> The <a href="https://en.wikipedia.org/wiki/Normal-inverse-gamma_distribution">Gaussian-inverse-gamma</a> distribution is a conjugate family.</p>
</div>
<div id="exm-exponential-family-binom" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Binomial as exponential family)</strong></span> The binomial log density with <span class="math inline">\(y\)</span> successes out of <span class="math inline">\(n\)</span> trials is proportional to <span class="math display">\[\begin{align*}
y \log(p) + (n-y) \log(1-p) = y\log\left( \frac{p}{1-p}\right) + n \log(1-p)
\end{align*}\]</span> with canonical parameter <span class="math inline">\(Q_1(p) = \mathrm{logit}(p) = \log\{p/(1-p)\}\)</span> with <span class="math inline">\(t_1(y) = y.\)</span> The canonical link function for Bernoulli gives rise to logistic regression model. The binomial distribution is thus an exponential family. The beta distribution is conjugate to the binomial.</p>
</div>
<div id="exm-exponential-family-poisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3 (Poisson as exponential family)</strong></span> Consider <span class="math inline">\(Y \sim \mathsf{Poisson}(\mu)\)</span> with mass function <span class="math display">\[\begin{align*}
f(y; p)=\exp \left\{ - \mu + y \log \mu - \log \Gamma(x+1)\right\}.
\end{align*}\]</span> and so the canonical parameter is <span class="math inline">\(Q_1(p) = \log \mu\)</span> with the gamma distribution as conjugate family.</p>
</div>
<div id="prp-change-variable" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1.1 (Change of variable formula)</strong></span> Consider an injective (one-to-one) differentiable function <span class="math inline">\(\boldsymbol{g}: \mathbb{R}^d \to \mathbb{R}^d,\)</span> with inverse <span class="math inline">\(\boldsymbol{g}^{-1}.\)</span> Then, if <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{g}(\boldsymbol{X}),\)</span> <span class="math display">\[\begin{align*}
\Pr(\boldsymbol{Y} \leq \boldsymbol{y}) = \Pr\{\boldsymbol{g}(\boldsymbol{X}) \leq \boldsymbol{y}\} = \Pr\{\boldsymbol{X} \leq \boldsymbol{x} = \boldsymbol{g}^{-1}(\boldsymbol{y})\}.
\end{align*}\]</span> Using the chain rule, we get that the density of <span class="math inline">\(\boldsymbol{Y}\)</span> may be written as <span class="math display">\[\begin{align*}
f_{\boldsymbol{Y}}(\boldsymbol{y}) = f_{\boldsymbol{X}}\left\{\boldsymbol{g}^{-1}(\boldsymbol{y})\right\} \left| \mathbf{J}_{\boldsymbol{g}^{-1}}(\boldsymbol{y})\right| = f_{\boldsymbol{X}}(\boldsymbol{x}) \left| \mathbf{J}_{\boldsymbol{g}}(\boldsymbol{x})\right|^{-1}
\end{align*}\]</span> where <span class="math inline">\(\mathbf{J}_{\boldsymbol{g}}(\boldsymbol{x})\)</span> is the Jacobian matrix with <span class="math inline">\((i,j)\)</span>th element <span class="math inline">\(\partial [\boldsymbol{g}(\boldsymbol{x})]_i / \partial x_j.\)</span></p>
</div>
<div id="exm-locationscale-Gauss" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4 (Location-scale transformation of Gaussian vectors)</strong></span> Consider <span class="math inline">\(d\)</span> independent standard Gaussian variates <span class="math inline">\(X_j \sim \mathsf{Gauss}(0, 1)\)</span> for <span class="math inline">\(j=1, \ldots, d,\)</span> with joint density function <span class="math display">\[\begin{align*}
f_{\boldsymbol{X}}(\boldsymbol{x})= (2\pi)^{-d/2} \exp \left( - \frac{\boldsymbol{x}^\top\boldsymbol{x}}{2}\right).
\end{align*}\]</span> Consider the transformation <span class="math inline">\(\boldsymbol{Y} = \mathbf{A}\boldsymbol{X}+\boldsymbol{b},\)</span> with <span class="math inline">\(\mathbf{A}\)</span> an invertible matrix. The inverse transformation is <span class="math inline">\(\boldsymbol{g}^{-1}(\boldsymbol{y}) = \mathbf{A}^{-1}(\boldsymbol{y}-\boldsymbol{b}).\)</span> The Jacobian <span class="math inline">\(\mathbf{J}_{\boldsymbol{g}}(\boldsymbol{x})\)</span> is simply <span class="math inline">\(\mathbf{A},\)</span> so the joint density of <span class="math inline">\(\boldsymbol{Y}\)</span> is <span class="math display">\[\begin{align*}
f_{\boldsymbol{Y}}(\boldsymbol{y}) &amp;= (2\pi)^{-d/2} |\mathbf{A}|^{-1}\exp \left\{ - \frac{(\boldsymbol{y}-\boldsymbol{b})^\top\mathbf{A}^{-\top}\mathbf{A}^{-1}(\boldsymbol{y}-\boldsymbol{b})}{2}\right\}.
\end{align*}\]</span> Since <span class="math inline">\(|\mathbf{A}^{-1}| = |\mathbf{A}|^{-1}\)</span> and <span class="math inline">\(\mathbf{A}^{-\top}\mathbf{A}^{-1} = (\mathbf{AA}^\top)^{-1},\)</span> we recover that <span class="math inline">\(\boldsymbol{Y} \sim \mathsf{Gauss}_d(\boldsymbol{b}, \mathbf{AA}^\top).\)</span></p>
</div>
<div id="exm-inverse-gamma" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.5 (Inverse gamma distribution)</strong></span> Consider <span class="math inline">\(Y \sim \mathsf{gamma}(\alpha, \beta)\)</span> and the reciprocal <span class="math inline">\(g(x) = 1/x\)</span>. The Jacobian of the transformation is <span class="math inline">\(|g'(y)| = 1/y^2.\)</span> The density of the inverse gamma <span class="math inline">\(\mathsf{inv. gamma}(\alpha, \beta)\)</span> with shape <span class="math inline">\(\alpha&gt;0\)</span> and scale <span class="math inline">\(\beta&gt;0\)</span> is thus <span class="math display">\[\begin{align*}
f_Y(y) = \frac{\beta^\alpha}{\Gamma(\alpha)} y^{-\alpha-1} \exp(-\beta/y), \qquad y &gt; 0.
\end{align*}\]</span> The expected value and variance are <span class="math inline">\(\mathsf{E}(Y)=\beta/(\alpha-1)\)</span> for <span class="math inline">\(\alpha &gt; 1\)</span> and <span class="math inline">\(\mathsf{Va}(Y) = \beta^2/\{(\alpha-1)^2(\alpha-2)\}\)</span> for <span class="math inline">\(\alpha&gt;2.\)</span></p>
</div>
<div id="prp-gaussian-dist-property" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1.2 (Simulation of Gaussian vectors)</strong></span> <a href="#exm-locationscale-Gauss" class="quarto-xref">Example&nbsp;<span>1.4</span></a> shows that the Gaussian distribution is a location-scale family: if <span class="math inline">\(\boldsymbol{L} = \mathrm{chol}(\boldsymbol{Q}),\)</span> meaning <span class="math inline">\(\boldsymbol{Q}=\boldsymbol{LL}^\top\)</span> for some lower triangular matrix <span class="math inline">\(\boldsymbol{L},\)</span> then <span class="math display">\[\boldsymbol{L}^\top(\boldsymbol{Y}-\boldsymbol{\mu}) \sim \mathsf{Gauss}_d(\boldsymbol{0}_d, \mathbf{I}_d).\]</span> Conversely, we can use the Cholesky root to sample multivariate Gaussian vectors by first drawing <span class="math inline">\(d\)</span> independent standard Gaussians <span class="math inline">\(\boldsymbol{Z}=(Z_1, \ldots, Z_d)^\top,\)</span> then computing <span class="math display">\[\boldsymbol{Y} \gets \boldsymbol{L}^{-1}\boldsymbol{Z}+ \boldsymbol{\mu}.\]</span></p>
</div>
<div id="exm-gamma-Drichlet" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.6 (Dirichlet vectors from Gamma random variables)</strong></span> Consider <span class="math inline">\(\boldsymbol{X}\)</span> a <span class="math inline">\(d\)</span> vector of independent gamma random variables <span class="math inline">\(\mathsf{gamma}(\alpha_i, 1).\)</span> Then, if <span class="math inline">\(Z = X_1 + \cdots + X_d,\)</span> we have <span class="math inline">\((X_1, \ldots, X_{d-1}) / Z  \sim \mathsf{Dirichlet}(\boldsymbol{\alpha})\)</span> and <span class="math inline">\(Z \sim \mathsf{gamma}(1, \alpha_1 + \cdots + \alpha_d).\)</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The joint density for <span class="math inline">\(\boldsymbol{X}\)</span> is <span class="math display">\[\begin{align*}
f_{\boldsymbol{X}}(\boldsymbol{x}) = \prod_{j=1}^d  \frac{x_j^{\alpha_j-1}\exp(-x_j)}{\Gamma(\alpha_j)}.
\end{align*}\]</span> Let <span class="math inline">\(\boldsymbol{g}(\cdot)\)</span> be a <span class="math inline">\(d\)</span> place function with <span class="math inline">\(i\)</span>th element <span class="math inline">\(g_i(\boldsymbol{x}) = x_j/(x_1 + \cdots x_d)\)</span> for <span class="math inline">\(j=1, \ldots, d-1\)</span> and <span class="math inline">\(g_d=x_1 + \cdots x_d\)</span> and write the transformation as <span class="math inline">\(g(\boldsymbol{X}) = (\boldsymbol{Y}^\top, Z)^\top\)</span> with <span class="math inline">\(\boldsymbol{y} = (y_1, \ldots, y_{d-1})^\top\)</span> and the redundant coordinate <span class="math inline">\(y_d = 1- y_1 - \cdots y_{d-1}\)</span> to simplify the notation. The inverse transformation yields <span class="math inline">\(x_j = zy_j\)</span> for <span class="math inline">\(j=1, \ldots, d-1\)</span> and <span class="math inline">\(x_d = z(1-y_1-\cdots - y_{d-1}).\)</span> The Jabobian matrix is <span class="math display">\[\begin{align*}
\mathbf{J}_{\boldsymbol{g}^{-1}}(\boldsymbol{y}, z) = \begin{pmatrix} z \mathbf{I}_{d-1} &amp; \boldsymbol{y} \\ \boldsymbol{0}^\top_{d-1} &amp; y_d
\end{pmatrix}.
\end{align*}\]</span> The absolute value of the determinant is then <span class="math inline">\(z^{d-1}y_d.\)</span> Using the change of variable formula, <span class="math display">\[\begin{align*}
f_{\boldsymbol{Y}}(\boldsymbol{y}) &amp;= \prod_{j=1}^{d}  \frac{(zy_j)^{\alpha_j-1}\exp(-zy_j)}{\Gamma(\alpha_j)} \times z^{d-1}y_d
\\&amp;= z^{\alpha_1 + \cdots + \alpha_d - 1}\exp(-z) \prod_{j=1}^d \frac{y_j^{\alpha_j-1}}{\Gamma(\alpha_j)}.
\end{align*}\]</span> Since the density factorizes, we find the result upon multiplying and dividing by the normalizing constant <span class="math inline">\(\Gamma(\alpha_1 + \cdots + \alpha_d),\)</span> which yields both the Dirichlet for <span class="math inline">\(\boldsymbol{Y}\)</span> and the gamma for <span class="math inline">\(Z.\)</span></p>
</div>
</section>
<section id="marginal-and-conditional-distributions" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="marginal-and-conditional-distributions"><span class="header-section-number">1.1.2</span> Marginal and conditional distributions</h3>
<div id="def-marginal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.14 (Marginal distribution)</strong></span> The <strong>marginal distribution</strong> of a subvector <span class="math inline">\(\boldsymbol{X}_{1:k}=(X_1, \ldots, X_k)^\top,\)</span> without loss of generality consisting of the <span class="math inline">\(k\)</span> first components of <span class="math inline">\(\boldsymbol{X}\)</span> <span class="math inline">\((1 \leq k &lt; d)\)</span> is <span class="math display">\[\begin{align*}
F_{\boldsymbol{X}_{1:k}}(\boldsymbol{x}_{1:k}) = \Pr(\boldsymbol{X}_{1:k} \leq \boldsymbol{x}_{1:k}) = F_{\boldsymbol{X}}(x_1, \ldots, x_k, \infty, \ldots, \infty).
\end{align*}\]</span> and thus the marginal distribution of component <span class="math inline">\(j,\)</span> <span class="math inline">\(F_j(x_j),\)</span> is obtained by evaluating all components but the <span class="math inline">\(j\)</span>th at <span class="math inline">\(\infty.\)</span></p>
<p>We likewise obtain the marginal density <span class="math display">\[\begin{align*}
f_{1:k}(\boldsymbol{x}_{1:k}) = \frac{\partial^k F_{1:k}(\boldsymbol{x}_{1:k})}{\partial x_1 \cdots \partial x_{k}},
\end{align*}\]</span> or through integration from the joint density as <span class="math display">\[\begin{align*}
f_{1:k}(\boldsymbol{x}_{1:k}) = \int_{-\infty}^\infty \cdots  \int_{-\infty}^\infty  f_{\boldsymbol{X}}(x_1, \ldots, x_k, z_{k+1}, \ldots, z_{d}) \mathrm{d} z_{k+1} \cdots \mathrm{d}z_d.
\end{align*}\]</span></p>
</div>
<div id="def-conditional-dist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.15 (Conditional distribution)</strong></span> Let <span class="math inline">\((\boldsymbol{X}^\top, \boldsymbol{Y}^\top)^\top\)</span> be a <span class="math inline">\(d\)</span>-dimensional random vector with joint density or mass function <span class="math inline">\(f_{\boldsymbol{X}, \boldsymbol{Y}}(\boldsymbol{x}, \boldsymbol{y})\)</span> and marginal distribution <span class="math inline">\(f_{\boldsymbol{X}}(\boldsymbol{x}).\)</span> The conditional distribution function of <span class="math inline">\(\boldsymbol{Y}\)</span> given <span class="math inline">\(\boldsymbol{X}=\boldsymbol{x},\)</span> is <span class="math display">\[\begin{align*}
f_{\boldsymbol{Y} \mid \boldsymbol{X}}(\boldsymbol{y}; \boldsymbol{x}) = \frac{f_{\boldsymbol{X}, \boldsymbol{Y}}(\boldsymbol{x}, \boldsymbol{y})}{f_{\boldsymbol{X}}(\boldsymbol{x})}
\end{align*}\]</span> for any value of <span class="math inline">\(\boldsymbol{x}\)</span> in the support of <span class="math inline">\(\boldsymbol{X},\)</span> i.e., the set of values with non-zero density or mass, meaning <span class="math inline">\(f_{\boldsymbol{X}}(\boldsymbol{x})&gt;0\)</span>; it is undefined otherwise.</p>
</div>
<div id="thm-Bayes" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.1 (Bayes’ theorem)</strong></span> Denote by <span class="math inline">\(f_{\boldsymbol{X}}\)</span> and <span class="math inline">\(f_{\boldsymbol{Y}}\)</span> denotes the marginal density of <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{Y},\)</span> respectively, <span class="math inline">\(f_{\boldsymbol{X} \mid \boldsymbol{Y}}\)</span> the conditional of <span class="math inline">\(\boldsymbol{X}\)</span> given <span class="math inline">\(\boldsymbol{Y}\)</span> and <span class="math inline">\(f_{\boldsymbol{X},\boldsymbol{Y}}\)</span> the joint density. Bayes’ theorem states that for <span class="math inline">\(\boldsymbol{y}\)</span> in the support of <span class="math inline">\(\boldsymbol{Y},\)</span> <span class="math display">\[\begin{align*}
f_{\boldsymbol{X}\mid \boldsymbol{Y}}(\boldsymbol{x}; \boldsymbol{y}) = \frac{f_{\boldsymbol{Y}\mid \boldsymbol{X}}(\boldsymbol{y}; \boldsymbol{x})f_{\boldsymbol{X}}(\boldsymbol{x})}{f_{\boldsymbol{Y}}(\boldsymbol{y})}
\end{align*}\]</span> which follows since <span class="math inline">\(f_{\boldsymbol{X}|\boldsymbol{Y}}(\boldsymbol{x}; \boldsymbol{y})f_{\boldsymbol{Y}}(\boldsymbol{y}) = f_{\boldsymbol{X},\boldsymbol{Y}}(\boldsymbol{x},\boldsymbol{y})\)</span> and likewise <span class="math inline">\(f_{\boldsymbol{Y} \mid \boldsymbol{X}}(\boldsymbol{y}; \boldsymbol{x})f_{\boldsymbol{X}}(\boldsymbol{x}) = f_{\boldsymbol{X},\boldsymbol{Y}}(\boldsymbol{x},\boldsymbol{y}).\)</span></p>
<p>In the case of a discrete random variable <span class="math inline">\(X\)</span> with support <span class="math inline">\(\mathcal{X},\)</span> the denominator can be evaluated using the law of total probability, and</p>
<p><span class="math display">\[\begin{align*}
\Pr(X = x \mid Y=y) &amp;= \frac{\Pr(Y=y \mid X=x)\Pr(X=x)}{\Pr(Y=y)} \\&amp;= \frac{\Pr(Y=y \mid X=x)\Pr(X=x)}{\sum_{x \in \mathcal{X}}\Pr(Y =y \mid X=x)\Pr(X=x)}.
\end{align*}\]</span></p>
</div>
<div id="exm-covidrapidtest" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.7 (Covid rapid tests)</strong></span> Back in January 2021, the Quebec government was debating whether or not to distribute antigen rapid test, with <a href="https://www.cbc.ca/news/canada/montreal/quebec-avoids-relying-on-rapid-covid-19-tests-as-pressure-mounts-to-follow-ontario-s-lead-1.5896738">strong reluctance</a> from authorities given the paucity of available resources and the poor sensitivity.</p>
<p>A Swiss study analyse the efficiency of rapid antigen tests, comparing them to repeated polymerase chain reaction (PCR) test output, taken as benchmark <span class="citation" data-cites="Jegerlehner:2021">(<a href="references.html#ref-Jegerlehner:2021" role="doc-biblioref">Jegerlehner et al. 2021</a>)</span>. The results are presented in <a href="#tbl-covid19" class="quarto-xref">Table&nbsp;<span>1.1</span></a></p>
<div id="tbl-covid19" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-covid19-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1.1: Confusion matrix of Covid test results for PCR tests versus rapid antigen tests, from <span class="citation" data-cites="Jegerlehner:2021">Jegerlehner et al. (<a href="references.html#ref-Jegerlehner:2021" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div aria-describedby="tbl-covid19-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">PCR <span class="math inline">\(+\)</span></th>
<th style="text-align: right;">PCR <span class="math inline">\(-\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>rapid <span class="math inline">\(+\)</span></td>
<td style="text-align: right;">92</td>
<td style="text-align: right;">2</td>
</tr>
<tr class="even">
<td>rapid <span class="math inline">\(-\)</span></td>
<td style="text-align: right;">49</td>
<td style="text-align: right;">1319</td>
</tr>
<tr class="odd">
<td>total</td>
<td style="text-align: right;">141</td>
<td style="text-align: right;">1321</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Estimated seropositivity at the end of January 2021 according to projections of the Institute for Health Metrics and Evaluation (IHME) of 8.18M out of 38M inhabitants <span class="citation" data-cites="owidcoronavirus">(<a href="references.html#ref-owidcoronavirus" role="doc-biblioref">Mathieu et al. 2020</a>)</span>, a prevalence of 21.4%. Assuming the latter holds uniformly over the country, what is the probability of having Covid if I get a negative result to a rapid test?</p>
<p>Let <span class="math inline">\(R^{-}\)</span> (<span class="math inline">\(R^{+}\)</span>) denote a negative (positive) rapid test result and <span class="math inline">\(C^{+}\)</span> (<span class="math inline">\(C^{-}\)</span>) Covid positivity (negativity). Bayes’ formula gives <span class="math display">\[\begin{align*}
\Pr(C^{+} \mid R^{-}) &amp; = \frac{\Pr(R^{-} \mid C^{+})\Pr(C^{+})}{\Pr(R^{-} \mid C^{+})\Pr(C^{+}) + \Pr(R^{-} \mid C^{-})\Pr(C^{-})} \\&amp;=
\frac{49/141 \cdot 0.214}{49/141 \cdot 0.214 + 1319/1321 \cdot 0.786}
\end{align*}\]</span> so there is a small, but non-negligible probability of 8.66% that the rapid test result is misleading. <span class="citation" data-cites="Jegerlehner:2021">Jegerlehner et al. (<a href="references.html#ref-Jegerlehner:2021" role="doc-biblioref">2021</a>)</span> indeed found that the sensitivity was 65.3% among symptomatic individuals, but dropped down to 44% for asymptomatic cases. This may have fueled government experts skepticism.</p>
</div>
<p>Bayes’ rule is central to updating beliefs: given initial beliefs (priors) and information in the form of data, we update our beliefs iteratively in light of new information.</p>
<div id="exm-discrete-conditional-marginal" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.8 (Conditional and marginal for contingency table)</strong></span> Consider a bivariate distribution for <span class="math inline">\((Y_1, Y_2)\)</span> supported on <span class="math inline">\(\{1, 2, 3\} \times \{1, 2\},\)</span> whose joint probability mass function is given in <a href="#tbl-bivardiscrete" class="quarto-xref">Table&nbsp;<span>1.2</span></a></p>
<div class="cell" data-tbl-align="center">
<div id="tbl-bivardiscrete" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-tbl-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bivardiscrete-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1.2: Bivariate mass function with probability of each outcome for <span class="math inline">\((Y_1, Y_2).\)</span>
</figcaption>
<div aria-describedby="tbl-bivardiscrete-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\(Y_1=1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(Y_1=2\)</span></th>
<th style="text-align: right;"><span class="math inline">\(Y_1=3\)</span></th>
<th style="text-align: right;">total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(Y_2=1\)</span></td>
<td style="text-align: right;">0.20</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.6</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(Y_2=2\)</span></td>
<td style="text-align: right;">0.15</td>
<td style="text-align: right;">0.2</td>
<td style="text-align: right;">0.05</td>
<td style="text-align: right;">0.4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">total</td>
<td style="text-align: right;">0.35</td>
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">0.15</td>
<td style="text-align: right;">1.0</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>The marginal distribution of <span class="math inline">\(Y_1\)</span> is obtain by looking at the total probability for each column, as <span class="math display">\[\Pr(Y_1=i) = \Pr(Y_1=i, Y_2=1)+ \Pr(Y_1=i, Y_2=2).\]</span> This gives <span class="math inline">\(\Pr(Y_1=1)=0.35,\)</span> <span class="math inline">\(\Pr(Y_1=2)=0.5\)</span> and <span class="math inline">\(\Pr(Y_1=3) = 0.15.\)</span> Similarly, we find that <span class="math inline">\(\Pr(Y_2=1)=0.6\)</span> and <span class="math inline">\(\Pr(Y_2=2)=0.4\)</span> for the other random variable.</p>
<p>The conditional distribution <span class="math display">\[\Pr(Y_2 = i \mid Y_1=2) = \frac{\Pr(Y_1=2, Y_2=i)}{\Pr(Y_1=2)},\]</span> so <span class="math inline">\(\Pr(Y_2 = 1 \mid Y_1=2) = 0.3/0.5 = 0.6\)</span> and <span class="math inline">\(\Pr(Y_2=2 \mid Y_1=2) = 0.4.\)</span> We can condition on more complicated events, for example <span class="math display">\[\begin{align*}
\Pr(Y_2 = i \mid Y_1 \ge 2) = \frac{\Pr(Y_1=2, Y_2=i) + \Pr(Y_1=3, Y_2=i)}{\Pr(Y_1=2) + \Pr(Y_1=3)}.
\end{align*}\]</span></p>
</div>
<div id="exm-marginal-multinom" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.9 (Margins and conditional distributions of multinomial vectors)</strong></span> Consider <span class="math inline">\(\boldsymbol{Y} = (Y_1, Y_2, n-Y_1-Y_2)\)</span> a trinomial vector giving the number of observations in group <span class="math inline">\(j \in \{1,2,3\}\)</span> with <span class="math inline">\(n\)</span> trials and probabilities of each component respectively <span class="math inline">\((p_1, p_2, 1-p_1-p_2).\)</span> The marginal distribution of <span class="math inline">\(Y_2\)</span> is obtained by summing over all possible values of <span class="math inline">\(Y_1,\)</span> which ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(n,\)</span> so <span class="math display">\[\begin{align*}
f(y_2) &amp;= \frac{n!p_2^{y_2}}{y_2!}\sum_{y_1=1}^n \frac{p_1^{y_1}(1-p_1 -p_2)^{n-y_1-y_2}}{y_1!(n-y_1-y_2)!}
\end{align*}\]</span></p>
<p>A useful trick is to complete the expression on the right so that it sum (in the discrete case) or integrate (in the continuous case) to <span class="math inline">\(1.\)</span> If we multiply and divide by <span class="math inline">\((1-p_2)^{n-y_2} /(n-y_2)!,\)</span> we get <span class="math inline">\(p_1^*=p_1/(1-p_2)\)</span> and</p>
<p><span class="math display">\[\begin{align*}
f(y_2) &amp;= \frac{n!p_2^{y_2}}{(1-p_2)^{n-y_2} y_2!(n-y_2)!}\sum_{y_1=1}^n \binom{n-y_2}{y_1} p_1^{\star y_1}(1-p^{\star}_1)^{n-y_2}
\\&amp;= \frac{n!p_2^{y_2}}{(1-p_2)^{n-y_2} y_2!(n-y_2)!}
\end{align*}\]</span> is binomial with <span class="math inline">\(n\)</span> trials and probability of success <span class="math inline">\(p_2.\)</span> We can generalize this argument to multinomials of arbitrary dimensions.</p>
<p>The conditional density of <span class="math inline">\(Y_2 \mid Y_1=y_1\)</span> is, up to proportionality, <span class="math display">\[\begin{align*}
f_{Y_2 \mid Y_1}(y_2; y_1) &amp;\propto \frac{p_2^{y_2}(1-p_1 -p_2)^{n-y_1-y_2}}{y_2!(n-y_1-y_2)!}
\end{align*}\]</span> If we write <span class="math inline">\(p_2^\star=p_2/(1-p_1),\)</span> we find that <span class="math inline">\(Y_2 \mid Y_1 \sim \mathsf{binom}(n-y_1, p_2^\star).\)</span> Indeed, we can see that <span class="math display">\[\begin{align*}
f_{\boldsymbol{Y}}(\boldsymbol{y}) &amp;= f_{Y_2 \mid Y_1}(y_2; y_1) f_{Y_1}(y_1)
\\&amp;= \binom{n-y_1}{y_2} \left( \frac{p_2}{1-p_1}\right)^{y_2}\left(\frac{1-p_1-p_2}{1-p_1}\right)^{n-y_1-y_2}\!\!\cdot\binom{n}{y_1} p_1^{y_1}(1-p_1)^{n-y_1}.
\end{align*}\]</span></p>
<!--and $1-p_2^\star = (1-p_1 -p_2)/(1-p_1),$-->
<!--
Consider $\boldsymbol{Y}=(Y_1, \ldots, Y_J)$ a multinomial vector giving the number of observations in group $j$ (out of $J$) out of $n$ trials. We denote the probability of falling in the latter by $p_j,$ with $p_1 + \cdots + p_J=1.$ The marginal distribution of subcomponents is obtained by marginalizing out variable. To illustrate this, we remove the first component by summing over all possible values of $Y_1,$ which ranges from $0$ to $n.$


\begin{align*}
f(y_2, \ldots, y_{J-1}) &= \frac{n!p_2^{y_2} \cdots p_{J-1}^{y_{J-1}}}{y_2!\cdots y_{J-1}!}\sum_{y_1=1}^n \frac{p_1^{y_1}(1-p_1 - \cdots - p_{J-1})^{n-y_1-\cdots - y_{J-1}}}{y_1!(n-y_1-\cdots -y_{J-1})!}
\end{align*}

A useful trick is to complete the expression on the right so that it sum (in the discrete case) or integrate (in the continuous case) to $1.$ If we multiply and divide by $(1-p_2-\cdots- p_{J-1})^{n-y_2-\cdots - y_{J-1}} /(n-y_2-\cdots -y_{J-1})!,$ we get $p_1^*=p_1/(1-p_1-\cdots - p_{J-1})$ and

\begin{align*}
f(y_2, \ldots, y_{J-1}) &= \frac{n!p_2^{y_2} \cdots p_{J-1}^{y_{J-1}}}{(1-p_2-\cdots- p_{J-1})^{n-y_2-\cdots - y_{J-1}} y_2!\cdots y_{J-1}!(n-y_2-\cdots -y_{J-1})!}\sum_{y_1=1}^n \binom{n-y_2-\cdots - y_{J-1}}{y_1} p_1^{\star y_1}(1-p^{\star}_1)^{n-y_2\cdots - y_{J-1}}
\\&= \frac{n!p_2^{y_2} \cdots p_{J-1}^{y_{J-1}}}{(1-p_2-\cdots- p_{J-1})^{n-y_2-\cdots - y_{J-1}} y_2!\cdots y_{J-1}!(n-y_2-\cdots -y_{J-1})!}
\end{align*}
is again multinomial with $n$ trials, but probability of success $p_j/(1-p_2-\cdots- p_{J-1})$ for $j=2, \ldots, J.$ We can iterate this to deduce that the pairwise margins are binomial distributed. 
-->
</div>
<div id="exm-conditional-marginal" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.10 (Gaussian-gamma model)</strong></span> Consider the bivariate density function of the pair <span class="math inline">\((X, Y),\)</span> where for <span class="math inline">\(\lambda&gt;0,\)</span> <span class="math display">\[\begin{align*}
f(x,y) = \frac{\lambda y^{1/2}}{(2\pi)^{1/2}}\exp\left\{ - y(x^2 + \lambda)\right\}, \qquad x \in \mathbb{R}, y&gt;0.
\end{align*}\]</span> We see that the conditional distribution of <span class="math inline">\(X \mid Y=y \sim \mathsf{Gauss}(0, y^{-1}).\)</span> The marginals are <span class="math display">\[\begin{align*}
f(y) &amp;= \int_{-\infty}^\infty \frac{\lambda y^{1/2}}{(2\pi)^{1/2}}\exp\left\{ - y(x^2 + \lambda)\right\} \mathrm{d} x
\\&amp;= \lambda \exp(-\lambda y)
\end{align*}\]</span> so marginally <span class="math inline">\(Y\)</span> follows an exponential distribution with rate <span class="math inline">\(\lambda.\)</span> The marginal of <span class="math inline">\(X\)</span> can be obtained by noting that the joint distribution, as a function of <span class="math inline">\(y,\)</span> is proportional to the kernel of a gamma distribution with shape <span class="math inline">\(3/2\)</span> and rate <span class="math inline">\(x^2+\lambda,\)</span> with <span class="math inline">\(Y \mid X=x \sim \mathsf{gamma}(3/2, x^2+\lambda).\)</span> If we pull out the normalizing constant, we find <span class="math display">\[\begin{align*}
f(x) &amp;= \int_{0}^{\infty} \frac{\lambda y^{1/2}}{(2\pi)^{1/2}}\exp\left\{ - y(x^2 + \lambda)\right\} \mathrm{d} y\\&amp;=
\frac{\lambda \Gamma(3/2)}{(2\pi)^{1/2}(x^2+\lambda)^{3/2}} \int_0^\infty f_{Y \mid X}(y \mid x) \mathrm{d} y \\ &amp;= \frac{\lambda}{2^{3/2}(x^2+\lambda)^{3/2}}
\end{align*}\]</span> since <span class="math inline">\(\Gamma(a+1) = a\Gamma(a)\)</span> for <span class="math inline">\(a&gt;0\)</span> and <span class="math inline">\(\Gamma(1/2) = \sqrt{\pi}.\)</span> We conclude that marginally <span class="math inline">\(X \sim \mathsf{Student}(0, \lambda, 2),\)</span> a Student distribution with scale <span class="math inline">\(\lambda\)</span> and two degrees of freedom.</p>
</div>
<!--

:::{#exm-conditional-poisson-sum}

## Poisson counts conditional on their sum

Suppose that $Y_j\sim \mathsf{Poisson}(\lambda_j)$ are independent $(j=1, \ldots, J),$ so that the sum $S = Y_1 + \cdots Y_J \sim \mathsf{Poisson}(\lambda_1 + \ldots \lambda_J).$ One may prove this fact using moment generating functions, since the Poisson is infinitely divisible.

Let $f(y; \lambda_j)$ denote the Poisson mass function with rate $\lambda_j.$ Since the random variables $Y_1, \ldots, Y_J$ are independent, the joint mass function factorizes as
\begin{align*}
&f(y_1, \ldots, y_J=y_j \mid S=n) 
= \frac{\prod_{j=1}^J f(y_j; \lambda_j)}{f(n; \lambda_1 + \cdots + \lambda_J)}
\\&\quad=\prod_{j=1}^J\frac{\exp(-\lambda_j)\lambda_j^{y_j}}{y_j!}\frac{n!}{\exp(-\lambda_1 + \cdots \lambda_J) \lambda^{n}}
\\&\quad= \frac{n!}{ (\lambda_1 + \cdots + \lambda_J)^{n}}\prod_{j=1}^J\frac{\lambda_j^{y_j}}{y_j!} 
\\&\quad=\frac{n!}{y_1! \cdots y_J!}\prod_{j=1}^J \left(\frac{\lambda_j}{\lambda_1 + \cdots + \lambda_J}\right)^{y_j}
\end{align*}
for $y_j \geq 0$ for all $j=1, \ldots, J$ such that $y_1 + \cdots + y_j = n,$ and the joint mass is zero otherwise. If we define $p_j = \lambda_j/(\lambda_1 + \cdots + \lambda_J),$ we recognize a multinomial distribution with vector of probability of success $\boldsymbol{p}$ with $p_1 + \cdots p_J=1$ and $n$ trials.

:::

-->
<div id="exm-biv-geometric" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.11 (Bivariate geometric distribution of Marshall and Olkin)</strong></span> Consider a couple <span class="math inline">\((U_1, U_2)\)</span> of Bernoulli random variables whose mass function is <span class="math inline">\(\Pr(U_1=i, U_2=j=p_{ij}\)</span> for <span class="math inline">\((i,j) \in \{0,1\}^2.\)</span> The marginal distributions are, by the law of total probability <span class="math display">\[\begin{align*}
\Pr(U_1=i) &amp;= \Pr(U_1=i, U_2=0) + \Pr(U_1=i, U_2=1) = p_{i0} + p_{i1} = p_{i\bullet}\\
\Pr(U_2=j) &amp;= \Pr(U_1=0, U_2=j) + \Pr(U_1=1, U_2=j) = p_{0j} + p_{1j} = p_{\bullet j}
\end{align*}\]</span> We consider a joint geometric distribution (<span class="citation" data-cites="Marshall.Olkin:1985">Marshall and Olkin (<a href="references.html#ref-Marshall.Olkin:1985" role="doc-biblioref">1985</a>)</span>, Section 6) and the pair <span class="math inline">\((Y_1, Y_2)\)</span> giving the number of zeros for <span class="math inline">\((U_1, U_2)\)</span> before the variable equals one for the first time. The bivariate mass function is <span class="citation" data-cites="Nadarajah:2008">(<a href="references.html#ref-Nadarajah:2008" role="doc-biblioref">Nadarajah 2008</a>)</span> <span class="math display">\[\begin{align*}
\Pr(Y_1 = k, Y_2 = l) &amp;=
\begin{cases}
p_{00}^kp_{01}p_{0 \bullet}^{l-k-1}p_{1 \bullet} &amp; 0 \leq k &lt; l; \\
p_{00}^kp_{11} &amp; k=l; \\
p_{00}^lp_{10}p_{\bullet 0}^{k-l-1}p_{\bullet 1} &amp; 0 \leq l &lt; k.
\end{cases}
\end{align*}\]</span> We can compute the joint survival function <span class="math inline">\(\Pr(Y_1 \ge k, Y_2 \ge l)\)</span> by using properties of the partial sum of geometric series, using the fact <span class="math inline">\(\sum_{i=0}^n p^i = p^n/(1-p).\)</span> Thus, for the case <span class="math inline">\(0\leq k &lt; l,\)</span> we have <span class="math display">\[\begin{align*}
\Pr(Y_1 \ge k, Y_2 \ge l) &amp;= \sum_{i=k}^\infty \sum_{j=l}^\infty \Pr(Y_1 = i, Y_2 = j)\\
&amp;= \sum_{i=k}^\infty p_{00}^ip_{01}p_{0 \bullet}^{-i-1}p_{1 \bullet} \sum_{j=l}^\infty p_{0 \bullet}^{j}
\\&amp;=\sum_{i=k}^\infty p_{00}^ip_{01}p_{0 \bullet}^{-i-1}p_{1 \bullet} \frac{p_{0 \bullet}^{l}}{1-p_{0\bullet}} \\&amp;= p_{0 \bullet}^{l-1}p_{01} \sum_{i=k}^\infty \left(\frac{p_{00}}{p_{0 \bullet}}\right)^{i}
\\&amp; = p_{00}^k p_{0 \bullet}^{l-k}
\end{align*}\]</span> since <span class="math inline">\(p_{0\bullet} + p_{1\bullet} = 1.\)</span> We can proceed similarly with other subcases to find <span class="math display">\[\begin{align*}
\Pr(Y_1 \ge k, Y_2 \ge l) = \begin{cases}
p_{00}^k p_{0 \bullet}^{l-k} &amp; 0\leq k &lt; l\\
p_{00}^k &amp; 0 \leq k=l \\
p_{00}^l p_{\bullet 0}^{k-l} &amp; 0\leq l &lt; k\\
\end{cases}
\end{align*}\]</span> and we can obtain the marginal survival function by considering <span class="math inline">\(\Pr(Y_1 \ge 0, Y_2 \ge l),\)</span> etc., which yields <span class="math inline">\(\Pr(Y_2 \ge l) = p_{0 \bullet}^{l},\)</span> whence <span class="math display">\[\begin{align*}
\Pr(Y_2 = l) &amp;= \Pr(Y_2 \ge l) -\Pr(Y_2 \ge l+1)
\\&amp;= p_{0 \bullet}^{l} (1-p_{0 \bullet})
\\&amp;=p_{0 \bullet}^{l}p_{1\bullet}
\end{align*}\]</span> and so both margins are geometric.</p>
</div>
<div id="def-independence" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.16 (Independence)</strong></span> We say that <span class="math inline">\(\boldsymbol{Y}\)</span> and <span class="math inline">\(\boldsymbol{X}\)</span> are independent if their joint distribution function factorizes as <span class="math display">\[\begin{align*}
F_{\boldsymbol{X}, \boldsymbol{Y}}(\boldsymbol{x}, \boldsymbol{y}) = F_{\boldsymbol{X}}(\boldsymbol{x})F_{\boldsymbol{Y}}(\boldsymbol{y})
\end{align*}\]</span> for any value of <span class="math inline">\(\boldsymbol{x},\)</span> <span class="math inline">\(\boldsymbol{y}.\)</span> It follows from the definition of joint density that, should the latter exists, it also factorizes as <span class="math display">\[\begin{align*}
f_{\boldsymbol{X}, \boldsymbol{Y}}(\boldsymbol{x}, \boldsymbol{y}) = f_{\boldsymbol{X}}(\boldsymbol{x})f_{\boldsymbol{Y}}(\boldsymbol{y}).
\end{align*}\]</span></p>
<p>If two subvectors <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{Y}\)</span> are independent, then the conditional density <span class="math inline">\(f_{\boldsymbol{Y} \mid \boldsymbol{X}}(\boldsymbol{y}; \boldsymbol{x})\)</span> equals the marginal <span class="math inline">\(f_{\boldsymbol{Y}}(\boldsymbol{y}).\)</span></p>
</div>
<div id="prp-independence" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1.3 (Gaussian vectors, independence and conditional independence properties)</strong></span> &nbsp;</p>
<!--
Simulation via Cholesky root
Kriging
Joint distribution, conditional
-->
<p>A unique property of the multivariate normal distribution is the link between independence and the covariance matrix: components <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> are independent if and only if the <span class="math inline">\((i,j)\)</span> off-diagonal entry of the covariance matrix <span class="math inline">\(\boldsymbol{Q}^{-1}\)</span> is zero.</p>
<p>If <span class="math inline">\(q_{ij}=0,\)</span> then <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> are conditionally independent given the other components.</p>
</div>
</section>
</section>
<section id="expectations" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="expectations"><span class="header-section-number">1.2</span> Expectations</h2>
<p>The expected value of some function of a random vector <span class="math inline">\(g(\boldsymbol{Y}),\)</span> where <span class="math inline">\(\boldsymbol{Y}\)</span> has density <span class="math inline">\(f_{\boldsymbol{Y}},\)</span> is <span class="math display">\[\begin{align*}
\mathsf{E}\{g(\boldsymbol{Y})\} = \int g(\boldsymbol{y}) f_{\boldsymbol{Y}}(\boldsymbol{y}) \mathrm{d} \boldsymbol{y}
\end{align*}\]</span> and can be understood as a weighted integral of <span class="math inline">\(g\)</span> with weight <span class="math inline">\(f_{\boldsymbol{Y}}\)</span>; the latter does not exist unless the integral is finite.</p>
<p>Taking <span class="math inline">\(g(\boldsymbol{y}) = \boldsymbol{y}\)</span> yields the <strong>expected value</strong> of the random variable <span class="math inline">\(\mathsf{E}(\boldsymbol{Y}).\)</span> We define the covariance matrix of <span class="math inline">\(\boldsymbol{Y}\)</span> as <span class="math display">\[\begin{align*}
\mathsf{Va}(\boldsymbol{Y}) = \mathsf{E}\left[\left\{\boldsymbol{Y} - \mathsf{E}(\boldsymbol{Y})\right\}\left\{\boldsymbol{Y} - \mathsf{E}(\boldsymbol{Y})\right\}^\top\right],
\end{align*}\]</span> which reduces in the unidimensional setting to <span class="math display">\[\mathsf{Va}(Y) = \mathsf{E}\{Y - \mathsf{E}(Y)\}^2 = \mathsf{E}(Y^2) - \mathsf{E}(Y)^2.\]</span></p>
<p>More generally, the <span class="math inline">\(k \times m\)</span> covariance matrix between two random vectors <span class="math inline">\(\boldsymbol{Y}\)</span> of size <span class="math inline">\(k\)</span> and <span class="math inline">\(\boldsymbol{X}\)</span> of size <span class="math inline">\(m\)</span> is <span class="math display">\[\begin{align*}
\mathsf{Co}(\boldsymbol{Y}, \boldsymbol{X}) = \mathsf{E}\left[\left\{\boldsymbol{Y} - \mathsf{E}(\boldsymbol{Y})\right\}\left\{\boldsymbol{X} - \mathsf{E}(\boldsymbol{X})\right\}^\top\right],
\end{align*}\]</span></p>
<p>If <span class="math inline">\(\boldsymbol{Y}\)</span> is <span class="math inline">\(d\)</span>-dimensional and <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(p \times d\)</span> and <span class="math inline">\(\boldsymbol{b}\)</span> is a <span class="math inline">\(p\)</span> vector, then</p>
<p><span class="math display">\[\begin{align*}
\mathsf{E}(\boldsymbol{AY} + \boldsymbol{b}) &amp;= \boldsymbol{A}\mathsf{E}(\boldsymbol{Y}) + \boldsymbol{b},\\
\mathsf{Va}(\boldsymbol{AY} + \boldsymbol{b}) &amp;= \boldsymbol{A}\mathsf{Va}(\boldsymbol{Y})\boldsymbol{A}^\top.
\end{align*}\]</span></p>
<p>The expected value (theoretical mean) of the vector <span class="math inline">\(\boldsymbol{Y}\)</span> is thus calculated componentwise using each marginal density, i.e., <span class="math display">\[\begin{align*}
\mathsf{E}(\boldsymbol{Y}) &amp;= \boldsymbol{\mu}=
\begin{pmatrix}
\mathsf{E}(Y_1) &amp;
\cdots  &amp;
\mathsf{E}(Y_n)
\end{pmatrix}^\top
\end{align*}\]</span> whereas the second moment of <span class="math inline">\(\boldsymbol{Y}\)</span> is encoded in the <span class="math inline">\(n \times n\)</span> <strong>covariance</strong> matrix <span class="math display">\[\begin{align*}
\mathsf{Va}(\boldsymbol{Y}) &amp;= \boldsymbol{\Sigma} = \begin{pmatrix} \mathsf{Va}(Y_1) &amp; \mathsf{Co}(Y_1, Y_2)  &amp; \cdots &amp; \mathsf{Co}(Y_1, Y_n) \\
\mathsf{Co}(Y_2, Y_1) &amp; \mathsf{Va}(Y_2) &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
\mathsf{Co}(Y_n, Y_1) &amp; \mathsf{Co}(Y_n, Y_2) &amp;\cdots &amp; \mathsf{Va}(Y_n)
\end{pmatrix}
\end{align*}\]</span> The <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\boldsymbol{\Sigma},\)</span> <span class="math inline">\(\sigma_{ii}=\sigma_i^2,\)</span> is the variance of <span class="math inline">\(Y_i,\)</span> whereas the off-diagonal entries <span class="math inline">\(\sigma_{ij}=\sigma_{ji}\)</span> <span class="math inline">\((i \neq j)\)</span> are the covariance of pairwise entries, with <span class="math display">\[\begin{align*}
\mathsf{Co}(Y_i, Y_j) &amp;= \int_{\mathbb{R}^2} (y_i-\mu_i)(y_j-\mu_j) f_{Y_i, Y_j}(y_i, y_j) \mathrm{d} y_i \mathrm{d} y_j \\
&amp;= \mathsf{E}_{Y_i, Y_j}\left[\left\{Y_i-\mathsf{E}_{Y_i}(Y_i)\right\}\left\{Y_j-\mathsf{E}_{Y_j}(Y_j)\right\}\right]
\end{align*}\]</span> The covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is thus symmetric. It is customary to normalize the pairwise dependence so they do not depend on the component variance. The linear <strong>correlation</strong> between <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> is <span class="math display">\[\begin{align*}
\rho_{ij}=\mathsf{Cor}(Y_i,Y_j)=\frac{\mathsf{Co}(Y_i, Y_j)}{\sqrt{\mathsf{Va}(Y_i)}\sqrt{\mathsf{Va}(Y_j)}}=\frac{\sigma_{ij}}{\sigma_i\sigma_j}.
\end{align*}\]</span></p>
<div id="prp-iteratedexpectation" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1.4 (Law of iterated expectation and variance)</strong></span> Let <span class="math inline">\(\boldsymbol{Z}\)</span> and <span class="math inline">\(\boldsymbol{Y}\)</span> be random vectors. The expected value of <span class="math inline">\(\boldsymbol{Y}\)</span> is <span class="math display">\[\begin{align*}
\mathsf{E}_{\boldsymbol{Y}}(\boldsymbol{Y}) = \mathsf{E}_{\boldsymbol{Z}}\left\{\mathsf{E}_{\boldsymbol{Y} \mid \boldsymbol{Z}}(\boldsymbol{Y})\right\}.
\end{align*}\]</span></p>
<p>The <strong>tower</strong> property gives a law of iterated variance <span class="math display">\[\begin{align*}
\mathsf{Va}_{\boldsymbol{Y}}(\boldsymbol{Y}) = \mathsf{E}_{\boldsymbol{Z}}\left\{\mathsf{Va}_{\boldsymbol{Y} \mid \boldsymbol{Z}}(\boldsymbol{Y})\right\} + \mathsf{Va}_{\boldsymbol{Z}}\left\{\mathsf{E}_{\boldsymbol{Y} \mid \boldsymbol{Z}}(\boldsymbol{Y})\right\}.
\end{align*}\]</span></p>
<p>In a hierarchical model, the variance of the unconditional distribution is thus necessarily larger than that of the conditional distribution.</p>
</div>
<div id="exm-iteratedmean-gauss" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.12</strong></span> Let <span class="math inline">\(Y \mid X\sim \mathsf{Gauss}(X, \sigma^2)\)</span> and <span class="math inline">\(X \sim \mathsf{Gauss}(0, \tau^2).\)</span> The unconditional mean and variance of <span class="math inline">\(Y\)</span> are <span class="math display">\[\begin{align*}
\mathsf{E}(Y) = \mathsf{E}_{X}\{\mathsf{E}_{Y\mid X}(Y)\}= \mathsf{E}_{X}(X) = 0
\end{align*}\]</span> and <span class="math display">\[\begin{align*}
\mathsf{Va}(Y) &amp;= \mathsf{E}_{X}\{\mathsf{Va}_{Y\mid X}(Y)\} + \mathsf{Va}_{X}\{\mathsf{E}_{Y\mid X}(Y)\} \\&amp;= \mathsf{E}_{X}(\sigma^2) + \mathsf{Va}_{X}(X)
\\&amp;= \sigma^2 + \tau^2
\end{align*}\]</span></p>
</div>
<div id="exm-poisson-negbin" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.13 (Negative binomial as a Poisson mixture)</strong></span> &nbsp;</p>
<!-- Some canonical distributions have only one parameter which dictates moments: for a Poisson random variable $Y \sim \mathsf{Poisson}(\lambda),$ both $\mathsf{E}(Y) = \mathsf{Va}(Y) = \lambda,$ whereas if we have binomial data with $n$ independent trials, each with probability of success $p,$ then $Y \sim \mathsf{Bin}(n, p),$ then the theoretical mean of successes is $\mathsf{E}(Y)=np$ and the variance $\mathsf{Va}(Y) = np(1-p).$ -->
<!-- In generalized linear models, we model the mean as a function of covariate, but data may exhibit overdispersion relative to the theoretical mean. -->
<p>One restriction of the Poisson model is that the restriction on its moments is often unrealistic. The most frequent problem encountered is that of <strong>overdispersion</strong>, meaning that the variability in the counts is larger than that implied by a Poisson distribution.</p>
<p>One common framework for handling overdispersion is to have <span class="math inline">\(Y \mid \Lambda = \lambda \sim \mathsf{Poisson}(\lambda),\)</span> where the mean of the Poisson distribution is itself a positive random variable with mean <span class="math inline">\(\mu,\)</span> if <span class="math inline">\(\Lambda\)</span> follows a gamma distribution with shape <span class="math inline">\(k\mu\)</span> and rate <span class="math inline">\(k&gt;0,\)</span> <span class="math inline">\(\Lambda \sim \mathsf{gamma}(k\mu, k).\)</span> Since the joint density of <span class="math inline">\(Y\)</span> and <span class="math inline">\(\Lambda\)</span> can be written <span class="math display">\[\begin{align*}
p(y, \lambda) &amp;= p(y \mid \lambda)p(\lambda) \\
&amp;= \frac{\lambda^y\exp(-\lambda)}{\Gamma(y+1)}  \frac{k^{k\mu}\lambda^{k\mu-1}\exp(-k\lambda)}{\Gamma(k\mu)}
\end{align*}\]</span> so the conditional distribution of <span class="math inline">\(\Lambda \mid Y=y\)</span> can be found by considering only terms that are function of <span class="math inline">\(\lambda,\)</span> whence <span class="math display">\[\begin{align*}
f(\lambda \mid Y=y) \stackrel{\lambda}{\propto}\lambda^{y+k\mu-1}\exp(-(k+1)\lambda)
\end{align*}\]</span> and the conditional distribution is <span class="math inline">\(\Lambda \mid Y=y \sim \mathsf{gamma}(k\mu + y, k+1).\)</span></p>
<p>We can isolate the marginal density <span class="math display">\[\begin{align*}
p(y) &amp;= \frac{p(y, \lambda)}{p(\lambda \mid y)} \\&amp;= \frac{\frac{\lambda^y\exp(-\lambda)}{\Gamma(y+1)}  \frac{k^{k\mu}\lambda^{k\mu-1}\exp(-k\lambda)}{\Gamma(k\mu)}}{ \frac{(k+1)^{k\mu+y}\lambda^{k\mu+y-1}\exp\{-(k+1)\lambda\}}{\Gamma(k\mu+y)}}\\
&amp;= \frac{\Gamma(k\mu+y)}{\Gamma(k\mu)\Gamma(y+1)}k^{k\mu} (k+1)^{-k\mu-y}\\&amp;= \frac{\Gamma(k\mu+y)}{\Gamma(k\mu)\Gamma(y+1)}\left(1-\frac{1}{k+1}\right)^{k\mu} \left(\frac{1}{k+1}\right)^y
\end{align*}\]</span> and this is the density of a negative binomial distribution with probability of success <span class="math inline">\(1/(k+1).\)</span> We can thus view the negative binomial as a Poisson mean mixture.</p>
<p>By the laws of iterated expectation and iterative variance, <span class="math display">\[\begin{align*}
\mathsf{E}(Y) &amp;= \mathsf{E}_{\Lambda}\{\mathsf{E}(Y \mid \Lambda\} \\&amp; = \mathsf{E}(\Lambda) = \mu\\
\mathsf{Va}(Y) &amp;= \mathsf{E}_{\Lambda}\{\mathsf{Va}(Y \mid \Lambda)\} + \mathsf{Va}_{\Lambda}\{\mathsf{E}(Y \mid \Lambda)\} \\&amp;= \mathsf{E}(\Lambda) + \mathsf{Va}(\Lambda) \\&amp;= \mu + \mu/k.
\end{align*}\]</span> The marginal distribution of <span class="math inline">\(Y,\)</span> unconditionally, has a variance which exceeds its mean, as <span class="math display">\[\begin{align*}
\mathsf{E}(Y) = \mu, \qquad \mathsf{Va}(Y) = \mu (1+1/k).
\end{align*}\]</span> In a negative binomial regression model, the term <span class="math inline">\(k\)</span> is a dispersion parameter, which is fixed for all observations, whereas <span class="math inline">\(\mu = \exp(\boldsymbol{\beta}\mathbf{X})\)</span> is a function of covariates <span class="math inline">\(\mathbf{X}.\)</span> As <span class="math inline">\(k \to \infty,\)</span> the distribution of <span class="math inline">\(\Lambda\)</span> degenerates to a constant at <span class="math inline">\(\mu\)</span> and we recover the Poisson model.</p>
</div>
<div id="prp-partition-matrix" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1.5 (Partitioning of covariance matrices)</strong></span> Let <span class="math inline">\(\boldsymbol{\Sigma}\)</span> be a <span class="math inline">\(d \times d\)</span> positive definite covariance matrix. We define the precision matrix <span class="math inline">\(\boldsymbol{Q} = \boldsymbol{\Sigma}^{-1}.\)</span> Suppose the matrices are partitioned into blocks, <span class="math display">\[\begin{align*}
\boldsymbol{\Sigma}=
\begin{pmatrix}
\boldsymbol{\Sigma}_{11} &amp; \boldsymbol{\Sigma}_{12} \\
\boldsymbol{\Sigma}_{21} &amp; \boldsymbol{\Sigma}_{22}
\end{pmatrix} \text{ and }
\boldsymbol{\Sigma}^{-1}= \boldsymbol{Q} =
\begin{pmatrix}
\boldsymbol{Q}_{11} &amp;\boldsymbol{Q}_{12}
\\ \boldsymbol{Q}_{21} &amp; \boldsymbol{Q}_{22}
\end{pmatrix}
\end{align*}\]</span> with <span class="math inline">\(\dim(\boldsymbol{\Sigma}_{11})=k\times k\)</span> and <span class="math inline">\(\dim(\boldsymbol{\Sigma}_{22})=(d-k) \times (d-k).\)</span> The following relationships hold:</p>
<ul>
<li><span class="math inline">\(\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}=-\boldsymbol{Q}_{11}^{-1}\boldsymbol{Q}_{12}\)</span></li>
<li><span class="math inline">\(\boldsymbol{\Sigma}_{11}-\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}=\boldsymbol{Q}_{11}^{-1}\)</span></li>
<li><span class="math inline">\(\det(\boldsymbol{\Sigma})=\det(\boldsymbol{\Sigma}_{22})\det(\boldsymbol{\Sigma}_{1|2})\)</span> where <span class="math inline">\(\boldsymbol{\Sigma}_{1|2}=\boldsymbol{\Sigma}_{11}-\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}.\)</span></li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By writing explicitly the relationship <span class="math inline">\(\mathbf{Q}
\boldsymbol{\Sigma}=\mathbf{I}_n,\)</span> we get <span class="math display">\[\begin{eqnarray*}
\mathbf{Q}_{11}\boldsymbol{\Sigma}_{11}+\mathbf{Q}_{12}\boldsymbol{\Sigma}_{21}&amp;=&amp;\mathbf{I}_{k}\\
\mathbf{Q}_{21}\boldsymbol{\Sigma}_{12}+\mathbf{Q}_{22}\boldsymbol{\Sigma}_{22}&amp;=&amp;\mathbf{I}_{p-k}\\
\mathbf{Q}_{21}\boldsymbol{\Sigma}_{11}+\mathbf{Q}_{22}\boldsymbol{\Sigma}_{21}&amp;=&amp;\mathbf{O}_{p-k, k}\\
\mathbf{Q}_{11}\boldsymbol{\Sigma}_{12}+\mathbf{Q}_{12}\boldsymbol{\Sigma}_{22}&amp;=&amp;\mathbf{O}_{k, p-k}.
\end{eqnarray*}\]</span> Recall that we can only invert matrices whose double indices are identical and that both <span class="math inline">\(\mathbf{Q}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> are symmetric, so <span class="math inline">\(\boldsymbol{\Sigma}_{12} = \boldsymbol{\Sigma}_{21}^\top.\)</span> One easily obtains <span class="math inline">\(\boldsymbol{\Sigma}_{12} \boldsymbol{\Sigma}_{22}^{-1}=-\mathbf{Q}_{11}^{-1}\mathbf{Q}_{12}\)</span> making use of the last equation. Then, <span class="math inline">\(\boldsymbol{\Sigma}_{11}-\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}=\mathbf{Q}_{11}^{-1}\)</span> by substituting <span class="math inline">\(\mathbf{Q}_{12}\)</span> from the last equation into the first.</p>
<p>For the last result, take <span class="math inline">\(\boldsymbol{B}\coloneqq \left(\begin{smallmatrix}
\mathbf{I} &amp; -\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}^{-1}_{22}
\\ \mathbf{O} &amp; \mathbf{I} \end{smallmatrix}\right),\)</span> noting that <span class="math inline">\(\det (\boldsymbol{B})=\det\big( \boldsymbol{B}^\top\big)=1
.\)</span> Computing the quadratic form <span class="math inline">\(\boldsymbol{B}\boldsymbol{\Sigma}\boldsymbol{B}^\top,\)</span> we get <span class="math inline">\(\det(\boldsymbol{\Sigma})=\det(\boldsymbol{\Sigma}_{22})\det(\boldsymbol{\Sigma}_{1|2})\)</span> where <span class="math inline">\(\boldsymbol{\Sigma}_{1|2}=\boldsymbol{\Sigma}_{11}-\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}.\)</span></p>
</div>
<div id="prp-conditional-gaussian" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1.6 (Conditional distribution of Gaussian vectors)</strong></span> Let <span class="math inline">\(\boldsymbol{Y} \sim \mathsf{Gauss}_d(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span> and consider the partition <span class="math display">\[\begin{align*}
\boldsymbol{Y} = \begin{pmatrix} \boldsymbol{Y}_1 \\ \boldsymbol{Y}_2\end{pmatrix}, \quad
\boldsymbol{\mu} = \begin{pmatrix} \boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2\end{pmatrix}, \quad
\boldsymbol{\Sigma} = \begin{pmatrix} \boldsymbol{\Sigma}_{11} &amp; \boldsymbol{\Sigma}_{12}\\ \boldsymbol{\Sigma}_{21} &amp; \boldsymbol{\Sigma}_{22}\end{pmatrix},
\end{align*}\]</span> where <span class="math inline">\(\boldsymbol{Y}_1\)</span> is a <span class="math inline">\(k \times 1\)</span> and <span class="math inline">\(\boldsymbol{Y}_2\)</span> is a <span class="math inline">\((d-k) \times 1\)</span> vector for some <span class="math inline">\(1\leq k &lt; d.\)</span> Then, we have the conditional distribution <span class="math display">\[\begin{align*}
\boldsymbol{Y}_1 \mid \boldsymbol{Y}_2 =\boldsymbol{y}_2 &amp;\sim \mathsf{Gauss}_k(\boldsymbol{\mu}_1+\boldsymbol{\Sigma}_{12} \boldsymbol{\Sigma}_{22}^{-1}(\boldsymbol{y}_2-\boldsymbol{\mu}_2), \boldsymbol{\Sigma}_{1|2})
\\&amp; \sim  \mathsf{Gauss}_k(\boldsymbol{\mu}_1-\boldsymbol{Q}_{11}^{-1}\boldsymbol{Q}_{12}(\boldsymbol{y}_2-\boldsymbol{\mu}_2), \boldsymbol{Q}^{-1}_{11})
\end{align*}\]</span> and <span class="math inline">\(\boldsymbol{\Sigma}_{1|2}=\boldsymbol{\Sigma}_{11}-\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}\)</span> is the Schur complement of <span class="math inline">\(\boldsymbol{\Sigma}_{22}.\)</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>It is easier to obtain this result by expressing the density of the Gaussian distribution in terms of the precision matrix <span class="math inline">\(\boldsymbol{Q}= \left(\begin{smallmatrix}\boldsymbol{Q}_{11} &amp; \boldsymbol{Q}_{12}\\
\boldsymbol{Q}_{21} &amp; \boldsymbol{Q}_{22}\end{smallmatrix}\right)\)</span> rather than in terms of the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}.\)</span></p>
<p>Consider the partition <span class="math inline">\(\boldsymbol{Y}=(\boldsymbol{Y}_1, \boldsymbol{Y}_2).\)</span> The log conditional density <span class="math inline">\(\log f(\boldsymbol{y}_1 \mid \boldsymbol{y}_2)\)</span> as a function of <span class="math inline">\(\boldsymbol{y}_1\)</span> is, up to proportionality, <span class="math display">\[\begin{align*}
&amp;-\frac{1}{2}\left(\boldsymbol{y}_1-\boldsymbol{\mu}_1\right)^\top
\boldsymbol{Q}_{11}\left(\boldsymbol{y}_1-\boldsymbol{\mu}_1\right) - \left(\boldsymbol{y}_1-\boldsymbol{\mu}_1\right)^\top
\boldsymbol{Q}_{12}\left(\boldsymbol{y}_2-\boldsymbol{\mu}_2\right)\\
&amp;-\frac{1}{2}\boldsymbol{y}_1^\top\boldsymbol{Q}_{11}\boldsymbol{y}_1-\boldsymbol{y}_1^\top
\left\{\boldsymbol{Q}_{11}\boldsymbol{\mu}_1-\boldsymbol{Q}_{12}\left(\boldsymbol{y}_2-\boldsymbol{\mu}_2\right)\right\}
\end{align*}\]</span> upon completing the square in <span class="math inline">\(\boldsymbol{y}_1.\)</span> This integrand is proportional to the density of a Gaussian distribution (and hence must be Gaussian) with precision matrix <span class="math inline">\(\boldsymbol{Q}_{11},\)</span> while the mean vector and covariance matrix are <span class="math display">\[\begin{align*}
\boldsymbol{\mu}_{1|2} &amp;=
\boldsymbol{\mu}_1-\boldsymbol{Q}_{11}^{-1}\boldsymbol{Q}_{12}  \left(\boldsymbol{y}_2-\boldsymbol{\mu}_2\right)
\\&amp;=\boldsymbol{\mu}_1+ \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\left(\boldsymbol{y}_2-\boldsymbol{\mu}_2\right)
\\\boldsymbol{\Sigma}_{1|2} &amp;= \boldsymbol{\Sigma}_{11}-\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}.
\end{align*}\]</span> Note that <span class="math inline">\(\boldsymbol{\Sigma}_{1|2}=\boldsymbol{Q}_{11}^{-1}\)</span> corresponds to the Schur complement of <span class="math inline">\(\boldsymbol{\Sigma}_{22}.\)</span></p>
<p>Remark that the above is sufficient (why?) The quadratic form appearing in the exponential term of the density of a Gaussian vector with mean <span class="math inline">\(\boldsymbol{\nu}\)</span> and precision <span class="math inline">\(\boldsymbol{\varPsi}\)</span> is <span class="math display">\[\begin{align*}
(\boldsymbol{x}-\boldsymbol{\nu})^\top\boldsymbol{\varPsi}(\boldsymbol{x}-\boldsymbol{\nu})= \boldsymbol{x}^\top\boldsymbol{\varPsi}\boldsymbol{x} - \boldsymbol{x}^\top\boldsymbol{\varPsi}\boldsymbol{\nu} -
\boldsymbol{\nu}^\top\boldsymbol{\varPsi}\boldsymbol{x} + \boldsymbol{\nu}^\top\boldsymbol{\varPsi}\boldsymbol{\nu}.
\end{align*}\]</span> uniquely determines the parameters of the Gaussian distribution. The quadratic term in <span class="math inline">\(\boldsymbol{x}\)</span> forms a sandwich around the precision matrix, while the linear term identifies the location vector. Since any (conditional) density function integrates to one, there is a unique normalizing constant and the latter need not be computed.</p>
<!--
Indeed, suppose without loss of generality that 
$\boldsymbol{\mu}=\boldsymbol{0}_n.$ One can write the joint density as the product of the marginal and conditional densities
\begin{align*}
f_{\boldsymbol{Y}}(\boldsymbol{y}_1, \boldsymbol{y}_2) = f_{\boldsymbol{Y}_1 \mid \boldsymbol{Y}_2}(\boldsymbol{y}_1 \mid \boldsymbol{y}_2)f_{\boldsymbol{Y}_2}(\boldsymbol{y}_2).
\end{align*}
 Completing the square in $\boldsymbol{y}_1$ in the exponential gives 
 \begin{align}
\boldsymbol{y}^\top \boldsymbol{Q}\boldsymbol{y} = (\boldsymbol{y}_1+ \boldsymbol{Q}_{11}^{-1}\boldsymbol{Q}_{12}\boldsymbol{y}_2)^\top\boldsymbol{Q}_{11}(\boldsymbol{y}_1+ 
\boldsymbol{Q}_{11}^{-1}\boldsymbol{Q}_{12}\boldsymbol{y}_2) + \boldsymbol{y}_2^\top \boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{y}_2 \tag{S1} \label{eq:square}
 \end{align}
and we can interpret the first term on the right hand side as a function of $\boldsymbol{y}_1$ given $\boldsymbol{y}_2$ (the conditional distribution) and the second as $\boldsymbol{y}_2$ alone. 
-->
<!--
By using the transformation matrix $\mathbf{A}=(\mathbf{I}_k, \mathbf{O}_{k, n-k})^\top,$ we conclude that $\boldsymbol{Y}_2 \sim \mathsf{Gauss}_{n-k}(\boldsymbol{\mu}_2, \boldsymbol{\Sigma}_{22}).$ The marginal distribution can also be obtained by integrating out $\boldsymbol{y}_1$ in $f_{\boldsymbol{Y}}(\boldsymbol{y}_1, \boldsymbol{y}_2).$ Thus, the conditional density is the ratio
\begin{align*}
 \frac{f_{\boldsymbol{Y}}(\boldsymbol{y}_1, \boldsymbol{y}_2)}{f_{\boldsymbol{Y}_2}(\boldsymbol{y}_2)}&= \frac{(2\pi)^{-n/2} |\boldsymbol{Q}|^{1/2}\exp\left\{-\frac{1}{2} 
\left(\boldsymbol{y}_1^\top\boldsymbol{Q}_{11}\boldsymbol{y}_1+\boldsymbol{y}_1^\top\boldsymbol{Q}_{12}\boldsymbol{y}_2+\boldsymbol{y}_2^\top\boldsymbol{Q}_{21}\boldsymbol{y}_1+\boldsymbol{y}_2^\top\boldsymbol{ \Psi}_{22}\boldsymbol{y}_2\right)\right\}}{(2\pi)^{-(n-k)/2}|\boldsymbol{\Sigma}_{22}|^{-1/2}\exp\left(-\frac{1}{2} \boldsymbol{y}_2 \boldsymbol{\Sigma}^{-1}_{22}\boldsymbol{y}_2\right)}
\\&=(2\pi)^{-k/2}|\boldsymbol{Q}|^{1/2} \frac{\exp \left\{ -\frac{1}{2}\left(\boldsymbol{y}_1^\top\boldsymbol{Q}_{11}\boldsymbol{y}_1+2\boldsymbol{y}_1^\top\boldsymbol{Q}_{12}\boldsymbol{y}_2 + \boldsymbol{y}_2^\top\boldsymbol{Q}_{21}\boldsymbol{Q}_{11}^{-1}\boldsymbol{Q}_{12}\boldsymbol{y}_2 \right)\right\}}{\exp\left\{-\frac{1}{2} \left(\boldsymbol{y}_2 \boldsymbol{\Sigma}^{-1}_{22}\boldsymbol{y}_2 + \boldsymbol{y}_2^\top\boldsymbol{Q}_{22}\boldsymbol{y}_2-\boldsymbol{y}_2^\top\boldsymbol{Q}_{21}\boldsymbol{Q}_{11}^{-1}\boldsymbol{Q}_{12}\boldsymbol{y}_2\right)\right\}}
\end{align*}
making use of the identity $|\boldsymbol{Q}| = |\boldsymbol{Q}_{11}||\boldsymbol{\Sigma}_{22}|^{-1}.$ 
-->
</div>
</section>
<section id="likelihood" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="likelihood"><span class="header-section-number">1.3</span> Likelihood</h2>
<div id="def-likelihood" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.17 (Likelihood)</strong></span> The <strong>likelihood</strong> <span class="math inline">\(L(\boldsymbol{\theta})\)</span> is a function of the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> that gives the probability (or density) of observing a sample under a postulated distribution, treating the observations as fixed, <span class="math display">\[\begin{align*}
L(\boldsymbol{\theta}; \boldsymbol{y}) = f(\boldsymbol{y}; \boldsymbol{\theta}),
\end{align*}\]</span> where <span class="math inline">\(f(\boldsymbol{y}; \boldsymbol{\theta})\)</span> denotes the joint density or mass function of the <span class="math inline">\(n\)</span>-vector containing the observations.</p>
<p>If the latter are independent, the joint density factorizes as the product of the density of individual observations, and the likelihood becomes <span class="math display">\[\begin{align*}
L(\boldsymbol{\theta}; \boldsymbol{y})=\prod_{i=1}^n f_i(y_i; \boldsymbol{\theta}) = f_1(y_1; \boldsymbol{\theta}) \times \cdots \times f_n(y_n; \boldsymbol{\theta}).
\end{align*}\]</span> The corresponding log likelihood function for independent and identically distributions observations is <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\theta}; \boldsymbol{y}) = \sum_{i=1}^n \log f(y_i; \boldsymbol{\theta})
\end{align*}\]</span></p>
</div>
<div id="def-information" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.18 (Score and information matrix)</strong></span> Let <span class="math inline">\(\ell(\boldsymbol{\theta}),\)</span> <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta} \subseteq \mathbb{R}^p,\)</span> be the log likelihood function. The gradient of the log likelihood <span class="math inline">\(U(\boldsymbol{\theta}) = \partial \ell(\boldsymbol{\theta}) / \partial \boldsymbol{\theta}\)</span> is termed <strong>score</strong> function.</p>
<p>The <strong>observed information matrix</strong> is the hessian of the negative log likelihood <span class="math display">\[\begin{align*}
j(\boldsymbol{\theta}; \boldsymbol{y})=-\frac{\partial^2 \ell(\boldsymbol{\theta}; \boldsymbol{y})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\top},
\end{align*}\]</span> evaluated at the maximum likelihood estimate <span class="math inline">\(\widehat{\boldsymbol{\theta}},\)</span> so <span class="math inline">\(j(\widehat{\boldsymbol{\theta}}).\)</span> Under regularity conditions, the <strong>expected information</strong>, also called <strong>Fisher information</strong> matrix, is <span class="math display">\[\begin{align*}
i(\boldsymbol{\theta}) = \mathsf{E}\left\{U(\boldsymbol{\theta}; \boldsymbol{Y}) U(\boldsymbol{\theta}; \boldsymbol{Y})^\top\right\} = \mathsf{E}\left\{j(\boldsymbol{\theta}; \boldsymbol{Y})\right\}
\end{align*}\]</span> Both the Fisher (or expected) and the observed information matrices are symmetric and encode the curvature of the log likelihood and provide information about the variability of <span class="math inline">\(\widehat{\boldsymbol{\theta}}.\)</span></p>
<p>The information of an independent and identically distributed sample of size <span class="math inline">\(n\)</span> is <span class="math inline">\(n\)</span> times that of a single observation, so information accumulates at a linear rate.</p>
</div>
<div id="exm-likelihood-survival" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.14 (Likelihood for right-censoring)</strong></span> Consider a survival analysis problem for independent time-to-event data subject to (noninformative) random right-censoring. We assume failure times <span class="math inline">\(Y_i (i=1, \ldots, n)\)</span> are drawn from a common distribution <span class="math inline">\(F(\cdot; \boldsymbol{\theta})\)</span> supported on <span class="math inline">\((0, \infty)\)</span> and complemented with an independent censoring indicator <span class="math inline">\(C_i \in \{0,1\},\)</span> with <span class="math inline">\(0\)</span> indicating right-censoring and <span class="math inline">\(C_i=1\)</span> observed failure time. If individual observation <span class="math inline">\(i\)</span> has not experienced the event at the end of the collection period, then the likelihood contribution <span class="math inline">\(\Pr(Y &gt; y) = 1-F(y; \boldsymbol{\theta}),\)</span> where <span class="math inline">\(y_i\)</span> is the maximum time observed for <span class="math inline">\(Y_i.\)</span></p>
<p>We write the log likelihood in terms of the right-censoring binary indicator as <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\theta}) = \sum_{i: c_i=0} \log \{1- F(y_i; \boldsymbol{\theta})\} + \sum_{i: c_i=1} \log f(y_i; \boldsymbol{\theta})
\end{align*}\]</span></p>
<p>Suppose for simplicity that <span class="math inline">\(Y_i \sim \mathsf{expo}(\lambda)\)</span> and let <span class="math inline">\(m=c_1 + \cdots + c_n\)</span> denote the number of observed failure times. Then, the log likelihood and the Fisher information are <span class="math display">\[\begin{align*}
\ell(\lambda) &amp;= \lambda \sum_{i=1}^n y_i + \log \lambda m\\
i(\lambda) &amp;= m/\lambda^2
\end{align*}\]</span> and the right-censored observations for the exponential model do not contribute to the information.</p>
</div>
<div id="exm-gauss" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.15 (Information for the Gaussian distribution)</strong></span> Consider <span class="math inline">\(Y \sim \mathsf{Gauss}(\mu, \tau^{-1}),\)</span> parametrized in terms of precision <span class="math inline">\(\tau.\)</span> The likelihood contribution for an <span class="math inline">\(n\)</span> sample is, up to proportionality, <span class="math display">\[\begin{align*}
\ell(\mu, \tau) \propto \frac{n}{2}\log(\tau) - \frac{\tau}{2}\sum_{i=1}^n(Y_i^2-2\mu Y_i+\mu^2)
\end{align*}\]</span></p>
<p>The observed and Fisher information matrices are <span class="math display">\[\begin{align*}
j(\mu, \tau) &amp;= \begin{pmatrix}
n\tau &amp; -\sum_{i=1}^n (Y_i-\mu)\\
-\sum_{i=1}^n (Y_i-\mu) &amp; \frac{n}{2\tau^2}
\end{pmatrix}, \\
i(\mu, \tau) &amp;= n\begin{pmatrix}
\tau &amp; 0\\
0 &amp; \frac{1}{2\tau^2}
\end{pmatrix}
\end{align*}\]</span> Since <span class="math inline">\(\mathsf{E}(Y_i) = \mu,\)</span> the expected value of the off-diagonal entries of the Fisher information matrix are zero.</p>
</div>
<div id="exm-weibull-info" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.16 (Likelihood, score and information of the Weibull distribution)</strong></span> The log likelihood for a simple random sample whose realizations are <span class="math inline">\(y_1, \ldots, y_n\)</span> of size <span class="math inline">\(n\)</span> from a <span class="math inline">\(\mathsf{Weibull}(\lambda, \alpha)\)</span> model is <span class="math display">\[\begin{align*}
\ell(\lambda, \alpha) = n \log(\alpha) - n\alpha\log(\lambda) + (\alpha-1) \sum_{i=1}^n \log y_i  - \lambda^{-\alpha}\sum_{i=1}^n y_i^\alpha.
\end{align*}\]</span></p>
<p>The score, which is the gradient of the log likelihood, is easily obtained by differentiation<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="math display">\[\begin{align*}
U(\lambda, \alpha) &amp;= \begin{pmatrix}\frac{\partial \ell(\lambda, \alpha)}{\partial \lambda} \\
\frac{\partial \ell(\lambda, \alpha)}{\partial \alpha} \end{pmatrix} \\&amp;=
\begin{pmatrix}
-\frac{n\alpha}{\lambda} +\alpha\lambda^{-\alpha-1}\sum_{i=1}^n y_i^\alpha
\\
\frac{n}{\alpha} + \sum_{i=1}^n \log (y_i/\lambda)  - \sum_{i=1}^n \left(\frac{y_i}{\lambda}\right)^{\alpha} \times\log\left(\frac{y_i}{\lambda}\right).
\end{pmatrix}
\end{align*}\]</span> and the observed information is the <span class="math inline">\(2 \times 2\)</span> matrix-valued function <span class="math display">\[\begin{align*}
j(\lambda, \alpha) &amp;= - \begin{pmatrix}
\frac{\partial^2 \ell(\lambda, \alpha)}{\partial \lambda^2} &amp;  \frac{\partial^2 \ell(\lambda, \alpha)}{\partial \lambda \partial \alpha} \\ \frac{\partial^2 \ell(\lambda, \alpha)}{\partial \alpha \partial \lambda} &amp; \frac{\partial^2 \ell(\lambda, \alpha)}{\partial \alpha^2}
\end{pmatrix}  = \begin{pmatrix} j_{\lambda, \lambda} &amp; j_{\lambda, \alpha} \\ j_{\lambda, \alpha} &amp; j_{\alpha, \alpha} \end{pmatrix}
\end{align*}\]</span> whose entries are <span class="math display">\[\begin{align*}
j_{\lambda, \lambda} &amp;= \lambda^{-2}\left\{-n\alpha + \alpha(\alpha+1)\sum_{i=1}^n (y_i/\lambda)^\alpha\right\} \\
j_{\lambda, \alpha} &amp;= \lambda^{-1}\sum_{i=1}^n [1-(y_i/\lambda)^\alpha\{1+\alpha\log(y_i/\lambda)\}] \\
j_{\alpha,\alpha} &amp;= n\alpha^{-2} + \sum_{i=1}^n (y_i/\lambda)^\alpha \{\log(y_i/\lambda)\}^2
\end{align*}\]</span> To compute the expected information matrix, we need to compute expectation of <span class="math inline">\(\mathsf{E}\{(Y/\lambda)^\alpha\},\)</span> <span class="math inline">\(\mathsf{E}[(Y/\lambda)^\alpha\log\{(Y/\lambda)^\alpha\}]\)</span> and <span class="math inline">\(\mathsf{E}[(Y/\lambda)^\alpha\log^2\{(Y/\lambda)^\alpha\}].\)</span> By definition, <span class="math display">\[\begin{align*}
\mathsf{E}\left\{(Y/\lambda)^\alpha\right\} &amp; = \int_0^\infty (x/\lambda)^\alpha \frac{\alpha}{\lambda^\alpha} x^{\alpha-1}\exp\left\{-(x/\lambda)^\alpha\right\} \mathrm{d} x \\
&amp;= \int_0^\infty s\exp(-s) \mathrm{d} s =1
\end{align*}\]</span> making a change of variable <span class="math inline">\(S = (Y/\lambda)^\alpha\sim \mathsf{Exp}(1).\)</span> The two other integrals are tabulated in <span class="citation" data-cites="Gradshteyn.Ryzhik:2014">Gradshteyn and Ryzhik (<a href="references.html#ref-Gradshteyn.Ryzhik:2014" role="doc-biblioref">2014</a>)</span>, and are equal to <span class="math inline">\(1-\gamma\)</span> and <span class="math inline">\(\gamma^2-2\gamma + \pi^2/6,\)</span> respectively, where <span class="math inline">\(\gamma \approx 0.577\)</span> is the Euler–Mascherroni constant. <!-- \begin{align*} --> <!-- \mathsf{E}\left\{S\} &= \int_0^\infty s\exp(-s) \mathrm{d} s \\ --> <!-- \mathsf{E}\left\{S\} &= \int_0^\infty s^{2/\alpha}\exp(-s) \mathrm{d} s \\ --> <!-- \mathsf{E}\left\{S\} &= \int_0^\infty s\log\exp(-s) \mathrm{d} s \\ --> <!-- \end{align*} --> The expected information matrix of the Weibull distribution has entries <span class="math display">\[\begin{align*}
i_{\lambda, \lambda} &amp; = n \lambda^{-2}\alpha\left\{ (\alpha+1)-1\right\} \\
i_{\lambda, \alpha} &amp; = -n\lambda^{-1} (1-\gamma) \\
i_{\alpha, \alpha} &amp; = n\alpha^{-2}(1 + \gamma^2-2\gamma+\pi^2/6)
\end{align*}\]</span></p>
<p>We can check this result numerically by comparing the expected value of the observed information matrix</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>exp_info_weib <span class="ot">&lt;-</span> <span class="cf">function</span>(scale, shape){</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  i11 <span class="ot">&lt;-</span> shape<span class="sc">*</span>((shape <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span>(scale<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  i12 <span class="ot">&lt;-</span> <span class="sc">-</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">digamma</span>(<span class="dv">1</span>))<span class="sc">/</span>scale</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  i22 <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">+</span><span class="fu">digamma</span>(<span class="dv">1</span>)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="dv">2</span><span class="sc">*</span><span class="fu">digamma</span>(<span class="dv">1</span>)<span class="sc">+</span>pi<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">6</span>)<span class="sc">/</span>(shape<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matrix</span>(<span class="fu">c</span>(i11, i12, i12, i22), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>obs_info_weib <span class="ot">&lt;-</span> <span class="cf">function</span>(y, scale, shape){</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  ys <span class="ot">&lt;-</span> y<span class="sc">/</span>scale <span class="co"># scale family</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  o11 <span class="ot">&lt;-</span> shape<span class="sc">*</span>((shape <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">*</span><span class="fu">mean</span>(ys<span class="sc">^</span>shape)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>scale<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  o12 <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">-</span><span class="fu">mean</span>(ys<span class="sc">^</span>shape<span class="sc">*</span>(<span class="dv">1</span><span class="sc">+</span>shape<span class="sc">*</span><span class="fu">log</span>(ys))))<span class="sc">/</span>scale</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  o22 <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(shape<span class="sc">*</span>shape) <span class="sc">+</span> <span class="fu">mean</span>(ys<span class="sc">^</span>shape<span class="sc">*</span>(<span class="fu">log</span>(ys))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matrix</span>(<span class="fu">c</span>(o11, o12, o12, o22), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>nll_weib <span class="ot">&lt;-</span> <span class="cf">function</span>(pars, y){</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dweibull</span>(<span class="at">x =</span> y, <span class="at">scale =</span> pars[<span class="dv">1</span>], <span class="at">shape =</span> pars[<span class="dv">2</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))}</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Fix parameters</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>scale <span class="ot">&lt;-</span> <span class="fu">rexp</span>(<span class="at">n =</span> <span class="dv">1</span>, <span class="at">rate =</span> <span class="fl">0.5</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>shape <span class="ot">&lt;-</span> <span class="fu">rexp</span>(<span class="at">n =</span> <span class="dv">1</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>nobs <span class="ot">&lt;-</span> <span class="dv">1000</span>L</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">rweibull</span>(<span class="at">n =</span> nobs, <span class="at">scale =</span> scale, <span class="at">shape =</span> shape)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare Hessian with numerical differentiation</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>o_info <span class="ot">&lt;-</span> <span class="fu">obs_info_weib</span>(dat, <span class="at">scale =</span> scale, <span class="at">shape =</span> shape)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="fu">all.equal</span>(</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>  numDeriv<span class="sc">::</span><span class="fu">hessian</span>(nll_weib, <span class="at">x =</span> <span class="fu">c</span>(scale, shape), <span class="at">y =</span> dat) <span class="sc">/</span> nobs,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  o_info)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute approximation to Fisher information</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>exp_info_sim <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="at">n =</span> <span class="dv">1000</span>, <span class="at">expr =</span> {</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">obs_info_weib</span>(<span class="at">y =</span> <span class="fu">rweibull</span>(<span class="at">n =</span> nobs,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>                        <span class="at">shape =</span> shape,</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>                        <span class="at">scale =</span> scale),</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>           <span class="at">scale =</span> scale, <span class="at">shape =</span> shape)})</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="fu">all.equal</span>(<span class="fu">apply</span>(exp_info_sim, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, mean),</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>          <span class="fu">exp_info_weib</span>(scale, shape))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>The joint density function only factorizes for independent data, but an alternative sequential decomposition can be helpful. For example, we can write the joint density <span class="math inline">\(f(y_1, \ldots, y_n)\)</span> using the factorization <span class="math display">\[\begin{align*}
f(\boldsymbol{y}) = f(y_1) \times f(y_2 \mid y_1) \times \ldots f(y_n \mid y_1, \ldots, y_n)
\end{align*}\]</span> in terms of conditional. Such a decomposition is particularly useful in the context of time series, where data are ordered from time <span class="math inline">\(1\)</span> until time <span class="math inline">\(n\)</span> and models typically relate observation <span class="math inline">\(y_n\)</span> to it’s past.</p>
<div id="exm-autoregressive-one" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.17 (First-order autoregressive process)</strong></span> Consider an <span class="math inline">\(\mathsf{AR}(1)\)</span> model of the form <span class="math display">\[Y_t = \mu + \phi(Y_{t-1} - \mu) + \varepsilon_t,\]</span> where <span class="math inline">\(\phi\)</span> is the lag-one correlation, <span class="math inline">\(\mu\)</span> the global mean and <span class="math inline">\(\varepsilon_t\)</span> is an iid innovation with mean zero and variance <span class="math inline">\(\sigma^2.\)</span> If <span class="math inline">\(|\phi| &lt; 1,\)</span> the process is stationary.</p>
<p>The Markov property states that the current realization depends on the past, <span class="math inline">\(Y_t \mid Y_1, \ldots, Y_{t-1},\)</span> only through the most recent value <span class="math inline">\(Y_{t-1}.\)</span> The log likelihood thus becomes <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\theta}) = \log f(y_1) + \sum_{i=2}^n \log f(y_i \mid y_{i-1}).
\end{align*}\]</span></p>
<p>The <span class="math inline">\(\mathsf{AR}(1)\)</span> stationary process <span class="math inline">\(Y_t,\)</span> marginally, has mean <span class="math inline">\(\mu\)</span> and unconditional variance <span class="math inline">\(\sigma^2/(1-\phi^2).\)</span> If we use the recursive definition, we find <span class="math display">\[\begin{align*}
Y_t &amp;= \mu (1-\phi) + \varepsilon_t + \phi \{\mu + \phi(Y_{t-2}-\mu) + \varepsilon_{t-1}\}
= \mu + \sum_{j=0}^\infty \phi^j\varepsilon_{t-j}
\end{align*}\]</span> whence <span class="math inline">\(\mathsf{E}(Y_t) = \mu\)</span> and <span class="math display">\[\begin{align*}
\mathsf{Va}(Y_t) &amp;= \mathsf{Va}\left(\sum_{j=0}^\infty \phi^j\varepsilon_{t-j}\right)
= \sum_{j=0}^\infty \phi^{2j} \mathsf{Va}(\varepsilon_{t-j})
=\frac{\sigma^2}{(1-\phi^2)}
\end{align*}\]</span> where the geometric series converges if <span class="math inline">\(\phi&lt;1\)</span> and diverges otherwise.</p>
<p>If innovations <span class="math inline">\(\{\varepsilon_t\}\)</span> are Gaussian, we have <span class="math display">\[Y_t \mid Y_{t-1}=y_{t-1} \sim \mathsf{Gauss}\{\mu(1-\phi)+ \phi y_{t-1}, \sigma^2\}, \qquad t&gt;1;\]</span> The likelihood can then be written as <span class="math display">\[\begin{align*}
\ell(\mu, \phi,\sigma^2)&amp; = -\frac{n}{2}\log(2\pi) - n\log \sigma + \frac{1}{2}\log(1-\phi^2) \\&amp;\quad -\frac{(1-\phi^2)(y_1- \mu)^2}{2\sigma^2} - \sum_{i=2}^n \frac{(y_t - \mu(1-\phi)- \phi y_{t-1})^2}{2\sigma^2}
\end{align*}\]</span></p>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Student:1908" class="csl-entry" role="listitem">
Gosset, William Sealy. 1908. <span>“The Probable Error of a Mean.”</span> <em>Biometrika</em> 6 (1): 1–25. <a href="https://doi.org/10.1093/biomet/6.1.1">https://doi.org/10.1093/biomet/6.1.1</a>.
</div>
<div id="ref-Gradshteyn.Ryzhik:2014" class="csl-entry" role="listitem">
Gradshteyn, I. S., and I. M. Ryzhik. 2014. <em>Table of Integrals, Series, and Products</em>. 8th ed. Academic Press. <a href="https://doi.org/10.1016/c2010-0-64839-5">https://doi.org/10.1016/c2010-0-64839-5</a>.
</div>
<div id="ref-Held.Bove:2020" class="csl-entry" role="listitem">
Held, Leonhard, and Daniel Sabanés Bové. 2020. <em>Likelihood and <span>B</span>ayesian Inference: With Applications in Biology and Medicine</em>. 2nd ed. Heidelberg: Springer Berlin. <a href="https://doi.org/10.1007/978-3-662-60792-3">https://doi.org/10.1007/978-3-662-60792-3</a>.
</div>
<div id="ref-Jegerlehner:2021" class="csl-entry" role="listitem">
Jegerlehner, Sabrina, Franziska Suter-Riniker, Philipp Jent, Pascal Bittel, and Michael Nagler. 2021. <span>“Diagnostic Accuracy of a <span>SARS-CoV-2</span> Rapid Antigen Test in Real-Life Clinical Settings.”</span> <em>International Journal of Infectious Diseases</em> 109: 118–22. <a href="https://doi.org/10.1016/j.ijid.2021.07.010">https://doi.org/10.1016/j.ijid.2021.07.010</a>.
</div>
<div id="ref-Marshall.Olkin:1985" class="csl-entry" role="listitem">
Marshall, Albert W., and Ingram Olkin. 1985. <span>“A Family of Bivariate Distributions Generated by the Bivariate <span>B</span>ernoulli Distribution.”</span> <em>Journal of the American Statistical Association</em> 80 (390): 332–38. <a href="https://doi.org/10.1080/01621459.1985.10478116">https://doi.org/10.1080/01621459.1985.10478116</a>.
</div>
<div id="ref-owidcoronavirus" class="csl-entry" role="listitem">
Mathieu, Edouard, Hannah Ritchie, Lucas Rodés-Guirao, Cameron Appel, Charlie Giattino, Joe Hasell, Bobbie Macdonald, et al. 2020. <span>“Coronavirus Pandemic (COVID-19).”</span> <em>Our World in Data</em>.
</div>
<div id="ref-McNeil.Frey.Embrechts:2005" class="csl-entry" role="listitem">
McNeil, A. J., R. Frey, and P. Embrechts. 2005. <em>Quantitative Risk Management: Concepts, Techniques, and Tools</em>. 1st ed. Princeton, NJ: Princeton University Press.
</div>
<div id="ref-Nadarajah:2008" class="csl-entry" role="listitem">
Nadarajah, Saralees. 2008. <span>“<span>M</span>arshall and <span>O</span>lkin’s Distributions.”</span> <em>Acta Applicandae Mathematicae</em> 103 (1): 87–100. <a href="https://doi.org/10.1007/s10440-008-9221-7">https://doi.org/10.1007/s10440-008-9221-7</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Using for example a symbolic calculator.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lbelzile\.github\.io\/MATH80601A\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Welcome">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Welcome</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./bayesics.html" class="pagination-link" aria-label="Bayesics">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2024-2025, Léo Belzile</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/lbelzile/MATH80601A/edit/main/introduction.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>