[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian modelling",
    "section": "",
    "text": "Welcome\nThis book is a web complement to MATH 80601A Bayesian modelling, a graduate course offered at HEC Montréal.\nThese notes are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License and were last compiled on Wednesday, October 25 2023.\nThe objective of the course is to provide a hands on introduction to Bayesian data analysis. The course will cover the formulation, evaluation and comparison of Bayesian models through examples and real-data applications."
  },
  {
    "objectID": "introduction.html#probability-and-frequency",
    "href": "introduction.html#probability-and-frequency",
    "title": "1  Bayesics",
    "section": "1.1 Probability and frequency",
    "text": "1.1 Probability and frequency\nIn classical (frequentist) parametric statistic, we treat observations \\(\\boldsymbol{Y}\\) as realizations of a distribution whose parameters \\(\\boldsymbol{\\theta}\\) are unknown. All of the information about parameters is encoded by the likelihood function.\nThe interpretation of probability in the classical statistic is in terms of long run frequency, which is why we term this approach frequentist statistic. Think of a fair die: when we state that values \\(\\{1, \\ldots, 6\\}\\) are equiprobable, we mean that repeatedly tossing the die should result, in large sample, in each outcome being realized roughly \\(1/6\\) of the time (the symmetry of the object also implies that each facet should be equally likely to lie face up). This interpretation also carries over to confidence intervals: a \\((1-\\alpha)\\) confidence interval either contains the true parameter value or it doesn’t, so the probability level \\((1-\\alpha)\\) is only the long-run proportion of intervals created by the procedure that should contain the true fixed value, not the probability that a single interval contains the true value. This is counter-intuitive to most.\nIn practice, the true value of the parameter \\(\\boldsymbol{\\theta}\\) vector is unknown to the practitioner, thus uncertain: Bayesians would argue that we should treat the latter as a random quantity rather than a fixed constant. Since different people may have different knowledge about these potential values, the prior knowledge is a form of subjective probability. For example, if you play cards, one person may have recorded the previous cards that were played, whereas other may not. They thus assign different probability of certain cards being played. In Bayesian inference, we consider \\(\\boldsymbol{\\theta}\\) as random variables to reflect our lack of knowledge about potential values taken. Italian scientist Bruno de Finetti, who is famous for the claim ``Probability does not exist’’, stated in the preface of Finetti (1974):\n\nProbabilistic reasoning — always to be understood as subjective — merely stems from our being uncertain about something. It makes no difference whether the uncertainty relates to an unforseeable future, or to an unnoticed past, or to a past doubtfully reported or forgotten: it may even relate to something more or less knowable (by means of a computation, a logical deduction, etc.) but for which we are not willing or able tho make the effort; and so on […] The only relevant thing is uncertainty — the extent of our knowledge and ignorance. The actual fact of whether or not the events considered are in some sense determined, or known by other people, and so on, is of no consequence.\n\nOn page 3, de Finetti continues (Finetti 1974)\n\nonly subjective probabilities exist — i.e., the degree of belief in the occurrence of an event attributed by a given person at a given instant and with a given set of information."
  },
  {
    "objectID": "introduction.html#posterior-distribution",
    "href": "introduction.html#posterior-distribution",
    "title": "1  Bayesics",
    "section": "1.2 Posterior distribution",
    "text": "1.2 Posterior distribution\nWe consider a parametric model with parameters \\(\\boldsymbol{\\theta}\\) defined on \\(\\boldsymbol{\\Theta} \\subseteq \\mathbb{R}^p\\). In Bayesian learning, we adjoin to the likelihood \\(\\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{y}) \\equiv p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})\\) a prior function \\(p(\\boldsymbol{\\theta})\\) that reflects the prior knowledge about potential values taken by the \\(p\\)-dimensional parameter vector, before observing the data \\(\\boldsymbol{y}\\). The prior makes \\(\\boldsymbol{\\theta}\\) random and the distribution of the parameter reflects our uncertainty about the true value of the model parameters.\nIn a Bayesian analysis, observations are random variables but inference is performed conditional on the observed sample values. By Bayes’ theorem, our target is therefore the posterior density \\(p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\), defined as\n\\[\n\\underbracket[0.25pt]{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})}_{\\text{posterior}} = \\frac{\\overbracket[0.25pt]{p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})}^{\\text{likelihood}} \\times  \\overbracket[0.25pt]{p(\\boldsymbol{\\theta})}^{\\text{prior}}}{\\underbracket[0.25pt]{\\int p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}}_{\\text{marginal likelihood }p(\\boldsymbol{y})}}.\n\\tag{1.1}\\]\nThe posterior \\(p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\) is proportional, as a function of \\(\\theta,\\) to the product of the likelihood and the prior function.\nFor the posterior to be proper, we need the product of the prior and the likelihood on the right hand side to be integrable as a function of \\(\\boldsymbol{\\theta}\\) over the parameter domain \\(\\boldsymbol{\\Theta}\\). The integral in the denominator, termed marginal likelihood or prior predictive distribution and denoted \\(p(\\boldsymbol{y}) = \\mathsf{E}_{\\boldsymbol{\\theta}}\\{p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})\\}\\). It represents the distribution of the data before data collection, the respective weights being governed by the prior probability of different parameters values. The denominator of Equation 1.1 is a normalizing constant, making the posterior density integrate to unity. The marginal likelihood plays a central role in Bayesian testing.\nIf \\(\\boldsymbol{\\theta}\\) is low dimensional, numerical integration such as quadrature methods can be used to compute the marginal likelihood.\nTo fix ideas, we consider next a simple one-parameter model where the marginal likelihood can be computed explicitly.\n\nExample 1.1 (Binomial model with beta prior) Consider a binomial likelihood with probability of success \\(\\theta \\in [0,1]\\) and \\(n\\) trials, \\(Y \\sim \\mathsf{Binom}(n, \\theta)\\). If we take a beta prior, \\(\\theta \\sim \\mathsf{Beta}(\\alpha, \\beta)\\) and observe \\(y\\) successes, the posterior is \\[\\begin{align*}\np(\\theta \\mid y = y) &\\propto \\binom{n}{y} \\theta^y (1-\\theta)^{n-y} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}\\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}\n\\\\&\\stackrel{\\theta}{\\propto} \\theta^{y+\\alpha-1}(1-\\theta)^{n-y+\\beta-1}\n\\end{align*}\\] and is \\[\\int_{0}^{1} \\theta^{y+\\alpha-1}(1-\\theta)^{n-y+\\beta-1}\\mathrm{d} \\theta = \\frac{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)}{\\Gamma(n+\\alpha+\\beta)},\\] a Beta function. Since we need only to keep track of the terms that are function of the parameter \\(\\theta\\), we could recognize directly that the posterior distribution is \\(\\mathsf{Beta}(y+\\alpha, n-y+\\beta)\\) and deduce the normalizing constant from there.\nIf \\(Y \\sim \\mathsf{Binom}(n, \\theta)\\), the expected number of success is \\(n\\theta\\) and the expected number of failures \\(n(1-\\theta)\\) and so the likelihood contribution, relative to the prior, will dominate as the sample size \\(n\\) grows.\nAnother way to see this is to track moments (expectation, variance, etc.) The Beta distribution, whose density is \\(f(x; \\alpha, \\beta) \\propto x^{\\alpha-1} (1-x)^{\\beta-1}\\), has expectation \\(\\alpha/(\\alpha+\\beta)\\) and variance \\(\\alpha\\beta/\\{(\\alpha+\\beta)^2(\\alpha+\\beta+1)\\}\\). The posterior mean is \\[\\begin{align*}\n\\mathsf{E}(\\theta \\mid y) = w\\frac{y}{n} + (1-w) \\frac{\\alpha}{\\alpha+\\beta},\n\\qquad w = \\frac{n}{n+\\alpha + \\beta},\n\\end{align*}\\] a weighted average of the maximum likelihood estimator and the prior mean. We can think of the parameter \\(\\alpha\\) (respectively \\(\\beta\\)) as representing the fixed prior number of success (resp. failures). The variance term is \\(\\mathrm{O}(n^{-1})\\) and, as the sample size increases, the likelihood weight \\(w\\) dominates.\nFigure 1.1 shows three different posterior distributions with different beta priors: the first prior, which favors values closer to 1/2, leads to a more peaked posterior density, contrary to the second which is symmetric, but concentrated toward more extreme values near endpoints of the support. The rightmost panel is truncated: as such, the posterior is zero for any value of \\(\\theta\\) beyond 1/2 and so the posterior mode may be close to the endpoint of the prior. The influence of such a prior will not necessarily vanish as sample size and should be avoided, unless there are compelling reasons for restricting the domain.\n\n\n\n\n\nFigure 1.1: Scaled binomial likelihood for six successes out of 14 trials, with \\(\\mathsf{Beta}(3/2, 3/2)\\) prior (left), \\(\\mathsf{Beta}(1/4, 1/4)\\) (middle) and truncated uniform on \\([0,1/2]\\) (right), with the corresponding posterior distributions.\n\n\n\n\n\n\nRemark (Proportionality). Any term appearing in the likelihood times prior function that does not depend on parameters can be omitted since they will be absorbed by the normalizing constant. This makes it useful to compute normalizing constants or likelihood ratios.\n\n\nRemark. An alternative parametrization for the beta distribution sets \\(\\alpha=\\mu \\kappa\\), \\(\\beta = (1-\\mu)\\kappa\\) for \\(\\mu \\in (0,1)\\) and \\(\\kappa&gt;0\\), so that the model is parametrized directly in terms of mean \\(\\mu\\), with \\(\\kappa\\) capturing the dispersion.\n\n\nRemark. A density integrates to 1 over the range of possible outcomes, but there is no guarantee that the likelihood function, as a function of \\(\\boldsymbol{\\theta}\\), integrates to one over the parameter domain \\(\\boldsymbol{\\Theta}\\).\nFor example, the binomial likelihood with \\(n\\) trials and \\(y\\) successes satisfies \\[\\int_0^1 \\binom{n}{y}\\theta^y(1-\\theta)^{n-y} \\mathrm{d} \\theta = \\frac{1}{n+1}.\\]\nMoreover, the binomial distribution is discrete with support \\(0, \\ldots, n\\), whereas the likelihood is continuous as a function of the probability of success, as evidenced by Figure 1.2\n\n\n\n\n\nFigure 1.2: Binomial mass function (left) and scaled likelihood function (right).\n\n\n\n\n\n\nProposition 1.1 (Sequentiality and Bayesian updating) The likelihood is invariant to the order of the observations if they are independent Thus, if we consider two blocks of observations \\(\\boldsymbol{y}_1\\) and \\(\\boldsymbol{y}_2\\) \\[p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\boldsymbol{y}_2) = p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1) p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_2),\\] so it makes no difference if we treat data all at once or in blocks. More generally, for data exhibiting spatial or serial dependence, it makes sense to consider rather the conditional (sequential) decomposition \\[f(\\boldsymbol{y}; \\boldsymbol{\\theta}) = f(\\boldsymbol{y}_1; \\boldsymbol{\\theta}) f(\\boldsymbol{y}_2; \\boldsymbol{\\theta}, \\boldsymbol{y}_1) \\cdots f(\\boldsymbol{y}_n; \\boldsymbol{\\theta}, \\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_{n-1})\\] where \\(f(\\boldsymbol{y}_k; \\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_{k-1})\\) denotes the conditional density function given observations \\(\\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_{k-1}\\).\nBy Bayes’ rule, we can consider updating the posterior by adding terms to the likelihood, noting that \\[\\begin{align*}\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\boldsymbol{y}_2) \\propto p(\\boldsymbol{y}_2 \\mid \\boldsymbol{y}_1, \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1)\n\\end{align*}\\] which amounts to treating the posterior \\(p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1)\\) as a prior. If data are exchangeable, the order in which observations are collected and the order of the belief updating is irrelevant to the full posterior. Figure 1.3 shows how the posterior becomes gradually closer to the scaled likelihood as we increase the sample size, and the posterior mode moves towards the true value of the parameter (here 0.3).\n\n\n\n\n\nFigure 1.3: Beta posterior and binomial likelihood with a uniform prior for increasing number of observations (from left to right) out of a total of 100 trials.\n\n\n\n\n\n\nExample 1.2 While we can calculate analytically the value of the normalizing constant for the beta-binomial model, we could also for arbitrary priors use numerical integration or Monte Carlo methods in the event the parameter vector \\(\\boldsymbol{\\theta}\\) is low-dimensional.\nWhile estimation of the normalizing constant is possible in simple models, the following highlights some challenges that are worth keeping in mind. In a model for discrete data (that is, assigning probability mass to a countable set of outcomes), the terms in the likelihood are probabilities and thus the likelihood becomes smaller as we gather more observations (since we multiply terms between zero or one). The marginal likelihood term becomes smaller and smaller, so it’s reciprocal is big and this can lead to arithmetic underflow.\n\ny &lt;- 6L # number of successes \nn &lt;- 14L # number of trials\nalpha &lt;- beta &lt;- 1.5 # prior parameters\nunnormalized_posterior &lt;- function(theta){\n  theta^(y+alpha-1) * (1-theta)^(n-y + beta - 1)\n}\nintegrate(f = unnormalized_posterior,\n          lower = 0,\n          upper = 1)\n\n1.066906e-05 with absolute error &lt; 1e-12\n\n# Compare with known constant\nbeta(y + alpha, n - y + beta)\n\n[1] 1.066906e-05\n\n# Monte Carlo integration\nmean(unnormalized_posterior(runif(1e5)))\n\n[1] 1.064067e-05\n\n\n\nWhen \\(\\boldsymbol{\\theta}\\) is high-dimensional, the marginal likelihood is intractable. This is one of the main challenges of Bayesian statistics and the popularity and applicability has grown drastically with the development and popularity of numerical algorithms, following the publication of Geman and Geman (1984) and Gelfand and Smith (1990). Markov chain Monte Carlo methods circumvent the calculation of the denominator by drawing approximate samples from the posterior."
  },
  {
    "objectID": "introduction.html#posterior-predictive-distribution",
    "href": "introduction.html#posterior-predictive-distribution",
    "title": "1  Bayesics",
    "section": "1.3 Posterior predictive distribution",
    "text": "1.3 Posterior predictive distribution\nPrediction in the Bayesian paradigm is obtained by considering the posterior predictive distribution, \\[\\begin{align*}\np(y_{\\text{new}} \\mid \\boldsymbol{y}) =\n\\int_{\\Theta} p(y_{\\text{new}}  \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} \\mid  \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\]\nGiven draws from the posterior distribution, say \\(\\boldsymbol{\\theta}_b\\) \\((b=1, \\ldots, B)\\), we sample from each a new realization from the distribution appearing in the likelihood \\(p(y_{\\text{new}} \\mid \\boldsymbol{\\theta}_b)\\). This is different from the frequentist setting, which fixes the value of the parameter to some estimate \\(\\widehat{\\boldsymbol{\\theta}}\\); by contrast, the posterior predictive, here a beta-binomial distribution \\(\\mathsf{BetaBin}(n, \\alpha + y, n - y + \\beta)\\), carries over the uncertainty so will typically be wider and overdispersed relative to the corresponding binomial model. This can be easily seen from the left-panel of Figure 1.4, which contrasts the binomial mass function evaluated at the maximum likelihood estimator \\(\\widehat{\\theta}=6/14\\) with the posterior predictive.\n\n\nnpost &lt;- 1e4L\n# Sample draws from the posterior distribution\npost_samp &lt;- rbeta(n = npost, y + alpha, n - y + beta)\n# For each draw, sample new observation\npost_pred &lt;- rbinom(n = npost, size = n, prob = post_samp)\n\n\n\n\n\n\nFigure 1.4: Beta-binomial posterior predictive distribution with corresponding binomial mass function evaluated at the maximum likelihood estimator.\n\n\n\n\n\n\nExample 1.3 (Posterior predictive distribution of univariate Gaussian with known mean) Consider an \\(n\\) sample of independent and identically distributed Gaussian, \\(Y_i \\sim \\mathsf{Norm}(0, \\tau^{-1})\\) (\\(i=1, \\ldots, n\\)), where we assign a gamma prior on the precision \\(\\tau \\sim \\mathsf{Ga}(\\alpha, \\beta)\\). The posterior is \\[\\begin{align*}\np(\\tau \\mid \\boldsymbol{y}) \\stackrel{\\tau}{\\propto} \\prod_{i=1}^n \\tau^{n/2}\\exp\\left(-\\tau \\frac{\\sum_{i=1}^n{y_i^2}}{2}\\right) \\times \\tau^{\\alpha-1} \\exp(-\\beta \\tau)\n\\end{align*}\\] and rearranging the terms to collect powers of \\(\\tau\\), etc. we find that the posterior for \\(\\tau\\) must also be gamma, with shape parameter \\(\\alpha^* = \\alpha + n/2\\) and rate \\(\\beta^* = \\beta + \\sum_{i=1}^n y_i^2/2\\).\nThe posterior predictive is \\[\\begin{align*}\np(y_{\\text{new}} \\mid \\boldsymbol{y}) &= \\int_0^\\infty \\frac{\\tau^{1/2}}{(2\\pi)^{1/2}}\\exp(-\\tau y_{\\text{new}}^2/2) \\frac{\\beta^{*\\alpha^*}}{\\Gamma(\\alpha^*)}\\tau^{\\alpha^*-1}\\exp(-\\beta^* \\tau) \\mathrm{d} \\tau\n\\\\&= (2\\pi)^{-1/2} \\frac{\\beta^{*\\alpha^*}}{\\Gamma(\\alpha^*)} \\int_0^\\infty\\tau^{\\alpha^*-1/2} \\exp\\left\\{- \\tau (y_{\\text{new}}^2/2 + \\beta^*)\\right\\} \\mathrm{d} \\tau\n\\\\&= (2\\pi)^{-1/2} \\frac{\\beta^{*\\alpha^*}}{\\Gamma(\\alpha^*)} \\frac{\\Gamma(\\alpha^* + 1/2)}{(y_{\\text{new}}^2/2 + \\beta^*)^{\\alpha^*+1/2}}\n\\\\&= \\frac{\\Gamma\\left(\\frac{2\\alpha^* + 1}{2}\\right)}{\\sqrt{2\\pi}\\Gamma\\left(\\frac{2\\alpha^*}{2}\\right)\\beta^{*1/2}} \\left( 1+ \\frac{y_{\\text{new}}^2}{2\\beta^*}\\right)^{-\\alpha^*-1/2}\n\\\\&= \\frac{\\Gamma\\left(\\frac{2\\alpha^* + 1}{2}\\right)}{\\sqrt{\\pi}\\sqrt{ 2\\alpha^*}\\Gamma\\left(\\frac{2\\alpha^*}{2}\\right)(\\beta^*/\\alpha^*)^{1/2}} \\left( 1+ \\frac{1}{2\\alpha^*}\\frac{y_{\\text{new}}^2}{(\\beta^*/\\alpha^*)}\\right)^{-\\alpha^*-1/2}\n\\end{align*}\\] which entails that \\(Y_{\\text{new}}\\) is a scaled Student-\\(t\\) distribution with scale \\((\\beta^*/\\alpha^*)^{1/2}\\) and \\(2\\alpha+n\\) degrees of freedom. This example also exemplifies the additional variability relative to the distribution generating the data: indeed, the Student-\\(t\\) distribution is more heavy-tailed than the Gaussian, but since the degrees of freedom increase linearly with \\(n\\), the distribution converges to a Gaussian as \\(n \\to \\infty\\), reflecting the added information as we collect more and more data points and the variance gets better estimated through \\(\\sum_{i=1}^n y_i^2/n\\)."
  },
  {
    "objectID": "introduction.html#summarizing-posterior-distributions",
    "href": "introduction.html#summarizing-posterior-distributions",
    "title": "1  Bayesics",
    "section": "1.4 Summarizing posterior distributions",
    "text": "1.4 Summarizing posterior distributions\nThe output of the Bayesian learning problem will be either of:\n\na fully characterized distribution\na numerical approximation to the posterior distribution (pointwise)\nan exact or approximate sample drawn from the posterior distribution\n\nIn the first case, we will be able to directly evaluate quantities of interest if there are closed-form expressions for the latter, or else we could draw samples from the distribution and evaluate them via Monte-Carlo. In case of numerical approximations, we will need to resort to numerical integration or otherwise to get our answers.\nOften, we will also be interested in the marginal posterior distribution of each component \\(\\theta_j\\) in turn (\\(j=1, \\ldots, J\\)). To get these, we carry out additional integration steps, \\[p(\\theta_j \\mid \\boldsymbol{y}) = \\int p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}_{-j}.\\] With a posterior sample, this is trivial: it suffices to keep the column corresponding to \\(\\theta_j\\) and discard the others.\nMost of the field of Bayesian statistics revolves around the creation of algorithms that either circumvent the calculation of the normalizing constant (notably using Monte Carlo and Markov chain Monte Carlo methods) or else provide accurate numerical approximation of the posterior pointwise, including for marginalizing out all but one parameters (integrated nested Laplace approximations, variational inference, etc.) The target of inference is the whole posterior distribution, a potentially high-dimensional object which may be difficult to summarize or visualize. We can thus report only characteristics of the the latter.\nThe choice of point summary to keep has it’s root in decision theory.\n\nDefinition 1.1 (Loss function) A loss function \\(c(\\boldsymbol{\\theta}, \\boldsymbol{\\upsilon})\\) is a mapping from \\(\\boldsymbol{\\Theta} \\to \\mathbb{R}^k\\) that assigns a weight to each value of \\(\\boldsymbol{\\theta}\\), corresponding to the regret or loss arising from choosing this value. The corresponding point estimator \\(\\widehat{\\boldsymbol{\\upsilon}}\\) is the minimizer of the expected loss,\n\\[\\begin{align*}\n\\widehat{\\boldsymbol{\\upsilon}} &= \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\upsilon}}\\mathsf{E}_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{Y}}\\{c(\\boldsymbol{\\theta}, \\boldsymbol{v})\\} \\\\&=\\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\upsilon}} \\int_{\\boldsymbol{\\Theta}} c(\\boldsymbol{\\theta}, \\boldsymbol{\\upsilon})p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\]\n\nFor example, in a univariate setting, the quadratic loss \\(c(\\theta, \\upsilon) = (\\theta-\\upsilon)^2\\) returns the posterior mean, the absolute loss \\(c(\\theta, \\upsilon)=|\\theta - \\upsilon|\\) returns the posterior median and the 0-1 loss \\(c(\\theta, \\upsilon) = \\mathrm{I}(\\upsilon \\neq \\theta)\\) returns the posterior mode. All of these point estimators are central tendency measures, but some may be more adequate depending on the setting as they can correspond to potentially different values, as shown in the left-panel of Figure 1.5. The choice is application specific: for multimodal distributions, the mode is likely a better choice.\nIf we know how to evaluate the distribution numerically, we can optimize to find the mode or else return the value for the pointwise evaluation on a grid at which the density achieves it’s maximum. The mean and median would have to be evaluated by numerical integration if there is no closed-form expression for the latter.\nIf we have rather a sample from the posterior with associated posterior density values, then we can obtain the mode as the parameter combination with the highest posterior, the median from the value at rank \\(\\lfloor n/2\\rfloor\\) and the mean through the sample mean of posterior draws.\n\n\n\n\n\nFigure 1.5: Point estimators from a right-skewed distribution (left) and from a multimodal distribution (right).\n\n\n\n\nThe loss function is often a functional (meaning a one-dimensional summary) from the posterior. The following example shows how it reduces a three-dimensional problem into a single risk measure.\n\nExample 1.4 (Danish insurance losses) In extreme value, we are often interested in assessing the risk of events that are rare enough that they lie beyond the range of observed data. To provide a scientific extrapolation, it often is justified to fit a generalized Pareto distribution to exceedances of \\(Z=Y-u\\), for some user-specified threshold \\(u\\) which is often taken as a large quantile of the distribution of \\(Y\\). The generalized Pareto distribution function is \\[\\begin{align*}\nF(z; \\tau, \\xi) = 1- \\begin{cases}\n\\left(1+\\xi/\\tau z\\right)^{-1/\\xi}_{+}, & \\xi \\neq 0\\\\\n\\exp(-z/\\tau), & \\xi = 0. \\end{cases}\n\\end{align*}\\] The shape \\(\\xi\\) governs how heavy-tailed the distribution is, while \\(\\tau\\) is a scale parameter.\nInsurance companies provide coverage in exchange for premiums, but need to safeguard themselves against very high claims by buying reinsurance products. These risks are often communicated through the value-at-risk (VaR), a high quantile exceeded with probability \\(p\\). We model Danish fire insurance claim amounts for inflation-adjusted data collected from January 1980 until December 1990 that are in excess of a million Danish kroner, found in the evir package and analyzed in Example 7.23 of McNeil, Frey, and Embrechts (2005). These claims are denoted \\(Y\\) and there are 2167 observations.\nWe fit a generalized Pareto distribution to exceedances above 10 millions krones, keeping 109 observations or roughly the largest 5% of the original sample. Preliminary analysis shows that we can treat data as roughly independent and identically distributed and goodness-of-fit diagnostics (not shown) for the generalized Pareto suggest that the fit is adequate for all but the three largest observations, which are (somewhat severely) underestimated by the model.\n\n\n\n\n\nFigure 1.6: Time series of Danish fire claims exceeding a million krone (left) and posterior samples from the scale \\(\\tau\\) and shape \\(\\xi\\) of the generalized Pareto model fitted to exceedances above 10 million krone (right).\n\n\n\n\nThe generalized Pareto model only describes the \\(n_u\\) exceedances above \\(u=10\\), so we need to incorporate in the likelihood a binomial contribution for the probability \\(\\zeta_u\\) of exceeding the threshold \\(u\\).      Provided that the priors for \\((\\tau, \\xi)\\) are independent of those for \\(\\zeta_u\\), the posterior also factorizes as a product, so \\(\\zeta_u\\) and \\((\\tau, \\xi)\\) are a posteriori independent.\nSuppose for now that we set a \\(\\mathsf{Beta}(0.5, 0.5)\\) prior for \\(\\zeta_u\\) and a non-informative prior for the generalized Pareto parameters. The post_samp matrix contains exact samples from the posterior distribution of \\((\\tau, \\xi, \\zeta_u)\\), obtained using a Monte Carlo algorithm. Our aim is to evaluate the posterior distribution for the value-at-risk, the \\(\\alpha\\) quantile of \\(Y\\) for high values of \\(\\alpha\\) and see what point estimator one would obtain depending on our choice of loss function. For any \\(\\alpha &gt; 1-\\zeta_u\\), the \\(q_{\\alpha}\\) is \\[\\begin{align*}\n1- \\alpha  &= \\Pr(Y &gt; q_\\alpha \\mid Y &gt; u) \\Pr(Y &gt; u)\n\\\\ &= \\left(1+\\xi \\frac{q_{\\alpha}-u}{\\tau}\\right)_{+}^{-1/\\xi}\\zeta_u\n\\end{align*}\\] and solving for \\(q_{\\alpha}\\) gives \\[\\begin{align*}\nq_{\\alpha} = u+ \\frac{\\tau}{\\xi} \\left\\{\\left(\\frac{\\zeta_u}{1-\\alpha}\\right)^\\xi-1\\right\\}.\n\\end{align*}\\]\nTo obtain the posterior distribution of the \\(\\alpha\\) quantile, \\(q_{\\alpha}\\), it suffices to plug in each posterior sample and evaluate the function: the uncertainty is carried over from the simulated values of the parameters to those of the quantile \\(q_{\\alpha}\\). The left panel of Figure 1.7 shows the posterior density estimate of the \\(\\mathsf{VaR}(0.99)\\) along with the maximum a posteriori (mode) of the latter.\nSuppose that we prefer to under-estimate the value-at-risk rather than overestimate: this could be captured by the custom loss function \\[\\begin{align*}\nc(q, q_0) =\n\\begin{cases}\n0.5(0.99q - q_0), & q &gt; q_0 \\\\\n0.75(q_0 - 1.01q), & q &lt; q_0.\n\\end{cases}\n\\end{align*}\\] For a given value of the value-at-risk \\(q_0\\) evaluated on a grid, we thus compute \\[\\begin{align*}\nr(q_0) = \\int_{\\boldsymbol{\\Theta}}c(q(\\boldsymbol{\\theta}), q_0) p (\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\] and we seek to minimize the risk, \\(\\widehat{q} =\\mathrm{argmin}_{q_0 \\in \\mathbb{R}_{+}} r(q_0)\\). The value returned that minimizes the loss, shown in Figure 1.7, is to the left of the posterior mean for \\(q_\\alpha\\).\n\n# Compute value at risk from generalized Pareto distribution quantile fn\nVaR_post &lt;- with(post_samp,   # data frame of posterior draws\n            revdbayes::qgp(   # with columns 'probexc', 'scale', 'shape'\n  p = 0.01/probexc, \n  loc = 10, \n  scale = scale, \n  shape = shape, \n  lower.tail = FALSE))\n# Loss function\nloss &lt;- function(qhat, q){\n    mean(ifelse(q &gt; qhat,\n           0.5*(0.99*q-qhat),\n           0.75*(qhat-1.01*q)))\n}\n# Create a grid of values over which to estimate the loss for VaR\nnvals &lt;- 101L\nVaR_grid &lt;- seq(\n  from = quantile(VaR_post, 0.01),\n  to = quantile(VaR_post, 0.99), \n  length.out = nvals)\n# Create a container to store results\nrisk &lt;- numeric(length = nvals)\nfor(i in seq_len(nvals)){\n  # Compute integral (Monte Carlo average over draws)\n risk[i] &lt;- loss(q = VaR_post, qhat = VaR_grid[i])\n}\n\n\n\n\n\n\nFigure 1.7: Posterior density (left) and losses functions for the 0.99 value-at-risk for the Danish fire insurance data. The vertical lines denote point estimates of the quantiles that minimize the loss functions.\n\n\n\n\n\nTo communicate uncertainty, we may resort to credible regions and intervals.\n\nDefinition 1.2 A \\((1-\\alpha)\\) credible region (or credible interval in the univariate setting) is a set \\(\\mathcal{S}_\\alpha\\) such that, with probability level \\(\\alpha\\), \\[\\begin{align*}\n\\Pr(\\boldsymbol{\\theta} \\in \\mathcal{S}_\\alpha \\mid \\boldsymbol{Y}=\\boldsymbol{y}) = 1-\\alpha\n\\end{align*}\\]\n\nThese intervals are not unique, as are confidence sets. In the univariate setting, the central or equitailed interval are the most popular, and easily obtained by considering the \\(\\alpha/2, 1-\\alpha/2\\) quantiles. These are easily obtained from samples by simply taking empirical quantiles. An alternative, highest posterior density credible sets, which may be a set of disjoint intervals obtained by considering the parts of the posterior with the highest density, may be more informative. The top panel Figure 1.8 shows the distinction for a bimodal mixture distribution, and a even more striking difference for 50% credible intervals for a symmetric beta distribution whose mass lie near the endpoints of the distribution, leading to no overlap between the two intervals.\n\n\n\n\n\nFigure 1.8: Density plots with 89% (top) and 50% (bottom) equitailed or central credible (left) and highest posterior density (right) regions for two data sets, highlighted in grey. The horizontal lign gives the posterior density value determining the cutoff for the HDP region.\n\n\n\n\n```\n\n\n\n\nFinetti, Bruno de. 1974. Theory of Probability: A Critical Introductory Treatment. Vol. 1. New York: Wiley.\n\n\nGelfand, Alan E., and Adrian F. M. Smith. 1990. “Sampling-Based Approaches to Calculating Marginal Densities.” Journal of the American Statistical Association 85 (410): 398–409. https://doi.org/10.1080/01621459.1990.10476213.\n\n\nGeman, Stuart, and Donald Geman. 1984. “Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images.” IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-6 (6): 721–41. https://doi.org/10.1109/TPAMI.1984.4767596.\n\n\nMcNeil, A. J., R. Frey, and P. Embrechts. 2005. Quantitative Risk Management: Concepts, Techniques, and Tools. 1st ed. Princeton, NJ: Princeton University Press."
  },
  {
    "objectID": "priors.html#prior-simulation",
    "href": "priors.html#prior-simulation",
    "title": "2  Priors",
    "section": "2.1 Prior simulation",
    "text": "2.1 Prior simulation\nExpert elicitation is difficult and it is hard to grasp what the impacts of the hyperparameters are. One way to see if the priors are reasonable is to sample values from them and generate new observations, resulting in prior predictive draws.\nThe prior predictive is \\(\\int_{\\boldsymbol{\\Theta}} p(y \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\\): we can simulate outcomes from it by first drawing parameter values from the prior, then sampling new observations from the distribution in the likelihood and keeping only the latter. If there are sensible bounds for the range of the response, we could discard values that do not abide with these.\nWorking with standardized inputs \\(x_i \\mapsto (x_i - \\overline{x})/\\mathrm{sd}(\\boldsymbol{x})\\) is useful. For example, in a simple linear regression (with a sole numerical explanatory), the slope is the correlation between standardized explanatory \\(\\mathrm{X}\\) and standardized response \\(Y\\) and the intercept should be mean zero.\n\nExample 2.1 Consider the daily number of Bixi bike sharing users for 2017–2019 at the Edouard Montpetit station next to HEC: we can consider a simple linear regression with log counts as a function of temperature,1 \\[\\log (\\texttt{nusers}) \\sim \\mathsf{Norm}_{+}\\{\\beta_0 + \\beta_1 (\\texttt{temp}-20), \\sigma^2\\}.\\] The \\(\\beta_1\\) slope measures units in degree Celsius per log number of person.\nThe hyperparameters depend of course on the units of the analysis, unless one standardizes response variable and explanatories: it is easier to standardize the temperature so that we consider deviations from, say 20\\(^{\\circ}\\)C, which is not far from the observed mean in the sample. After some tuning, the independent priors \\(\\beta_0 \\sim \\mathsf{Norm}(\\overline{y}, 0.5^2)\\), \\(\\beta_1 \\sim \\mathsf{Norm}(0, 0.05^2)\\) and \\(\\sigma \\sim \\mathsf{Exp}(3)\\) seem to yield plausible outcomes and relationships.2\n\n\n\n\n\nFigure 2.1: Prior draws of the linear regressions with observed data superimposed (left), and draws of observations from the prior predictive distribution (in gray) against observed data (right).\n\n\n\n\nWe can draw regression lines from the prior, as in the left panel of Figure 2.1: while some of the negative relationships appear unlikely after seeing the data, the curves all seem to pass somewhere in the cloud of point. By contrast, a silly prior is one that would result in all observations being above or below the regression line, or yield values that are much too large near the endpoints of the explanatory variable. Indeed, given the number of bikes for rental is limited (a docking station has only 20 bikes), it is also sensible to ensure that simulations do not return overly large numbers. The maximum number of daily users in the sample is 68, so priors that return simulations with more than 200 (rougly 5.3 on the log scale) are not that plausible. The prior predictive draws can help establish this and the right panel of Figure 2.1 shows that, expect for the lack of correlation between temperature and number of users, the simulated values from the prior predictive are plausible even if overdispersed."
  },
  {
    "objectID": "priors.html#conjugate-priors",
    "href": "priors.html#conjugate-priors",
    "title": "2  Priors",
    "section": "2.2 Conjugate priors",
    "text": "2.2 Conjugate priors\nIn very simple models, there may exists prior densities that result in a posterior distribution of the same family. We can thus directly extract characteristics of the posterior. Conjugate priors are chosen for computational convenience and because interpretation is convenient, as the parameters of the posterior will often be some weighted average of prior and likelihood component.\n\nDefinition 2.1 A prior density \\(p(\\boldsymbol{\\theta})\\) is conjugate for likelihood \\(L(\\boldsymbol{\\theta}; \\boldsymbol{y})\\) if the product \\(L(\\boldsymbol{\\theta}; \\boldsymbol{y})p(\\boldsymbol{\\theta})\\), after renormalization, is of the same parametric family as the prior.\nExponential families (including the binomial, Poisson, exponential, Gaussian distributions) admit conjugate priors3\n\n\nExample 2.2 (Conjugate prior for the binomial model) The binomial log density with \\(y\\) successes out of \\(n\\) trials is proportional to \\[\\begin{align*}\ny \\log(p) + (n-y) \\log(1-p) = y\\log\\left( \\frac{p}{1-p}\\right) + n \\log(1-p)\n\\end{align*}\\] with canonical parameter \\(\\mathrm{logit}(p)\\).4 The binomial distribution is thus an exponential family.\nSince the density of the binomial is of the form \\(p^y(1-p)^{n-y}\\), the beta distribution \\(\\mathsf{Beta}(\\alpha, \\beta)\\) with density \\[f(x) \\propto x^{\\alpha-1} (1-x)^{\\beta-1}\\] is the conjugate prior.\nThe beta distribution is also the conjugate prior for the negative binomial, geometric and Bernoulli distributions, since their likelihoods are all proportional to that of the beta. The fact that different sampling schemes that result in proportional likelihood functions give the same inference is called likelihood principle.\n\n\nExample 2.3 (Conjugate prior for the Poisson model) The Poisson distribution with mean \\(\\mu\\) has log density proportional to \\(f(y; \\mu) \\propto y\\log(\\mu) -\\mu\\), so is an exponential family with natural parameter \\(\\log(\\mu)\\). The gamma density, \\[ f(x) \\propto \\beta^{\\alpha}/\\Gamma(\\alpha)x^{\\alpha-1} \\exp(-\\beta x)\\] with shape \\(\\alpha\\) and rate \\(\\beta\\) is the conjugate prior for the Poisson. For an \\(n\\)-sample of independent observations \\(\\mathsf{Pois}(\\mu)\\) observations with \\(\\mu \\sim \\mathsf{Gamma}(\\alpha, \\beta)\\), the posterior is \\(\\mathsf{Gamma}(\\sum_{i=1}^n y_i + \\alpha, \\beta + n)\\).\n\nKnowing the analytic expression for the posterior can be useful for calculations of the marginal likelihood, as Example 2.4 demonstrates.\n\nExample 2.4 (Negative binomial as a Poisson mixture)  \n\n\nOne restriction of the Poisson model is that the restriction on its moments is often unrealistic. The most frequent problem encountered is that of overdispersion, meaning that the variability in the counts is larger than that implied by a Poisson distribution.\nOne common framework for handling overdispersion is to have \\(Y \\mid \\Lambda = \\lambda \\sim \\mathsf{Pois}(\\lambda)\\), where the mean of the Poisson distribution is itself a positive random variable with mean \\(\\mu\\), if \\(\\Lambda\\) follows a conjugate gamma distribution with shape \\(k\\mu\\) and rate \\(k&gt;0\\), \\(\\Lambda \\sim \\mathsf{Gamma}(k\\mu, k)\\), the posterior \\(\\Lambda \\mid Y=y \\sim \\mathsf{Gamma}(k\\mu + y, k+1)\\).\nSince the joint density of \\(Y\\) and \\(\\Lambda\\) can be written \\[\np(y, \\lambda) = p(y \\mid \\lambda)p(\\lambda) = p(\\lambda \\mid y) p(y)\n\\] we can isolate the marginal density \\[\\begin{align*}\np(y) &= \\frac{p(y \\mid \\lambda)p(\\lambda)}{p(\\lambda \\mid y)} \\\\&= \\frac{\\frac{\\lambda^y\\exp(-\\lambda)}{\\Gamma(y+1)}  \\frac{k^{k\\mu}\\lambda^{k\\mu-1}\\exp(-k\\lambda)}{\\Gamma(k\\mu)}}{ \\frac{(k+1)^{k\\mu+y}\\lambda^{k\\mu+y-1}\\exp\\{-(k+1)\\lambda\\}}{\\Gamma(k\\mu+y)}}\\\\\n&= \\frac{\\Gamma(k\\mu+y)}{\\Gamma(k\\mu)\\Gamma(y+1)}k^{k\\mu} (k+1)^{-k\\mu-y}\\\\&= \\frac{\\Gamma(k\\mu+y)}{\\Gamma(k\\mu)\\Gamma(y+1)}\\left(1-\\frac{1}{k+1}\\right)^{k\\mu} \\left(\\frac{1}{k+1}\\right)^y\n\\end{align*}\\] and this is the density of a negative binomial distribution with probability of success \\(1/(k+1)\\). We can thus view the negative binomial as a Poisson mean mixture.\nBy the laws of iterated expectation and iterative variance, \\[\\begin{align*}\n\\mathsf{E}(Y) &= \\mathsf{E}_{\\Lambda}\\{\\mathsf{E}(Y \\mid \\Lambda\\} \\\\& = \\mathsf{E}(\\Lambda) = \\mu\\\\\n\\mathsf{Va}(Y) &= \\mathsf{E}_{\\Lambda}\\{\\mathsf{Va}(Y \\mid \\Lambda)\\} + \\mathsf{Va}_{\\Lambda}\\{\\mathsf{E}(Y \\mid \\Lambda)\\} \\\\&= \\mathsf{E}(\\Lambda) + \\mathsf{Va}(\\Lambda) \\\\&= \\mu + \\mu/k.\n\\end{align*}\\] The marginal distribution of \\(Y\\), unconditionally, has a variance which exceeds its mean, as \\[\\begin{align*}\n\\mathsf{E}(Y) = \\mu, \\qquad \\mathsf{Va}(Y) = \\mu (1+1/k).\n\\end{align*}\\] In a negative binomial regression model, the term \\(k\\) is a dispersion parameter, which is fixed for all observations, whereas \\(\\mu = \\exp(\\boldsymbol{\\beta}\\mathbf{X})\\) is a function of covariates \\(\\mathbf{X}\\). As \\(k \\to \\infty\\), the distribution of \\(\\Lambda\\) degenerates to a constant at \\(\\mu\\) and we recover the Poisson model.\n\n\nExample 2.5 (Posterior rates for A/B tests using conjugate Poisson model) Upworthy.com, a US media publisher, revolutionized headlines online advertisement by running systematic A/B tests to compare the different wording of headlines, placement and image and what catches attention the most. The Upworthy Research Archive (Matias et al. 2021) contains results for 22743 experiments, with a click through rate of 1.58% on average and a standard deviation of 1.23%. The clickability_test_id gives the unique identifier of the experiment, clicks the number of conversion out of impressions. See Section 8.5 of Alexander (2023) for more details about A/B testing and background information.\nConsider an A/B test from November 23st, 2014, that compared four different headlines for a story on Sesame Street workshop with interviews of children whose parents were in jail and visiting them in prisons. The headlines tested were:\n\n\nSome Don’t Like It When He Sees His Mom. But To Him? Pure Joy. Why Keep Her From Him?\nThey’re Not In Danger. They’re Right. See True Compassion From The Children Of The Incarcerated.\nKids Have No Place In Jail … But In This Case, They Totally Deserve It.\nGoing To Jail Should Be The Worst Part Of Their Life. It’s So Not. Not At All.\n\n\nAt first glance, the first and third headlines seem likely to lead to a curiosity gap. The wording of the second is more explicit (and searchable), whereas the first is worded as a question.\nWe model the conversion rate \\(\\lambda_i\\) for each headline separately using a Poisson distribution and compare the posterior distributions for all four choices. Using a conjugate prior and selecting the parameters by moment matching yields approximately \\(\\alpha = 1.64\\) and \\(\\beta = 0.01\\) for the hyperparameters.\n\n\n\n\nTable 2.1: Number of views, clicks for different headlines for the Upworthy data.\n\n\nheadline\nimpressions\nclicks\n\n\n\n\nH1\n3060\n49\n\n\nH2\n2982\n20\n\n\nH3\n3112\n31\n\n\nH4\n3083\n9\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Gamma posterior for conversion rate of the different Upworthy Sesame Street headline.\n\n\n\n\nWe can visualize the posterior distributions. In this context, the large sample size lead to the dominance of the likelihood contribution \\(p(Y_i \\mid \\lambda_i) \\sim \\mathsf{Pois}(n_i\\lambda_i)\\) relative to the prior. We can see there is virtually no overlap between different rates for headers H1 (preferred) relative to H4 (least favorable). The probability that the conversion rate for Headline 3 is higher than Headline 1 can be approximated by simulating samples from both posteriors and computing the proportion of times one is larger: we get 1.7% for H3 relative to H1, indicating a clear preference for the first headline H1.\n\n\nExample 2.6 (Should you phrase your headline as a question?) We can also consider aggregate records for Upworthy, as Alexander (2023) did. The upworthy_question database contains a balanced sample of all headlines where at least one of the choices featured a question, with at least one alternative statement. Whether a headline contains a question or not is determined by querying for the question mark. We consider aggregated counts for all such headlines, with the question factor encoding whether there was a question, yes or no. For simplicity, we treat the number of views as fixed, but keep in mind that A/B tests are often sequential experiments with a stopping rule.5\nWe model first the rates using a Poisson regression; the corresponding frequentist analysis would include an offset to account for differences in views. If \\(\\lambda_{j}\\) \\((j=1, 2)\\) are the average rate for each factor level (yes and no), then \\(\\mathsf{E}(Y_{ij}/n_{ij}) = \\lambda_j\\). In the frequentist setting, we can fit a simple Poisson generalized linear regression model with an offset term and a binary variable.\n\ndata(upworthy_question, package = \"hecbayes\")\npoismod &lt;- glm(\n  clicks ~ offset(log(impressions)) + question, \n  family = poisson(link = \"log\"),\n  data = upworthy_question)\ncoef(poismod)\n\n(Intercept)  questionno \n-4.51264669  0.07069677 \n\n\nThe coefficients represent the difference in log rate (multiplicative effect) relative to the baseline rate, with an increase of 6.3 percent when the headline does not contain a question. A likelihood ratio test can be performed by comparing the deviance of the null model (intercept-only), indicating strong evidence that including question leads to significatively different rates. This is rather unsurprising given the enormous sample sizes.\nConsider instead a Bayesian analysis with conjugate prior: we model separately the rates of each group (question or not). Suppose we think apriori that the click-rate is on average 1%, with a standard deviation of 2%, with no difference between questions or not. For a \\(\\mathsf{Gamma}(\\alpha, \\beta)\\) prior, this would translate, using moment matching, into a rate of \\(\\beta = 0.04 = \\mathsf{Var}_0(\\lambda_j)/\\mathsf{E}_0(\\lambda_j)\\) and a shape of \\(\\alpha = 2.5\\) (\\(j=1, 2\\)). If \\(\\lambda_{j}\\) is the average rate for each factor level (yes and no), then \\(\\mathsf{E}(Y_{ij}/n_{ij}) = \\lambda_j\\) so the log likelihood is proportional, as a function of \\(\\lambda_1\\) and \\(\\lambda_2\\), to \\[\\begin{align*}\n\\ell(\\boldsymbol{\\lambda}; \\boldsymbol{y}, \\boldsymbol{n}) \\stackrel{\\boldsymbol{\\lambda}}{\\propto} \\sum_{i=1}^n \\sum_{j=1}^2 y_{ij}\\log \\lambda_j - \\lambda_jn_{ij}\n\\end{align*}\\] and we can recognize that the posterior for \\(\\lambda_i\\) is gamma with shape \\(\\alpha + \\sum_{i=1}^n y_{ij}\\) and rate \\(\\beta + \\sum_{i=1}^n n_{ij}.\\) For inference, we thus only need to select hyperparameters and calculate the total number of clicks and impressions per group. We can then consider the posterior difference \\(\\lambda_1 - \\lambda_2\\) or, to mimic the Poisson multiplicative model, of the ratio \\(\\lambda_1/\\lambda_2\\). The former suggests very small differences, but one must keep in mind that rates are also small. The ratio, shown in the right-hand panel of Figure 2.3, gives a more easily interpretable portrait that is in line with the frequentist analysis.\n\n\n\n\n\nFigure 2.3: Histograms of posterior summaries for differences (left) and rates (right) based on 1000 simulations from the independent gamma posteriors.\n\n\n\n\nTo get an approximation to the posterior mean of the ratio \\(\\lambda_1/\\lambda_2,\\) it suffices to draw independent observations from their respective posterior, compute the ratio and take the sample mean of those draws. We can see that the sampling distribution of the ratio is nearly symmetrical, so we can expect Wald intervals to perform well should one be interested in building confidence intervals. This is however hardly surprising given the sample size at play.\n\n\nExample 2.7 (Conjugate prior for Gaussian mean with known variance) Consider an \\(n\\) simple random sample of independent and identically distributed Gaussian variables with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), denoted \\(Y_i \\sim \\mathsf{Norm}(\\mu, \\sigma^2)\\). We pick a Gaussian prior for the location parameter, \\(\\mu \\sim \\mathsf{Norm}(\\nu, \\tau^2)\\) where we assume \\(\\mu, \\tau\\) are fixed hyperparameter values. For now, we consider only inference for \\(p(\\mu \\mid \\sigma)\\): discarding any term that is not a function of \\(\\mu\\), the conditional posterior is \\[\\begin{align*}\np(\\mu \\mid \\sigma) &\\propto \\exp\\left\\{ -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_{i}-\\mu)^2\\right\\} \\exp\\left\\{-\\frac{1}{2\\tau^2}(\\mu - \\nu)^2\\right\\}\n\\\\&\\propto \\exp\\left\\{\\left(\\frac{\\sum_{i=1}^n y_{i}}{\\sigma^2} + \\frac{\\nu}{\\tau^2}\\right)\\mu - \\left( \\frac{n}{2\\sigma^2} +\\frac{1}{2\\tau^2}\\right)\\mu^2\\right\\}.\n\\end{align*}\\] The log of the posterior density conditional on \\(\\sigma\\) is quadratic in \\(\\mu\\), it must be a Gaussian distribution truncated over the positive half line. This can be seen by completing the square in \\(\\mu\\), or by comparing this expression to the density of \\(\\mathsf{Norm}(\\mu, \\sigma^2)\\), \\[\\begin{align*}\nf(x; \\mu, \\sigma) \\stackrel{\\mu}{\\propto} \\exp\\left(-\\frac{1}{2 \\sigma^2}\\mu^2 + \\frac{x}{\\sigma^2}\\mu\\right)\n\\end{align*}\\] we can deduce by matching mean and variance that the conditional posterior \\(p(\\mu \\mid \\sigma)\\) is Gaussian with reciprocal variance (precision) \\(n/\\sigma^2 + 1/\\tau^2\\) and mean \\((n\\overline{y}\\tau^2 + \\nu \\sigma^2)/(n\\tau^2 + \\sigma^2)\\). The precision is an average of that of the prior and data, but assigns more weight to the latter, which increases linearly with the sample size \\(n\\). Likewise, the posterior mean is a weighted average of prior and sample mean, with weights proportional to the relative precision.\n\nThe exponential family is quite large; Fink (1997) A Compendium of Conjugate Priors gives multiple examples of conjugate priors and work out parameter values.\nIn general, unless the sample size is small and we want to add expert opinion, we may wish to pick an uninformative prior, i.e., one that does not impact much the outcome. For conjugate models, one can often show that the relative weight of prior parameters (relative to the random sample likelihood contribution) becomes negligible by investigating their relative weights."
  },
  {
    "objectID": "priors.html#uninformative-priors",
    "href": "priors.html#uninformative-priors",
    "title": "2  Priors",
    "section": "2.3 Uninformative priors",
    "text": "2.3 Uninformative priors\n\nDefinition 2.2 (Proper prior) We call a prior function proper if it’s integral is finite over the parameter space; such prior function automatically leads to a valid posterior.\n\nThe best example of prior priors arise from probability density function. We can still employ this rule for improper priors: for example, taking \\(\\alpha, \\beta \\to 0\\) in the beta prior leads to a prior proportional to \\(x^{-1}(1-x)^{-1}\\), the integral of which diverges on the unit interval \\([0,1]\\). However, as long as the number of success and the number of failures is larger than 1, meaning \\(k \\geq 1, n-k \\geq 1\\), the posterior distribution would be proper, i.e., integrable. To find the posterior, normalizing constants are also superfluous.\nMany uninformative priors are flat, or proportional to a uniform on some subset of the real line and therefore improper. It may be superficially tempting to set a uniform prior on a large range to ensure posterior property, but the major problem is that a flat prior may be informative in a different parametrization, as the following example suggests.\nGelman et al. (2013) uses the following taxonomy for various levels of prior information: uninformative priors are generally flat or uniform priors with \\(p(\\beta) \\propto 1\\), vague priors are typically nearly flat even if proper, e.g., \\(\\beta \\sim \\mathsf{Norm}(0, 100)\\), weakly informative priors provide little constraints \\(\\beta \\sim \\mathsf{Norm}(0, 10)\\), and informative prior are typically application-specific, but constrain the ranges. Uninformative and vague priors are generally not recommended unless they are known to give valid posterior inference and the amount of information from the likelihood is high.\n\nExample 2.8 (Transformation of flat prior for scales) Consider the parameter \\(\\log(\\tau) \\in \\mathbb{R}\\) and the prior \\(p( \\log \\tau) \\propto 1\\). If we reparametrize the model in terms of \\(\\tau\\), the new prior (including the Jacobian of the transformation) is \\(\\tau^{-1}\\)\n\nSome priors are standard and widely used. In location scale families with location \\(\\nu\\) and scale \\(\\tau\\), the density is such that \\[\\begin{align*}\nf(x; \\nu, \\tau) =  \\frac{1}{\\tau} f\\left(\\frac{x - \\nu}{\\tau}\\right), \\qquad \\nu \\in \\mathbb{R}, \\tau &gt;0.\n\\end{align*}\\] We thus wish to have a prior so that \\(p(\\tau) = c^{-1}p(\\tau/c)\\) for any scaling \\(c&gt;0\\), whence it follows that \\(p(\\tau) \\propto \\tau^{-1}\\), which is uniform on the log scale.\nThe priors \\(p(\\nu) \\propto 1\\) and \\(p(\\tau) \\propto \\tau^{-1}\\) are both improper but lead to location and scale invariance, hence that the result is the same regardless of the units of measurement.\n\nOne criticism of the Bayesian approach is the arbitrariness of prior functions. However, the role of the prior is often negligible in large samples (consider for example the posterior of exponential families with conjugate priors). Moreover, the likelihood is also chosen for convenience, and arguably has a bigger influence on the conclusion. Data fitted using a linear regression model seldom follow Gaussian distributions conditionally, in the same way that the linearity is a convenience (and first order approximation).\n\n\nDefinition 2.3 (Jeffrey’s prior) In single parameter models, taking a prior function for \\(\\theta\\) proportional to the square root of the determinant of the information matrix, \\(p(\\theta) \\propto |\\imath(\\theta)|^{1/2}\\) yields a prior that is invariant to reparametrization, so that inferences conducted in different parametrizations are equivalent.6\n\nTo see this, consider a bijective transformation \\(\\theta \\mapsto \\vartheta\\). Under the reparametrized model and suitable regularity conditions7, the chain rule implies that \\[\\begin{align*}\ni(\\vartheta) &= - \\mathsf{E} \\left(\\frac{\\partial^2 \\ell(\\vartheta)}{\\partial^2 \\vartheta}\\right)\n\\\\&= - \\mathsf{E}\\left(\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2}\\right) \\left( \\frac{\\mathrm{d} \\theta}{\\mathrm{d} \\vartheta} \\right)^2 + \\mathsf{E}\\left(\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}\\right) \\frac{\\mathrm{d}^2 \\theta}{\\mathrm{d} \\vartheta^2}\n\\end{align*}\\] Since the score has mean zero, \\(\\mathsf{E}\\left\\{\\partial \\ell(\\theta)/\\partial \\theta\\right\\}=0\\) and the rightmost term vanishes. We can thus relate the Fisher information in both parametrizations, with \\[\\begin{align*}\n\\imath^{1/2}(\\vartheta) = \\imath^{1/2}(\\theta) \\left| \\frac{\\mathrm{d} \\theta}{\\mathrm{d} \\vartheta} \\right|,\n\\end{align*}\\] implying invariance.\nIn multiparameter models, the system isn’t invariant to reparametrization if we consider the determinant of the Fisher information.\n\n\nExample 2.9 (Jeffrey’s prior for the binomial distribution) Consider the binomial distribution \\(\\mathsf{Bin}(1, \\theta)\\) with density \\(f(y; \\theta) \\propto \\theta^y(1-\\theta)^{1-y}\\mathsf{I}_{\\theta \\in [0,1]}\\). The negative of the second derivative of the log likelihood with respect to \\(p\\) is \\[\\jmath(\\theta) = - \\partial^2 \\ell(\\theta; y) / \\partial \\theta^2 = y/\\theta^2 + (1-y)/(1-\\theta)^2\\] and since \\(\\mathsf{E}(Y)=\\theta\\), the Fisher information is \\[\\imath(\\vartheta) = \\mathsf{E}\\{\\jmath(\\theta)\\}=1/\\theta + 1/(1-\\theta) = 1/\\{\\theta(1-\\theta)\\}\\] Jeffrey’s prior is thus \\(p(\\theta) \\propto \\theta^{-1/2}(1-\\theta)^{-1/2}\\), a conjugate Beta prior \\(\\mathsf{Beta}(0.5,0.5)\\)."
  },
  {
    "objectID": "priors.html#jeffreys-prior-for-the-normal-distribution",
    "href": "priors.html#jeffreys-prior-for-the-normal-distribution",
    "title": "2  Priors",
    "section": "2.4 Jeffrey’s prior for the normal distribution",
    "text": "2.4 Jeffrey’s prior for the normal distribution\nCheck that for the Gaussian distribution \\(\\mathsf{Norm}(\\mu, \\sigma^2)\\), the Jeffrey’s prior obtained by treating each parameter as fixed in turn, are \\(p(\\mu) \\propto 1\\) and \\(p(\\sigma) \\propto 1/\\sigma\\), which also correspond to the default uninformative priors for location-scale families."
  },
  {
    "objectID": "priors.html#informative-priors",
    "href": "priors.html#informative-priors",
    "title": "2  Priors",
    "section": "2.5 Informative priors",
    "text": "2.5 Informative priors\nOne strength of the Bayesian approach is the capability of incorporating expert and domain-based knowledge through priors. Often, these will take the form of moment constraints, so one common way to derive a prior is to perform moment matching to related ellicited quantities with moments of the prior distribution. It may be easier to set priors on a different scale than those of the observations, as Example 2.11 demonstrates.\n\nExample 2.11 (Gamma quantile difference priors for extreme value distributions) The generalized extreme value distribution arises as the limiting distribution for the maximum of \\(m\\) independent observations from some common distribution \\(F\\). The \\(\\mathsf{GEV}(\\mu, \\sigma, \\xi)\\) distribution is a location-scale with distribution function \\[\\begin{align*}\nF(x) = \\exp\\left[ - \\left\\{1+\\xi(x-\\mu)/\\sigma\\right\\}^{-1/\\xi}_{+}\\right]\n\\end{align*}\\] where \\(x_{+} = \\max\\{0, x\\}\\).\nInverting the distribution function yields the quantile function \\[\\begin{align*}\nQ(p) \\mu + \\sigma \\frac{(-\\log p)^{-\\xi}-1}{\\xi}\n\\end{align*}\\]\nIn environmental data, we often model annual maximum. Engineering designs are often specified in terms of the \\(k\\)-year return levels, defined as the quantile of the annual maximum exceeded with probability \\(1/k\\) in any given year. Using a \\(\\mathsf{GEV}\\) for annual maximum, Coles and Tawn (1996) proposed modelling annual daily rainfall and specifying a prior on the quantile scale \\(q_1 &lt; q_2 &lt; q_3\\) for tail probabilities \\(p_1&gt; p_2 &gt; p_3\\). To deal with the ordering constraints, gamma priors are imposed on the differences \\(q_1 - o \\sim \\mathsf{Gamma}(\\alpha_1, \\beta_1)\\), \\(q_2 - q_1 \\sim \\mathsf{Gamma}(\\alpha_2, \\beta_2)\\) and \\(q_3-q_2 \\sim \\mathsf{Gamma}(\\alpha_3, \\beta_3)\\), where \\(o\\) is the lower bound of the support. The prior is thus of the form\n\\[\\begin{align*}\np(\\boldsymbol{q}) \\propto q_1^{\\alpha_1-1}\\exp(-\\beta_1 q_1) \\prod_{i=2}^3 (q_i-q_{i-1})^{\\alpha_i-1} \\exp\\{\\beta_i(q_i-q_{i-1})\\}.\n\\end{align*}\\] where \\(0 \\leq q_1 \\leq q_2 \\leq q_3\\). The fact that these quantities refer to moments or risk estimates which practitioners often must compute as part of regulatory requirements makes it easier to specify sensible values for hyperparameters.\nAs illustrating example, consider maximum daily cumulated rainfall in Abisko, Sweden. The time series spans from 1913 until December 2014; we compute the 102 yearly maximum, which range from 11mm to 62mm, and fit a generalized extreme value distribution to these.\nFor the priors, suppose an expert elicits quantiles of the 10, 50 and 100 years return levels; say 30mm, 45mm and 70mm, respectively, for the median and likewise 40mm, 70mm and 120mm for the 90% percentile of the return levels. We can compute the differences and calculate the parameters of the gamma distribution through moment-matching: this gives roughly a shape of \\(\\alpha_1=18.27\\) and \\(\\beta_1=0.6\\), etc. Figure 2.4 shows the transfer from the prior predictive to the posterior distribution. The prior is much more dispersed and concentrated on the tail, which translates in a less peaked posterior than using a weakly informative prior (dotted line): the mode of the latter is slightly to the left and with lower density in the tail.\n\n\n\n\n\nFigure 2.4: Kernel density estimates of draws from the posterior distribution of 100 year return levels with a Coles–Tawn quantile prior (full line) and from the corresponding prior predictive (dashed). The dotted line gives the posterior distribution for a maximum domain information prior on the shape with improper priors on location and scale.\n\n\n\n\n\nWhat would you do if we you had prior information from different sources? One way to combine these is through a mixture: given \\(M\\) different prior distributions \\(p_m(\\boldsymbol{\\theta})\\), we can assign each a positive weight \\(w_m\\) to form a mixture of experts prior through the linear combination \\[ p(\\boldsymbol{\\theta}) \\propto \\sum_{m=1}^M w_m p_m(\\boldsymbol{\\theta})\\] \n\n\n\n\n2.5.1 Penalized complexity priors\nOftentimes, there will be a natural family of prior density to impose on some model component, \\(p(\\boldsymbol{\\theta} \\mid \\zeta)\\), with hyperparameter \\(\\zeta\\). The flexibility of the underlying construction leads itself to overfitting. Penalized complexity priors (Simpson et al. 2017) aim to palliate this by penalizing models far away from a simple baseline model, which correspond to a fixed value \\(\\zeta_0\\). The prior will favour the simpler parsimonious model the more prior mass one places on \\(\\zeta_0\\), which is in line with Occam’s razor principle.\nTo construct a penalized-complexity prior, we compute the Kullback–Leibler divergence between the model \\(p_\\zeta \\equiv p(\\boldsymbol{\\theta} \\mid \\zeta)\\) relative to the baseline with \\(\\zeta_0\\), \\(p_0 \\equiv p(\\boldsymbol{\\theta} \\mid \\zeta_0)\\); the Kullback–Leibler divergence is\n\\[\n\\mathsf{KL}(p_\\zeta \\Vert\\, p_0)=\\int p_\\zeta \\log\\left(\\frac{p_\\zeta}{p_0}\\right) \\mathrm{d} \\boldsymbol{\\theta}.\n\\] The distance between the prior densities is then set to \\(d(\\zeta) = \\{2\\mathsf{KL}(p_\\zeta \\mid\\mid p_0)\\}^{1/2}\\). which is zero at the model with \\(\\zeta_0\\). The PC prior then constructs an exponential prior on the distance scale, which after back-transformation gives \\(p(\\zeta \\mid \\lambda) = \\lambda\\exp(-\\lambda d(\\zeta)) \\left| {\\partial d(\\zeta)}/{\\partial \\zeta}\\right|\\). To choose \\(\\lambda\\), the authors recommend elicitation of a pair \\((U, \\alpha)\\) such that \\(\\Pr(\\lambda &gt; U)=\\alpha\\).\n\nExample 2.12 (Penalized complexity prior for random effects models) Simpson et al. (2017) give the example of a Gaussian prior for random effects \\(\\boldsymbol{\\alpha}\\), of the form \\(\\boldsymbol{\\alpha} \\mid \\zeta \\sim \\mathsf{Norm}_J(\\boldsymbol{0}_J, \\zeta^2 \\mathbf{I}_J)\\) where \\(\\zeta_0=0\\) corresponds to the absence of random subject-variability. The penalized complexity prior for the scale \\(\\zeta\\) is then an exponential with rate \\(\\lambda\\),8 with density \\(p(\\zeta \\mid \\lambda) = \\lambda \\exp(-\\lambda \\zeta)\\). Using the recommendation for setting \\(\\lambda\\), we get that \\(\\lambda = -\\ln(\\alpha/U)\\) and this can be directly interpreted in terms of standard deviation of \\(\\zeta\\); simulation from the prior predictive may also be used for calibration.\n\n\nExample 2.13 (Penalized complexity prior for autoregressive model of order 1) Sørbye and Rue (2017) derive penalized complexity prior for the Gaussian stationary AR(1) model with autoregressive parameter \\(\\phi \\in (-1,1)\\), where \\(Y_t \\mid Y_{t-1}, \\phi, \\sigma^2 \\sim \\mathsf{Norm}(\\phi Y_{t-1}, \\sigma^2)\\). There are two based models that could be of interest: one with \\(\\phi=0\\), corresponding to a memoryless model with no autocorrelation, and a static mean \\(\\phi=1\\) for no change in time; note that the latter is not stationary. For the former, the penalized complexity prior is \\[\\begin{align*}\np(\\phi \\mid \\lambda) = \\frac{\\lambda}{2} \\exp\\left[-\\lambda \\left\\{-\\ln(1-\\phi^2)\\right\\}^{1/2}\\right] \\frac{|\\phi|}{(1-\\phi^2)\\left\\{-\\ln(1-\\phi^2)\\right\\}^{1/2}}.\n\\end{align*}\\] One can set \\(\\lambda\\) by considering plausible values by relating the parameter to the variance of the one-step ahead forecast error.\n\nGaussian components are widespread: not only for linear regression models, but more generally for the specification of random effects that capture group-specific effects, residuals spatial or temporal variability. In the Bayesian paradigm, there is no difference between fixed effects \\(\\boldsymbol{\\beta}\\) and the random effect parameters: both are random quantities that get assigned priors, but we will treat these priors differently.\nThe reason why we would like to use a penalized complexity prior for a random effect, say \\(\\alpha_j \\sim \\mathsf{Norm}(0, \\zeta^2)\\), is because we don’t know a prior if there is variability between groups. The inverse gamma prior for \\(\\zeta\\), \\(\\zeta \\sim \\mathsf{InvGamma}(\\epsilon, \\epsilon)\\) does not have a mode at zero unless it is improper with \\(\\epsilon \\to 0\\). Generally, we want our prior for the variance to have significant probability density at the null \\(\\zeta=0\\). The penalized complexity prior is not the only sensible choice. Posterior inference is unfortunately sensitive to the value of \\(\\epsilon\\) in hierarchical models when the random effect variance is close to zero, and more so when there are few levels for the groups since the relative weight of the prior relative to that of the likelihood contribution is then large.\n\nExample 2.14 (Student-t prior for variance components) Gelman (2006) recommends a Student-\\(t\\) distribution truncated below at \\(0\\), with low degrees of freedom. The rationale for this choice comes from the simple two level model with \\(n_j\\) independent in each group \\(j=1, \\ldots, J\\): for observation \\(i\\) in group \\(j\\), \\[\\begin{align*}\nY_{ij} &\\sim \\mathsf{Norm}(\\mu + \\alpha_j, \\sigma^2),\\\\\n\\alpha_j &\\sim \\mathsf{Norm}(0, \\tau^2_\\alpha),\n\\end{align*}\\] The conditionally conjugate prior \\(p(\\tau \\mid \\boldsymbol{\\alpha}, \\mu, \\sigma)\\) is inverse gamma. Standard inference with this parametrization is however complicated, because there is strong dependence between parameters.\nTo reduce this dependence, one can add a parameter, taking \\(\\alpha_j = \\xi \\eta_j\\) and \\(\\tau_\\alpha=|\\xi|\\tau_{\\eta}\\); the model is now overparametrized. Suppose \\(\\eta_j \\sim \\mathsf{Norm}(0, \\tau^2_\\eta)\\) and consider the likelihood conditional on \\(\\mu, \\eta_j\\): we have that \\((y_{ij} - \\mu)/\\eta_j \\sim \\mathsf{Norm}(\\xi, \\sigma^2/\\eta_j)\\) so conditionally conjugate priors for \\(\\xi\\) and \\(\\tau_\\eta\\) are respectively Gaussian and inverse gamma. This translates into a prior distribution for \\(\\tau_\\alpha\\) which is that of the absolute value of a noncentral Student-\\(t\\) with location, scale and degrees of freedom \\(\\nu\\). If we set the location to zero, the prior puts high mass at the origin, but is heavy tailed with polynomial decay. We recommend to set degrees of freedom so that the variance is heavy-tailed, e.g., \\(\\nu=3\\). While this prior is not conjugate, it compares favorably to the \\(\\mathsf{IGa}(\\epsilon, \\epsilon)\\).\n\n\nExample 2.15 (Poisson random effect models) We consider data from an experimental study conducted at Tech3Lab on road safety. In Brodeur et al. (2021), 31 participants were asked to drive in a virtual environment; the number of road violation was measured for different type of distractions (phone notification, phone on speaker, texting and smartwatch). The data are balanced, with each participant exposed to each task exactly once.\nWe model the data using a Poisson mixed model to measure the number of violations, nviolation, with a fixed effect for task, which captures the type of distraction, and a random effect for participant id. The hierarchical model fitted for individual \\(i\\) \\((i=1, \\ldots, 34)\\) and distraction type \\(j\\) \\((j=1, \\ldots, 4)\\) is \\[\\begin{align*}\nY_{ij} &\\sim \\mathsf{Pois}\\{\\mu = \\exp(\\beta_{j} + \\alpha_i)\\},\\\\\n\\beta_j &\\sim \\mathsf{Norm}(0, 100), \\\\\n\\alpha_i &\\sim \\mathsf{Norm}(0, \\kappa^2), \\\\\n\\kappa &\\sim \\mathsf{St}_{+}(3).\n\\end{align*}\\] so observations are conditionally independent given hyperparameters \\(\\boldsymbol{\\alpha}\\) and \\(\\boldsymbol{\\beta}\\).\nIn frequentist statistics, there is a distinction made in mixed-effect models between parameters that are treated as constants, termed fixed effects and corresponding in this example to \\(\\boldsymbol{\\beta}\\), and random effects, equivalent to \\(\\boldsymbol{\\alpha}\\). There is no such distinction in the Bayesian paradigm, except perhaps for the choice of prior.\nWe can look at some of posterior distribution of the 31 random effects (here the first five individuals) and the fixed effect parameters \\(\\boldsymbol{\\beta}\\), plus the variance of the random effect \\(\\kappa\\): there is strong evidence that the latter is non-zero, suggesting strong heterogeneity between individuals. The distraction which results in the largest number of violation is texting, while the other conditions all seem equally distracting on average (note that there is no control group with no distraction to compare with, so it is hard to draw conclusions).\n\n\n\n\n\nFigure 2.5: Posterior density plots with 50% credible intervals and median value for the random effects of the first five individuals (left) and the fixed effects and random effect variance (right)."
  },
  {
    "objectID": "priors.html#sensitivity-analysis",
    "href": "priors.html#sensitivity-analysis",
    "title": "2  Priors",
    "section": "2.6 Sensitivity analysis",
    "text": "2.6 Sensitivity analysis\nDo priors matter? The answer to that question depends strongly on the model, and how much information the data provides about hyperparameters. While this question is easily answered in conjugate models (the relative weight of hyperparameters relative to data can be derived from the posterior parameters), it is not so simple in hierarchical models, where the interplay between prior distributions is often more intricate. To see the impact, one often has to rely on doing several analyses with different values fr the prior and see the sensitivity of the conclusions to these changes, for example by considering a vague prior or modifying the parameters values (say halving or doubling). If the changes are immaterial, then this provides reassurance that our analyses are robust.\n\nExample 2.16 To check the sensitivity of the conclusion, we revisit the modelling of the smartwatch experiment data using a Poisson regression and compare four priors: a uniform prior truncated to \\([0, 10]\\), an inverse gamma \\(\\mathsf{InvGamma}(0.01, 0.01)\\) prior, a penalized complexity prior such that the 0.95 percentile of the scale is 5, corresponding to \\(\\mathsf{Exp}(0.6)\\). Since each distraction type appears 31 times, there is plenty of information to reliably estimate the dispersion \\(\\kappa\\) of the random effects \\(\\alpha\\): the different density plots in Figure 2.6 are virtually indistinguishable from one another. This is perhaps unsurprising given the large number of replicates, and the significant variability between groups.\n\n\n\n\n\nFigure 2.6: Posterior density of the scale of the random effects with uniform, inverse gamma, penalized complexity and folded Student-t with three degrees of freedom. The circle denotes the median and the bars the 50% and 95% percentile credible intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Boca Raton, FL: CRC Press.\n\n\nBrodeur, Mathieu, Perrine Ruer, Pierre-Majorique Léger, and Sylvain Sénécal. 2021. “Smartwatches Are More Distracting Than Mobile Phones While Driving: Results from an Experimental Study.” Accident Analysis & Prevention 149: 105846. https://doi.org/10.1016/j.aap.2020.105846.\n\n\nColes, Stuart G., and Jonathan A. Tawn. 1996. “A Bayesian Analysis of Extreme Rainfall Data.” Journal of the Royal Statistical Society. Series C (Applied Statistics) 45 (4): 463–78. https://doi.org/10.2307/2986068.\n\n\nGelman, Andrew. 2006. “Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).” Bayesian Analysis 1 (3): 515–34. https://doi.org/10.1214/06-BA117A.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd ed. New York: Chapman; Hall/CRC. https://doi.org/10.1201/b16018.\n\n\nMatias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles Ebersole. 2021. “The Upworthy Research Archive, a Time Series of 32,487 Experiments in U.S. Media.” Scientific Data 8 (195). https://doi.org/10.1038/s41597-021-00934-7.\n\n\nSimpson, Daniel, Håvard Rue, Andrea Riebler, Thiago G. Martins, and Sigrunn H. Sørbye. 2017. “Penalising Model Component Complexity: A Principled, Practical Approach to Constructing Priors.” Statistical Science 32 (1): 1–28. https://doi.org/10.1214/16-STS576.\n\n\nSørbye, Sigrunn Holbek, and Håvard Rue. 2017. “Penalised Complexity Priors for Stationary Autoregressive Processes.” Journal of Time Series Analysis 38 (6): 923–35. https://doi.org/10.1111/jtsa.12242."
  },
  {
    "objectID": "priors.html#footnotes",
    "href": "priors.html#footnotes",
    "title": "2  Priors",
    "section": "",
    "text": "If counts are Poisson, then the log transform is variance stabilizing.↩︎\nOne can object to the prior parameters depending on the data, but an alternative would be to model centered data \\(y-\\overline{y}\\), in which case the prior for the intercept parameter \\(\\beta_0\\) would be zero.↩︎\nA distribution belongs to an exponential family with parameter vector \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^D\\) if it can be written as \\[\\begin{align*}\nf(y; \\boldsymbol{\\theta}) = \\exp\\left\\{ \\sum_{k=1}^K Q_k(\\boldsymbol{\\theta}) t_k(y) + D(\\boldsymbol{\\theta})\\right\\}\n\\end{align*}\\] and in particular, the support does not depend on unknown parameters. If we have an independent and identically distributed sample of observations \\(y_1, \\ldots, y_n\\), the log likelihood is thus of the form \\[\\begin{align*}\n\\ell(\\boldsymbol{\\theta}) = \\sum_{k=1}^K \\phi_k(\\boldsymbol{\\theta}) \\sum_{i=1}^n t_k(y_i) + n D(\\boldsymbol{\\theta}),\n\\end{align*}\\] where the collection \\(\\sum_{i=1}^n t_k(y_i)\\) (\\(k=1, \\ldots, K\\)) are sufficient statistics and \\(\\phi_k(\\boldsymbol{\\theta})\\) are the canonical parameters. The number of sufficient statistics are the same regardless of the sample size. Exponential families play a prominent role in generalized linear models, in which the natural parameters are modeled as linear function of explanatory variables. A log prior density with parameters \\(\\eta, \\nu_1, \\ldots, \\nu_K\\) that is proportional to \\[\\begin{align*}\n\\log p(\\boldsymbol{\\theta}) \\propto \\eta D(\\boldsymbol{\\theta}) + \\sum_{k=1}^K Q_k(\\boldsymbol{\\theta}) \\nu_k\n\\end{align*}\\] is conjugate.↩︎\nThe canonical link function for Bernoulli gives rise to logistic regression model.↩︎\nThe stopping rule means that data stops being collected once there is enough evidence to determine if an option is more suitable, or if a predetermined number of views has been reached.↩︎\nThe Fisher information is linear in the sample size for independent and identically distributed data so we can derive the result for \\(n=1\\) without loss of generality.↩︎\nUsing Bartlett’s identity; Fisher consistency can be established using the dominated convergence theorem.↩︎\nPossibly truncated above if the support of \\(\\zeta\\) has a finite upper bound.↩︎"
  },
  {
    "objectID": "mcmc.html#monte-carlo-methods",
    "href": "mcmc.html#monte-carlo-methods",
    "title": "3  Simulation-based inference",
    "section": "3.1 Monte Carlo methods",
    "text": "3.1 Monte Carlo methods\nConsider a target distribution with finite expected value: think of the posterior of some model of interest, or some functional thereof. The law of large numbers guarantees that, if we can draw observations from our target distribution, then the sample average will converge to the expected value of that distribution, as the sample size becomes larger and larger, provided the expectation is finite.\nWe can thus compute the probability of any event or the expected value of any (integrable) function by computing sample averages; the cost to pay for this generality is randomness.\nSpecifically, suppose we are interested in the average \\(\\mathsf{E}\\{g(X)\\}\\) of \\(X_i \\sim F\\) for some function \\(g\\).\n\nExample 3.1 Consider \\(X \\sim \\mathsf{Gamma}(\\alpha, \\beta)\\), a gamma distribution with shape \\(\\alpha\\) and rate \\(\\beta\\). We can compute the probability that \\(X &lt; 1\\) easily by Monte Carlo since \\(\\Pr(X &lt;1) = \\mathsf{E}\\{\\mathrm{I}(X&lt;1)\\}\\) and this means we only need to compute the proportion of draws less than one. We can likewise compute the mean \\(g(x) = x\\) or variance.\nSuppose we have drawn a Monte Carlo sample of size \\(B\\). If the function \\(g(\\cdot)\\) is square integrable,1 with variance \\(\\sigma^2_g\\), then a central limit theorem applies. In large samples and for independent observations, our Monte Carlo average \\(\\widehat{\\mu}_g = B^{-1}\\sum_{b=1}^B g(X_i)\\) has variance \\(\\sigma^2_g/B\\). We can approximate the unknown variance \\(\\sigma^2_g\\) by it’s empirical counterpart.2. Note that, while the variance decreases linearly with \\(B\\), the choice of \\(g\\) impacts the speed of convergence: we can compute \\[\\sigma^2_g =\\Pr(X \\leq 1)\\{1-\\Pr(X \\leq 1)\\}=0.0434\\] (left) and \\(\\sigma^2_g=\\alpha/\\beta^2=1/8\\) (middle plot).\nFigure 3.1 shows the empirical trace plot of the Monte Carlo average (note the \\(\\sqrt{B}\\) \\(x\\)-axis scale!) as a function of the Monte Carlo sample size \\(B\\) along with 95% Wald-based confidence intervals (gray shaded region), \\(\\widehat{\\mu}_g \\pm 1.96 \\times \\sigma_g/\\sqrt{B}\\). We can see that the ‘likely region’ for the average shrinks with \\(B\\).\nWhat happens if our function is not integrable? The right-hand plot of Figure 3.1 shows empirical averages of \\(g(x) = x^{-1}\\), which is not integrable if \\(\\alpha &lt; 1\\). We can compute the empirical average, but the result won’t converge to any meaningful quantity regardless of the sample size. The large jumps are testimonial of this.\n\n\n\n\n\nFigure 3.1: Running mean trace plots for \\(g(x)=\\mathrm{I}(x&lt;1)\\) (left), \\(g(x)=x\\) (middle) and \\(g(x)=1/x\\) (right) for a Gamma distribution with shape 0.5 and rate 2, as a function of the Monte Carlo sample size.\n\n\n\n\n\nWe have already used Monte Carlo methods to compute posterior quantities of interest in conjugate models. Outside of models with conjugate priors, the lack of closed-form expression for the posterior precludes inference. Indeed, calculating the posterior probability of an event, or posterior moments, requires integration of the normalized posterior density and thus knowledge of the marginal likelihood. It is seldom possible to sample independent and identically distributed (iid) samples from the target, especially if the model is high dimensional: rejection sampling and the ratio of uniform method are examples of Monte Carlo methods which can be used to generate iid draws.\n\n\nProposition 3.1 (Rejection sampling) Rejection sampling (also termed accept-reject algorithm) samples from a random vector with density \\(p(\\cdot)\\) by drawing candidates from a proposal with density \\(q(\\cdot)\\) with nested support, \\(\\mathrm{supp}(p) \\subseteq \\mathrm{supp}(q)\\). The density \\(q(\\cdot)\\) must be such that \\(p(\\boldsymbol{\\theta}) \\leq C q(\\boldsymbol{\\theta})\\) for \\(C \\geq 1\\) for all values of \\(\\boldsymbol{\\theta}\\) in the support of \\(p(\\cdot)\\). A proof can be found in Devroye (1986, Theorem 3.1)\n\nGenerate \\(\\boldsymbol{\\theta}^{\\star}\\) from the proposal with density \\(q\\) and \\(U \\sim \\mathsf{U}(0,1)\\)\nCompute the ratio \\(R \\gets p(\\boldsymbol{\\theta}^{\\star})/ q(\\boldsymbol{\\theta}^{\\star})\\).\nIf \\(R \\geq CU\\), return \\(\\boldsymbol{\\theta}\\), else go back to step 1.\n\n\n\n\n\n\n\nFigure 3.2: Target density (full) and scaled proposal density (dashed): the vertical segment at \\(x=1\\) shows the percentage of acceptance for a uniform slice under the scaled proposal, giving an acceptance ratio of 0.58.\n\n\n\n\nRejection sampling requires the proposal \\(q\\) to have a support at least as large as that of \\(p\\) and resemble closely the density. It should be chosen so that the upper bound \\(C\\) is as sharp as possible and close to 1. The dominating density \\(q\\) must have heavier tails than the density of interest. The expected number of simulations needed to accept one proposal is \\(C.\\) Finally, for the method to be useful, we need to be able to simulate easily and cheaply from the proposal. The optimal value of \\(C\\) is \\(C = \\sup_{\\boldsymbol{\\theta}} p(\\boldsymbol{\\theta}) / q(\\boldsymbol{\\theta})\\). This quantity may be obtained by numerical optimization, by finding the mode of the ratio of the log densities if the maximum is not known analytically.\n\nExample 3.2 (Truncated Gaussian distribution) Consider the problem of sampling from a Gaussian distribution \\(Y \\sim \\mathsf{Norm}(\\mu, \\sigma^2)\\) truncated in the interval \\([a, b],\\) which has density \\[\\begin{align*}\nf(x; \\mu, \\sigma, a, b) = \\frac{1}{\\sigma}\\frac{\\phi\\left(\\frac{x-\\mu}{\\sigma}\\right)}{\\Phi\\{(b-\\mu)/\\sigma\\}-\\Phi\\{(a-\\mu)/\\sigma\\}}.\n\\end{align*}\\] where \\(\\phi(\\cdot), \\Phi(\\cdot)\\) are respectively the density and distribution function of the standard Gaussian distribution.\nSince the Gaussian is a location-scale family, we can reduce the problem to sampling \\(X\\) from a standard Gaussian truncated on \\(\\alpha = (a-\\mu)/\\sigma\\) and \\(\\beta = (b-\\mu)/\\sigma\\) and back transform the result as \\(Y = \\mu + \\sigma X\\).\nA crude accept-reject sampling algorithm would consider sampling from the same untruncated distribution with density \\(g(X) = \\sigma^{-1}\\phi\\{(x-\\mu)/\\sigma\\}\\), and the acceptance ratio is \\(C^{-1}=\\{\\Phi(\\beta) - \\Phi(\\alpha)\\}\\). We thus simply simulate points from the Gaussian and accept any that falls within the bounds.\n\n# Standard Gaussian truncated on [0,1]\ncandidate &lt;- rnorm(1e5)\ntrunc_samp &lt;- candidate[candidate &gt;= 0 & candidate &lt;= 1]\n# Acceptance rate\nlength(trunc_samp)/1e5\n\n[1] 0.34242\n\n# Theoretical acceptance rate\npnorm(1)-pnorm(0)\n\n[1] 0.3413447\n\n\nWe can of course do better: if we consider a random variable with distribution function \\(F,\\) but truncated over the interval \\([a,b],\\) then the resulting distribution function is \\[\\frac{F(x) - F(a)}{F(b)-F(a)}, \\qquad a \\leq x \\leq b,\\] and we can invert this expression to get the quantile function of the truncated variable in terms of the distribution function \\(F\\) and the quantile function \\(F^{-1}\\) of the original untruncated variable.\nFor the Gaussian, this gives \\[\\begin{align*}\nX \\sim \\Phi^{-1}\\left[\\Phi(\\alpha) + \\{\\Phi(\\beta)-\\Phi(\\alpha)\\}U\\right]\n\\end{align*}\\] for \\(U \\sim \\mathsf{U}(0,1)\\). Although the quantile and distribution functions of the Gaussian, pnorm and qnorm in R, are very accurate, this method will fail for rare event simulation because it will return \\(\\Phi(x) = 0\\) for \\(x \\leq -39\\) and \\(\\Phi(x)=1\\) for \\(x \\geq 8.3\\), implying that \\(a \\leq 8.3\\) for this approach to work (Botev and L’Écuyer 2017).\nConsider the problem of simulating events in the right tail for a standard Gaussian where \\(a &gt; 0\\); Marsaglia’s method (Devroye 1986, 381), can be used for that purpose. Write the density of the Gaussian as \\(f(x) = \\exp(-x^2/2)/c_1\\), where \\(c_1 = \\int_{a}^{\\infty}\\exp(-z^2/2)\\mathrm{d} z\\), and note that \\[c_1f(x) \\leq \\frac{x}{a}\\exp\\left(-\\frac{x^2}{2}\\right)= a^{-1}\\exp\\left(-\\frac{a^2}{2}\\right)g(x), \\qquad x \\geq a;\\] where \\(g(x)\\) is the density of a Rayleigh variable shifted by \\(a\\), which has distribution function \\(G(x) = 1-\\exp\\{(a^2-x^2)/2\\}\\) for \\(x \\geq a\\). We can simulate such a random variate \\(X\\) through the inversion method. The constant \\(C= \\exp(-a^2/2)(c_1a)^{-1}\\) approaches 1 quickly as \\(a \\to \\infty\\).\nThe accept-reject thus proceeds with\n\nGenerate a shifted Rayleigh above \\(a\\), \\(X \\gets \\{a^2 - 2\\log(U)\\}^{1/2}\\) for \\(U \\sim \\mathsf{U}(0,1)\\)\nAccept \\(X\\) if \\(XV \\leq a\\), where \\(V \\sim \\mathsf{U}(0,1)\\).\n\nShould we wish to obtain samples on \\([a,b]\\), we could instead propose from a Rayleigh truncated above at \\(b\\) (Botev and L’Écuyer 2017).\n\na &lt;- 8.3\nniter &lt;- 1000L\nX &lt;- sqrt(a^2 + 2*rexp(niter))\nsamp &lt;- X[runif(niter)*X &lt;= a]\n\n\nFor a given candidate density \\(g\\) which has a heavier tail than the target, we can resort to numerical methods to compute the mode of the ratio \\(f/g\\) and obtain the bound \\(C\\); see Albert (2009), Section 5.8 for an insightful example.\n\nProposition 3.2 (Ratio of uniform method) The ratio-of-uniform method (Kinderman and Monahan 1977; Wakefield, Gelfand, and Smith 1991) is a variant of accept-reject used to draw samples from a unnormalized density \\(f(\\boldsymbol{\\theta})\\) for \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta} \\subseteq \\mathbb{R}^d\\). For some \\(r \\geq 0\\), consider the set \\[\\begin{align*}\n\\mathcal{C}_r = \\left\\{ (u_0, \\ldots, u_d): 0 &lt; u_0 \\leq \\left[f(u_1/u_0^r, \\ldots, u_d/u_0^r)\\right]^{\\frac{1}{rd+1}}\\right\\}.\n\\end{align*}\\] If we can generate \\(u_0, \\ldots, u_d\\) uniformly over \\(\\mathcal{C}_R\\), then the draws \\((u_1/u_0^r, \\ldots, u_d/u_0^r)\\) are from the normalized density \\(f\\). Rejection sampling is used to obtain uniform draws over \\(\\mathcal{C}_r\\) under some conditions on the density and marginal moments. See the rust package vignette for technical details and examples. Like with other accept-reject algorithms, the acceptance rate of the proposal goes down with the dimension of the problem.\n\n\nExample 3.3 The ratio-of-uniform algorithm was used in Example 1.4 to generate draws from the posterior. We illustrate below the rust package with a user-specified prior and posterior. We fit a generalized Pareto distribution \\(Y \\sim \\mathsf{GP}(\\sigma, \\xi)\\) to exceedances above 10 millions krones to the danish fire insurance data, using a truncated maximal data information prior \\(p(\\sigma, \\xi) \\propto \\sigma^{-1}\\exp(-\\xi+1)\\mathrm{I}(\\xi &gt; -1)\\).\n\ndata(danish, package = \"evir\")\n# Extract threshold exceedances\nexc &lt;- danish[danish &gt; 10] - 10\n# Create a function for the log prior\nlogmdiprior &lt;- function(par, ...){\n  if(isTRUE(any(par[1] &lt;= 0, par[2] &lt; -1))){\n    return(-Inf)\n  }\n  -log(par[1]) - par[2]\n}\n# Same for log likelihood, assuming independent data\nloglik_gp &lt;- function(par, data = exc, ...){\n  if(isTRUE(any(par[1] &lt;= 0, par[2] &lt; -1))){\n    return(-Inf)\n  }\n  sum(mev::dgp(x = data, scale = par[1], shape = par[2], log = TRUE))\n}\nlogpost &lt;- function(par, ...){\n  logmdiprior(par) + loglik_gp(par)\n}\n# Sampler using ratio-of-uniform method\nru_output &lt;- rust::ru(\n  logf = logpost,  # log posterior function\n  n = 10000, # number of posterior draws\n  d = 2, # dimension of the parameter vector\n  init = mev::fit.gpd(danish, thresh = 10)$par,\n  lower = c(0, -1))\n## Acceptance rate \n# ru_output$pa\n## Posterior samples\npostsamp &lt;- ru_output$sim_vals\n\nEven without modification, the acceptance rate is 52%, which is quite efficient in the context. The generalized Pareto approximation suggests a very heavy tail: values of \\(\\xi \\geq 1\\) correspond to distributions with infinite first moment, and those with \\(\\xi \\geq 1/2\\) to infinite variance.\n\n\n\n\n\nFigure 3.3: Scatterplot of posterior samples from the generalized Pareto model applied to Danish fire insurance losses above 10 millions krones, with maximal data information prior (left) and posterior predictive density on log scale (right)."
  },
  {
    "objectID": "mcmc.html#markov-chain-monte-carlo",
    "href": "mcmc.html#markov-chain-monte-carlo",
    "title": "3  Simulation-based inference",
    "section": "3.2 Markov chain Monte Carlo",
    "text": "3.2 Markov chain Monte Carlo\nPlain ordinary Monte Carlo is great, but few algorithms are generic enough to be useful in complex high-dimensional problems. Instead, we will construct a Markov chain with a given invariant distribution corresponding to the posterior. Markov chain Monte Carlo methods generate correlated draws that will target the posterior under suitable conditions.3\nBefore going forward with algorithms for sampling, we introduce some terminology that should be familiar to people with a background in time series analysis.\n\nDefinition 3.1 (Stationarity and Markov property) A stochastic (i.e., random) process is (weakly) stationary if the distribution of \\(\\{X_1, \\ldots, X_t\\}\\) is the same as that of \\(\\{X_{n+1}, \\ldots X_{t+n}\\}\\) for any value of \\(n\\) and given \\(t\\).\nIt is Markov if it satisfies the Markov property: given the current state of the chain, the future only depends on the current state and not on the past.\n\n\nExample 3.4 Consider a first-order autoregressive process, or \\(\\mathsf{AR}(1)\\), of the form\n\\[Y_t = \\mu + \\phi(Y_{t-1} - \\mu) + \\varepsilon_t,\\] where \\(\\phi\\) is the lag-one correlation, \\(\\mu\\) the global mean and \\(\\varepsilon_t\\) is an iid innovation with mean zero and variance \\(\\sigma^2\\). If \\(|\\phi| &lt; 1\\), the process is stationary, and the variance does not increase with \\(t\\). If innovations are Gaussian, we have \\[Y_t \\mid Y_{t-1}=y_{t-1} \\sim \\mathsf{Norm}\\{\\mu(1-\\phi)+ \\phi y_{t-1}, \\sigma^2\\}.\\]\nThe \\(\\mathsf{AR}(1)\\) stationarity process \\(Y_t\\), marginally, has mean \\(\\mu\\) and unconditional variance \\(\\sigma^2/(1-\\phi^2)\\). The \\(\\mathsf{AR}(1)\\) process is first-order Markov since the conditional distribution \\(p(Y_t \\mid Y_{t-1}, \\ldots, Y_{t-p})\\) equals \\(p(Y_t \\mid Y_{t-1})\\).\n\nAutoregressive processes are not the only ones we can consider, although their simplicity lends itself to analytic calculations. More generally, for a correlated sequence, the variance of the stationary distribution is \\[\\begin{align*}\n\\mathsf{Va}(Y_t) + 2 \\sum_{k=1}^\\infty \\mathsf{Co}(Y_t, Y_{t-k})\n\\end{align*}\\]\n\nProposition 3.3 (Effective sample size) Intuitively, a sample of correlated observations carries less information than an independent sample of draws. If we want to compute sample averages \\(\\overline{Y}_T=(Y_1+ \\cdots + Y_T)/T\\), the variance will be \\[\\begin{align*}\n\\mathsf{Va}\\left(\\overline{Y}_T\\right) = \\frac{1}{T}\\sum_{t=1}^T \\mathsf{Va}(Y_t) + \\frac{2}{T} \\sum_{t=1}^{T-1}\\sum_{s = t+1}^T \\mathsf{Co}(Y_t, Y_s).\n\\end{align*}\\]\nIn the independent case, the covariance is zero so we get the sum of variances. If the process is stationary, the covariances at lag \\(k\\) are the same regardless of the time index and the variance is some constant, say \\(\\sigma^2\\); this allows us to simplify calculations, \\[\\begin{align*}\n\\mathsf{Va}(\\overline{Y}_T) = \\sigma^2 \\left\\{ 1 + \\frac{2}{T}\\sum_{t=1}^{T-1} (T-t) \\mathsf{Cor}(Y_{T-k}, Y_{T})\\right\\}.\n\\end{align*}\\] Denote the lag-\\(k\\) autocorrelation \\(\\mathsf{Cor}(Y_{t}, Y_{t+k})\\) by \\(\\gamma_k\\). Under technical conditions4, a central limit theorem applies and we get an asymptotic variance for the mean of \\[\\begin{align*}\n\\lim_{T \\to \\infty} T\\mathsf{Va}\\left(\\overline{Y}_T\\right) = \\sigma^2 \\left\\{1+2\\sum_{t=1}^\\infty \\gamma_t\\right\\}.\n\\end{align*}\\] This statement holds only if we start with draws from the stationary distribution, otherwise bets are off.\nWe need the effective sample size of our Monte Carlo averages based on a Markov chain of length \\(B\\) to be sufficient for the estimates to be meaningful. The effective sample size is, loosely speaking, the equivalent number of observations if the marginal posterior draws where independent and more formally \\[\n\\mathsf{ESS} = \\frac{B}{\\left\\{1+2\\sum_{t=1}^\\infty \\gamma_t\\right\\}}\n\\tag{3.1}\\] where \\(\\gamma_t\\) is the lag \\(t\\) correlation. The relative effective sample size is simply the fraction of the effective sample size over the Monte Carlo number of replications: small values of \\(\\mathsf{ESS}/B\\) indicate pathological or inefficient samplers. If the ratio is larger than one, it indicates the sample is superefficient (as it generates negatively correlated draws).\nIn practice, we replace the unknown autocorrelations by sample estimates and truncate the series in Equation 3.1 at the point where they become negligible — typically when the consecutive sum of two consecutive becomes negative; see Section 1.4 of the Stan manual or Section 1.10.2 of Geyer (2011) for details.\n\n\nExample 3.5 The lag-\\(k\\) correlation of the stationary autoregressive process of order 1 is \\(\\phi^k\\), so summing the series gives an asymptotic variance of \\(\\sigma^2(1+\\phi)/(1-\\phi)\\). We can constrast that to the variance of the stationary distribution for an independent sample, which is \\(\\sigma^2/(1-\\phi^2)\\). The price to pay for having correlated samples is inefficiency: the higher the autocorrelation, the larger the variability of our mean estimators.\n\n\n\n\n\nFigure 3.4: Scaled asymptotic variance of the sample mean for a stationary autoregressive first-order process with unit variance (full line) and a corresponding sample of independent observations with the same marginal variance (dashed line). The right panel gives the ratio of variances for positive correlation coefficients.\n\n\n\n\nWe can see from Figure 3.4 that, when the autocorrelation is positive (as will be the cause in all applications of interest), we will suffer from variance inflation. To get the same uncertainty estimates for the mean with an \\(\\mathsf{AR}(1)\\) process with \\(\\phi \\approx 0.75\\) than with an iid sample, we would need nine times as many observations: this is the prize to pay.\n\n\n3.2.1 Estimating uncertainty of point estimators with Markov chains\nWith a simple random sample containing independent and identically distributed observations, the standard error of the sample mean is \\(\\sigma/\\sqrt{n}\\) and we can use the empirical standard deviation \\(\\widehat{\\sigma}\\) to estimate the first term. For Markov chains, the correlation prevents us from using this approach. The output of thecoda package are based on fitting a high order autoregressive process to the Markov chain and using the formula of the unconditional variance of the \\(\\mathsf{AR}(p)\\) to obtain the central limit theorem variance. An alternative method recommended by Geyer (2011) and implemented in his R package mcmc, is to segment the time series into batch, compute the means of each non-overlapping segment and use this standard deviation with suitable rescaling to get the central limit variance for the posterior mean. Figure 3.5 illustrate the method of batch means.\n\nBreak the chain of length \\(B\\) (after burn in) in \\(K\\) blocks of size \\(\\approx K/B\\).\nCompute the sample mean of each segment. These values form a Markov chain and should be approximately uncorrelated.\nCompute the standard deviation of the segments mean. Rescale by \\(K^{-1/2}\\) to get standard error of the global mean.\n\nWhy does the approach work? If the chain samples from the stationary distribution, all samples have the same mean. If we partition the sample into long enough, the sample mean of each blocks should be roughly independent (otherwise we could remove an overlapping portion). We can then compute the empirical standard deviation of the estimators. We can then compute the overall mean and use a scaling argument to relate the variability of the global estimator with the variability of the means of the smaller blocks.\n\n\n\n\n\nFigure 3.5: Calculation of the standard error of the posterior mean using the batch method.\n\n\n\n\nWhen can we use output from a Markov chain in place of independent Monte Carlo draws? The assumptions laid out in the ergodic theorem are that the chain is irreducible and acyclic, ensuring that the chain has a unique stationary distribution. The ergodic theorem is a result about convergence of averages.\nTo make sense of these concepts, we consider a discrete Markov chain over the integers \\(1, 2, 3\\). A discrete-time stochastic process is a random sequences whose elements are part of some set, the state space, here the integers. We can encode the probability of moving from one state to the next via a transition matrix, whose rows contain the probabilities of moving from one state to the next and thus sum to one. We can run a Markov chain by sampling an initial state \\(X_0\\) at random from \\(\\{1, \\ldots, 5\\}\\) and then consider the transitions from the conditional distribution, sampling \\(p(X_t \\mid X_{t-1})\\). Because of the Markov property, the history of the chain does not matter: we only need to read the value \\(i=X_{t-1}\\) of the state and pick the \\(i\\)th row of \\(P_3\\) to know the probability of the different moves from the current state.\nIrreducible means that the chain can move from anywhere to anywhere, so it doesn’t get stuck in part of the space forever. A transition matrix such as \\(P_1\\) below describes a reducible Markov chain, because once you get into state \\(2\\) or \\(3\\), you won’t escape. With reducible chains, the stationary distribution need not be unique, and so the target would depend on the starting values.\nCyclical chains loop around and visit periodically a state: \\(P_2\\) is an instance of transition matrix describing a chain that cycles from \\(1\\) to \\(3\\), \\(3\\) to \\(2\\) and \\(2\\) to \\(1\\) every three iteration. An acyclic chain is needed for convergence of marginals.\n\\[\nP_1 = \\begin{pmatrix}\n0.5 & 0.3 & 0.2 \\\\\n0 & 0.4 & 0.6 \\\\\n0 & 0.5 & 0.5\n\\end{pmatrix},\n\\qquad\nP_2 = \\begin{pmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}.\n\\]\nIf a chain is irreducible and aperiodic, it has a unique stationary distribution and the limiting distribution of the Markov chain will converge there. For example, we consider a transition \\(P_3\\) on \\(1, \\ldots, 5\\) defined as \\[\nP_3 = \\begin{pmatrix}\n\\frac{2}{3} & \\frac{1}{3} &  0 & 0 & 0 \\\\\n\\frac{1}{6} & \\frac{2}{3} & \\frac{1}{6} & 0 & 0 \\\\\n0 & \\frac{1}{6} & \\frac{2}{3} & \\frac{1}{6} & 0 \\\\\n0 & 0 & \\frac{1}{6} & \\frac{2}{3} & \\frac{1}{6} \\\\\n0 & 0 & 0 &  \\frac{1}{3}  & \\frac{2}{3} \\\\\n\\end{pmatrix}\n\\] The stationary distribution is the value of the row vector \\(\\boldsymbol{p}\\), such that \\(\\boldsymbol{p} = \\boldsymbol{p}\\mathbf{P}\\) for transition matrix \\(\\mathbf{P}\\): we get \\(\\boldsymbol{p}_1=(0, 5/11, 6/11)\\) for \\(P_1\\), \\((1/3, 1/3, 1/3)\\) for \\(P_2\\) and \\((1,2,2,2,1)/8\\) for \\(P_3\\).\nFigure 3.6 shows the path of the walk and the empirical proportion of the time spent in each state, as time progress. Since the Markov chain has a unique stationary distribution, we expect these to converge to it.\n\n\n\n\n\nFigure 3.6: Discrete Markov chain on integers from 1 to 5, with transition matrix \\(P_3\\), with traceplot of 1000 first iterations (left) and running mean plots of sample proportion of each state visited per 100 iterations (right)."
  },
  {
    "objectID": "mcmc.html#markov-chain-monte-carlo-algorithms",
    "href": "mcmc.html#markov-chain-monte-carlo-algorithms",
    "title": "3  Simulation-based inference",
    "section": "3.3 Markov chain Monte Carlo algorithms",
    "text": "3.3 Markov chain Monte Carlo algorithms\nThe Markov chain Monte Carlo revolution in the 1990s made Bayesian inference mainstream by allowing inference for models when only approximations were permitted, and coincided with a time at which computers became more widely available. The idea is to draw correlated samples from a posterior via Markov chains, constructed to have the posterior as invariant stationary distribution.\n\n3.3.1 Metropolis–Hastings algorithm\nNamed after Metropolis et al. (1953), Hastings (1970), its relevance took a long time to gain traction in the statistical community. The idea of the Metropolis–Hastings algorithm is to construct a Markov chain targeting a distribution \\(p(\\cdot)\\).\n\nProposition 3.4 (Metropolis–Hastings algorithm) We consider from a density function \\(p(\\boldsymbol{\\theta})\\), known up to a normalizing factor not depending on \\(\\boldsymbol{\\theta}\\). We use a (conditional) proposal density \\(q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}^*)\\) which has non-zero probability over the support of \\(p(\\cdot)\\), as transition kernel to generate proposals.\nThe Metropolis–Hastings build a Markov chain starting from an initial value \\(\\boldsymbol{\\theta}_0\\):\n\ndraw a proposal value \\(\\boldsymbol{\\theta}_t^{\\star} \\sim q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}_{t-1})\\).\nCompute the acceptance ratio \\[\nR = \\frac{p(\\boldsymbol{\\theta}_t^{\\star})}{p(\\boldsymbol{\\theta}_{t-1})}\\frac{q(\\boldsymbol{\\theta}_{t-1} \\mid \\boldsymbol{\\theta}_t^{\\star} )}{q(\\boldsymbol{\\theta}_t^{\\star} \\mid \\boldsymbol{\\theta}_{t-1})}\n\\tag{3.2}\\]\nWith probability \\(\\min\\{R, 1\\}\\), accept the proposal and set \\(\\boldsymbol{\\theta}_t \\gets \\boldsymbol{\\theta}_t^{\\star}\\), otherwise set the value to the previous state, \\(\\boldsymbol{\\theta}_t \\gets \\boldsymbol{\\theta}_{t-1}\\).\n\n\nThe Metropolis–Hastings algorithm generates samples from the posterior \\(p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\) if the Markov chain it defines is reversible: we say it satisfies the detailed balance condition when the density of \\(\\boldsymbol{\\theta}_{t+1} \\mid \\boldsymbol{\\theta}_{t}\\), say \\(f(\\boldsymbol{\\theta}_{t+1} \\mid \\boldsymbol{\\theta}_{t})\\). Detailed balance means \\[\\begin{align*}\nf(\\boldsymbol{\\theta}_{t+1} \\mid \\boldsymbol{\\theta}_{t})p(\\boldsymbol{\\theta}_{t} \\mid \\boldsymbol{y}) = f(\\boldsymbol{\\theta}_{t} \\mid \\boldsymbol{\\theta}_{t+1})p(\\boldsymbol{\\theta}_{t+1} \\mid \\boldsymbol{y})\n\\end{align*}\\] This guarantees that, if \\(\\boldsymbol{\\theta}_{t}\\) is drawn from the posterior, then the left hand side is the joint density of \\((\\boldsymbol{\\theta}_{t}, \\boldsymbol{\\theta}_{t+1})\\) and the marginal distribution obtained by integrating over \\(\\boldsymbol{\\theta}_{t}\\), \\[\\begin{align*}\n&\\int f(\\boldsymbol{\\theta}_{t+1} \\mid \\boldsymbol{\\theta}_{t})p(\\boldsymbol{\\theta}_{t} \\mid \\boldsymbol{y})\\mathrm{d} \\boldsymbol{\\theta}_{t}\n\\\\&\\quad = \\int f(\\boldsymbol{\\theta}_{t} \\mid \\boldsymbol{\\theta}_{t+1})p(\\boldsymbol{\\theta}_{t+1} \\mid \\boldsymbol{y})\\mathrm{d} \\boldsymbol{\\theta}_{t}\n\\\\&\\quad= p(\\boldsymbol{\\theta}_{t+1} \\mid \\boldsymbol{y})\n\\end{align*}\\] and any draw from the posterior will generate a new realization from the posterior. We also ensure that, provided the starting value as non-zero probability under the posterior, the chain will converge to the stationarity distribution (albeit perhaps slowly).\n\nRemark (Interpretation of the algorithm). If \\(R&gt;1\\), the proposal has higher density and we always accept the move. If the ratio is less than one, the proposal is in a lower probability region, we accept the move with probability \\(R\\) and set \\(\\boldsymbol{\\theta}_{t}=\\boldsymbol{\\theta}^{\\star}_t\\); if we reject, the Markov chain stays at the current value, which induces autocorrelation. Since the acceptance probability depends only on the density through ratios, we can work with unnormalized density functions and this is what allows us, if our proposal density is the (marginal) posterior of the parameter, to obtain approximate posterior samples without having to compute the marginal likelihood.\n\n\nRemark (Blank run). To check that the algorithm is well-defined, we can remove the log likelihood component and run the algorithm: if it is correct, the resulting draws should be drawn from the prior provided the latter is proper (Green 2001, 55).\n\n\nRemark (Symmetric proposals). Suppose we generate a candidate sample \\(\\boldsymbol{\\theta}_t^{\\star}\\) from a symmetric distribution \\(q(\\cdot \\mid \\cdot)\\) centered at \\(\\boldsymbol{\\theta}_{t-1}\\), such as the random walk \\(\\boldsymbol{\\theta}_t^{\\star} =\\boldsymbol{\\theta}_{t-1}+ Z\\) where \\(Z\\) has a symmetric distribution. Then, the proposal density ratio cancels so need not be computed in the Metropolis ratio of Equation 3.2.\n\n\nRemark (Calculations). In practice, we compute the log of the acceptance ratio, \\(\\ln R\\), to avoid numerical overflow. If our target is log posterior density, we have \\[\n\\ln \\left\\{\\frac{p(\\boldsymbol{\\theta}_t^{\\star})}{p(\\boldsymbol{\\theta}_{t-1})}\\right\\} = \\ell(\\boldsymbol{\\theta}_t^{\\star}) + \\ln p(\\boldsymbol{\\theta}_t^{\\star}) - \\ell(\\boldsymbol{\\theta}_{t-1}) - \\ln p(\\boldsymbol{\\theta}_{t-1})\n\\] and we proceed likewise for the log of the ratio of transition kernels. We then compare the value of \\(\\ln R\\) (if less than zero) to \\(\\log(U)\\), where \\(U \\sim \\mathsf{U}(0,1)\\). We accept the move if \\(\\ln(R) &gt;\\log(U)\\) and keep the previous value otherwise.\n\n\nExample 3.6 Consider again the Upworthy data from Example 2.6. We model the Poisson rates \\(\\lambda_i\\) \\((i=1,2),\\) this time with the usual Poisson regression parametrization in terms of log rate for the baseline , \\(\\log(\\lambda_2) = \\beta\\), and log odds rates \\(\\kappa = \\log(\\lambda_1) - \\log(\\lambda_2)\\). Our model is \\[\\begin{align*}\nY_{i} &\\sim \\mathsf{Po}(n_i\\lambda_i), \\qquad (i=1,2)\\\\\n\\lambda_1 &= \\exp(\\beta + \\kappa) \\\\\n\\lambda_2 &= \\exp(\\beta) \\\\\n\\beta & \\sim \\mathsf{Norm}(\\log 0.01, 1.5) \\\\\n\\kappa &\\sim \\mathsf{Norm}(0, 1)\n\\end{align*}\\] There are two parameters in the model, which can be updated in turn or jointly.\n\ndata(upworthy_question, package = \"hecbayes\")\n# Compute sufficient statistics\ndata &lt;- upworthy_question |&gt;\n  dplyr::group_by(question) |&gt;\n  dplyr::summarize(ntot = sum(impressions),\n                   y = sum(clicks))\n# Code log posterior as sum of log likelihood and log prior\nloglik &lt;- function(par, counts = data$y, offset = data$ntot, ...){\n  lambda &lt;- exp(c(par[1] + log(offset[1]), par[1] + par[2] + log(offset[2])))\n sum(dpois(x = counts, lambda = lambda, log = TRUE))\n}\nlogprior &lt;- function(par, ...){\n  dnorm(x = par[1], mean = log(0.01), sd = 1.5, log = TRUE) +\n    dnorm(x = par[2], log = TRUE)\n}\nlogpost &lt;- function(par, ...){\n  loglik(par, ...) + logprior(par, ...)\n}\n# Compute maximum a posteriori (MAP)\nmap &lt;- optim(\n  par = c(-4, 0.07),\n  fn = logpost,\n  control = list(fnscale = -1),\n  offset = data$ntot,\n  counts = data$y,\n  hessian = TRUE)\n# Use MAP as starting value\ncur &lt;- map$par\n# Compute logpost_cur - we can keep track of this to reduce calculations\nlogpost_cur &lt;- logpost(cur)\n# Proposal covariance\ncov_map &lt;- -2*solve(map$hessian)\nchol &lt;- chol(cov_map)\n\nset.seed(80601)\nniter &lt;- 1e4L\nchain &lt;- matrix(0, nrow = niter, ncol = 2L)\ncolnames(chain) &lt;- c(\"beta\",\"kappa\")\nnaccept &lt;- 0L\nfor(i in seq_len(niter)){\n  # Multivariate normal proposal - symmetric random walk\n  prop &lt;- chol %*% rnorm(n = 2) + cur\n  logpost_prop &lt;- logpost(prop)\n  # Compute acceptance ratio (no q because the ratio is 1)\n  logR &lt;- logpost_prop - logpost_cur\n  if(logR &gt; -rexp(1)){\n    cur &lt;- prop\n    logpost_cur &lt;- logpost_prop\n    naccept &lt;- naccept + 1L\n  }\n  chain[i,] &lt;- cur\n}\n# Posterior summaries\nsummary(coda::as.mcmc(chain))\n\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean       SD  Naive SE Time-series SE\nbeta  -4.51268 0.001697 1.697e-05      6.176e-05\nkappa  0.07075 0.002033 2.033e-05      9.741e-05\n\n2. Quantiles for each variable:\n\n          2.5%      25%      50%      75%    97.5%\nbeta  -4.51591 -4.51385 -4.51273 -4.51154 -4.50929\nkappa  0.06673  0.06933  0.07077  0.07212  0.07463\n\n# Computing standard errors using batch means\nsqrt(diag(mcmc::olbm(chain, batch.length = niter/40)))\n\n[1] 5.717097e-05 8.220816e-05\n\n\nThe acceptance rate of the algorithm is 35.1% and the posterior means are \\(\\beta =-4.51\\) and \\(\\kappa =0.07\\).\n\n\n\n\n\nFigure 3.7: Traceplots of Markov chain of log rate and log odds rate for the Metropolis–Hastings sampler applied to the Upworthy question data.\n\n\n\n\nFigure 3.8 shows the posterior samples, which are very nearly bivariate Gaussian. The parametrization in terms of log odds ratio induces strong negative dependence, so if we were to sample \\(\\kappa\\), then \\(\\beta\\), we would have much larger inefficiency and slower exploration. Instead, the code used a bivariate Gaussian random walk proposal whose covariance matrix was taken as a multiple of the inverse of the negative hessian (equivalently, to the observed information matrix of the log posterior), evaluated at of the maximum a posteriori. This Gaussian approximation is called Laplace approximation: it is advisable to reparametrize the model so that the distribution is nearly symmetric, so that the approximation is good. In this example, because of the large sample, the Gaussian approximation implied by Bernstein–von Mises’ theorem is excellent.\n\n\n\n\n\nFigure 3.8: Scatterplot of posterior draws (left) and marginal density plot of log odds rate (right).\n\n\n\n\n\nThe quality of the mixing of the chain (autocorrelation), depends on the proposal variance, which can obtain by trial and error. Trace plots Figure 3.7 show the values of the chain as a function of iteration number. If our algorithm works well, we expect the proposals to center around the posterior mode and resemble a fat hairy caterpillar. If the variance is too small, the acceptance rate will increase but most steps will be small. If the variance of the proposal is too high, the acceptance rate will decrease (as many proposal moves will have much lower posterior), so the chain will get stuck for long periods of time. This is Goldilock’s principle, as illustrated in Figure 3.9.\n\n\n\n\n\nFigure 3.9: Example of traceplot with proposal variance that is too small (top), adequate (middle) and too large (bottom).\n\n\n\n\nOne way to calibrate is to track the acceptance rate of the proposals: for the three chains in Figure 3.9, these are 0.932, 0.33, 0.12. In one-dimensional toy problems with Gaussian distributions, an acceptance rate of 0.44 is optimal, and this ratio decreases to 0.234 when \\(D \\geq 2\\) Sherlock (2013). This need not generalize to other settings and depends on the context. Optimal rate for alternative algorithms, such as Metropolis-adjusted Langevin algorithm, are typically higher.\nWe can tune the variance of the global proposal (Andrieu and Thoms 2008) to improve the mixing of the chains at approximate stationarity. This is done by increasing (decreasing) the variance if the historical acceptance rate is too high (respectively low) during the burn in period, and reinitializing after any change with an acceptance target of \\(0.44\\). We stop adapting to ensure convergence to the posterior after a suitable number of initial iterations. Adaptive MCMC methods use an initial warm up period to find good proposals: we can consider a block of length \\(L\\), compute the acceptance rate, multiply the variance by a scaling factor and run the chain a little longer. We only keep samples obtained after the adaptation phase.\nWe can also plot the autocorrelation of the entries of the chain as a function of lags, a display known as correlogram in the time series literature but colloquially referred to as autocorrelation function (acf). The higher the autocorrelation, the more variance inflation one has and the longer the number of steps before two draws are treated as independent. Figure 3.10 shows the effect of the proposal variance on the correlation for the three chains. Practitioners designing very inefficient Markov chain Monte Carlo algorithms often thin their series: that is, they keep only every \\(k\\) iteration. This is not recommended practice unless storage is an issue and usually points towards inefficient sampling algorithms.\n\n\n\n\n\nFigure 3.10: Correlogram for the three Markov chains.\n\n\n\n\n\n\nRemark (Independence Metropolis–Hastings). If the proposal density \\(q(\\cdot)\\) does not depend on the current state \\(\\boldsymbol{\\theta}_{t-1}\\), the algorithm is termed independence. To maximize acceptance, we could design a candidate distribution whose mode is at the maximum a posteriori value. To efficiently explore the state space, we need to place enough density in all regions, for example by taking a heavy-tailed distributions, so that we explore the full support. Such proposals can be however inefficient and fail when the distribution of interest is multimodal. The independence Metropolis–Hastings algorithm then resembles accept-reject. If the ratio \\(p(\\boldsymbol{\\theta})/q(\\boldsymbol{\\theta})\\) is bounded above by \\(C \\geq 1\\), then we can make comparisons with rejection sampling. Lemma 7.9 of Robert and Casella (2004) shows that the probability of acceptance of a move for the Markov chain is at least \\(1/C\\), which is larger than the accept-reject.\n\nIn models with multiple parameter, we can use Metropolis–Hastings algorithm to update every parameter in turn, fixing the value of the others, rather than update them in block. The reason behind this pragmatic choice is that, as for ordinary Monte Carlo sampling, the acceptance rate goes down sharply with the dimension of the vector. Updating parameters one at a time can lead to higher acceptance rates, but slower exploration as a result of the correlation between parameters.\nIf we can factorize the log posterior, then some updates may not depend on all parameters: in a hierarchical model, hyperpriors parameter only appear through priors, etc. This can reduce computational costs.\n\nProposition 3.5 (Parameter transformation) If a parameter is bounded in the interval \\((a,b)\\), where \\(-\\infty \\leq a &lt; b \\leq \\infty\\), we can consider a bijective transformation \\(\\vartheta \\equiv t(\\theta): (a,b) \\to \\mathbb{R}\\) with differentiable inverse. The log density of the transformed variable, assuming it exists, is \\[\\begin{align*}\nf_\\vartheta(\\vartheta) = f_{\\theta}\\{t^{-1}(\\vartheta)\\} \\left| \\frac{\\mathrm{d}}{\\mathrm{d} \\vartheta} t^{-1}(\\vartheta)\\right|\n\\end{align*}\\] For example, we can use of the following transformations for finite \\(a, b\\) in the software:\n\nif \\(\\theta \\in (a, \\infty)\\) (lower bound only), then \\(\\vartheta = \\log(\\theta-a)\\) and \\(f_{\\vartheta}(\\vartheta)=f_{\\theta}\\{\\exp(\\vartheta) + a\\}\\cdot \\exp(\\vartheta)\\)\nif \\(\\theta \\in (-\\infty, b)\\) (upper bound only), then \\(\\vartheta = \\log(b-\\theta)\\) and \\(f_{\\vartheta}(\\vartheta)=f_{\\theta}\\{b-\\exp(\\vartheta)\\}\\cdot \\exp(\\vartheta)\\)\nif \\(\\theta \\in (a, b)\\) (both lower and upper bound), then \\(\\vartheta = \\mathrm{logit}\\{(\\theta-a)/(b-a)\\}\\) and \\[\\begin{align*}\nf_{\\vartheta}(\\vartheta)&=f_{\\theta}\\{a+(b-a) \\mathrm{expit}(\\vartheta)\\} (b-a)\\\\&\\quad \\times \\mathrm{expit}(\\vartheta)\\{1-\\mathrm{expit}(\\vartheta)\\}\n\\end{align*}\\]\n\nTo guarantee that our proposals fall in the support of \\(\\theta\\), we can thus run a symmetric random walk proposal on the transformed scale by drawing \\(\\vartheta_{t}^{\\star} \\sim \\vartheta_{t-1}+\\tau Z\\) where \\(Z\\sim\\mathsf{Norm}(0, 1)\\). Due to the transformation, the kernel ratio now contains the Jacobian.\n\n\nProposition 3.6 (Truncated proposals) As an alternative, if we are dealing with parameters that are restricted in \\([a,b]\\), we can simulate using a random walk but with truncated Gaussian steps, taking \\(\\theta^{\\star}_{t} \\sim \\mathsf{TruncNorm}(\\vartheta_{t-1}, \\tau^2, a, b).\\) The benefits of using the truncated proposal becomes more apparent when we move to more advanced proposals whose mean and variance depends on the gradient and or the hessian of the underlying unnormalized log posterior, as the mean can be lower than \\(a\\) or larger than \\(b\\): this would garantee zero acceptance with regular Gaussian random walk. The TruncatedNormal package can be used to efficiently evaluate such instances using results from Botev and L’Écuyer (2017) even when the truncation bounds are far from the mode. the normalizing constant of the truncated Gaussian in the denominator of the density is a function of the location and scale parameters: if these depend on the current value of \\(\\boldsymbol{\\theta}_{t-1}\\), as is the case for a random walk, we need to keep these terms as part of the Metropolis ratio. The mean and standard deviation of the truncated Gaussian are not equal to the parameters \\(\\mu\\) (which corresponds to the mode, provided \\(a &lt; \\mu &lt; b\\)) and \\(\\sigma\\).\n\n\nProposition 3.7 (Efficient proposals) Rather than simply build a random walk, we can exploit the geometry of the posterior using the gradient, via Metropolis-ajusted Langevin algorithm (MALA), or using local quadratic approximations of the target.\nLet \\(p(\\theta)\\) denote the conditional (unnormalized) log posterior for a scalar parameter \\(\\theta \\in (a, b)\\). We considering a Taylor series expansion of \\(p(\\cdot)\\) around the current parameter value \\(\\theta_{t-1}\\), \\[\\begin{align*}\np(\\theta) \\approx p(\\theta_{t-1}) + p'(\\theta_{t-1})(\\theta - \\theta_{t-1}) + \\frac{1}{2} p''(\\theta_{t-1})(\\theta - \\theta_{t-1})^2\n\\end{align*}\\] plus remainder, which suggests a Gaussian approximation with mean \\(\\mu_{t-1} = \\theta_{t-1} - f'(\\theta_{t-1})/f''(\\theta_{t-1})\\) and precision \\(\\tau^{-2} = -f''(\\theta_{t-1})\\). We can use truncated Gaussian distribution on \\((a, b)\\) with mean \\(\\mu\\) and standard deviation \\(\\tau\\), denoted \\(\\mathsf{TruncNorm}(\\mu, \\tau, a, b)\\) with corresponding density function \\(q(\\cdot; \\mu, \\tau, a, b)\\). The Metropolis acceptance ratio for a proposal \\(\\theta^{\\star}_{t} \\sim \\mathsf{TruncNorm}(\\mu_{t-1}, \\tau_{t-1}, a, b)\\) is \\[\\begin{align*}\n\\alpha = \\frac{p(\\theta^{\\star}_{t})}{p(\\theta_{t-1})} \\frac{ q(\\theta_{t-1} \\mid \\mu_{t}^{\\star}, \\tau_{t}^{\\star}, a, b)}{q(\\theta^{\\star}_{t} \\mid \\mu_{t-1}, \\tau_{t-1}, a, b)}\n\\end{align*}\\] and we set \\(\\theta^{(t+1)} = \\theta^{\\star}_{t}\\) with probability \\(\\min\\{1, r\\}\\) and \\(\\theta^{(t+1)} = \\theta_{t-1}\\) otherwise. To evaluate the ratio of truncated Gaussian densities \\(q(\\cdot; \\mu, \\tau, a, b)\\), we need to compute the Taylor approximation from the current parameter value, but also the reverse move from the proposal \\(\\theta^{\\star}_{t}\\). Another option is to modify the move dictated by the rescaled gradient by taking instead \\[\\mu_{t-1} = \\theta_{t-1} - \\eta f'(\\theta_{t-1})/f''(\\theta_{t-1}).\\] The proposal includes an additional learning rate parameter, \\(\\eta \\leq 1\\), whose role is to prevent oscillations of the quadratic approximation, as in a Newton–Raphson algorithm. Relative to a random walk Metropolis–Hastings, the proposal automatically adjusts to the local geometry of the target, which guarantees a higher acceptance rate and lower autocorrelation for the Markov chain despite the higher evaluation costs. The proposal requires that both \\(f''(\\theta_{t-1})\\) and \\(f''(\\theta^{\\star}_{t})\\) be negative since the variance is \\(-1/f''(\\theta)\\): this shouldn’t be problematic in the vicinity of the mode. Otherwise, one could use a global scaling derived from the hessian at the mode.\nThe simpler Metropolis-adjusted Langevin algorithm is equivalent to using a Gaussian random walk where the proposal has mean \\(\\boldsymbol{\\theta}_{t-1} + \\mathbf{A}\\eta \\nabla \\log p(\\boldsymbol{\\theta}_{t-1}; \\boldsymbol{y})\\) and variance \\(\\tau^2\\mathbf{A}\\), for some mass matrix \\(\\mathbf{A}\\) and learning rate \\(\\eta &lt; 1\\). Taking \\(\\mathbf{A}\\) as the identity matrix, which assumes the parameters are isotropic (same variance, uncorrelated) is the default choice although seldom far from optimal.\nFor MALA to work well, we need both to start near stationarity, to ensure that the gradient is relatively small and to prevent oscillations. One can dampen the size of the step initially if needed to avoid overshooting. The proposal variance, the other tuning parameter, is critical to the success of the algorithm. The usual target for the variance is one that gives an acceptance rate of roughly 0.574. These more efficient methods require additional calculations of the gradient and Hessian, either numerically or analytically. Depending on the situation and the computational costs of such calculations, the additional overhead may not be worth it.\n\n\nExample 3.7 We revisit the Upworthy data, this time modelling each individual headline as a separate observation. We view \\(n=\\)nimpression as the sample size of a binomial distribution and nclick as the number of successes. Since the number of trials is large, the sample average nclick/nimpression, denoted \\(y\\) in the sequel, is approximately Gaussian. We assume that each story has a similar population rate and capture the heterogeneity inherent to each news story by treating each mean as a sample. The variance of the sample average or click rate is proportional to \\(n^{-1}\\), where \\(n\\) is the number of impressions. To allow for underdispersion or overdispersion, we thus consider a Gaussian likelihood \\(Y_i \\sim \\mathsf{Norm}(\\mu, \\sigma^2/n_i)\\). We perform Bayesian inference for \\(\\mu, \\sigma\\) after assigning a truncated Gaussian prior for \\(\\mu \\sim \\mathsf{TruncNorm}(0.01, 0.1^2)\\) over \\([0,1]\\) and an penalized complexity prior for \\(\\sigma \\sim \\mathsf{Exp}(0.7)\\).\n\n\ndata(upworthy_question, package = \"hecbayes\")\n# Select data for a single question\nqdata &lt;- upworthy_question |&gt;\n  dplyr::filter(question == \"yes\") |&gt;\n  dplyr::mutate(y = clicks/impressions,\n                no = impressions)\n# Create functions with the same signature (...) for the algorithm\nlogpost &lt;- function(par, data, ...){\n  mu &lt;- par[1]; sigma &lt;- par[2]\n  no &lt;- data$no\n  y &lt;- data$y\n  if(isTRUE(any(sigma &lt;= 0, mu &lt; 0, mu &gt; 1))){\n    return(-Inf)\n  }\n  dnorm(x = mu, mean = 0.01, sd = 0.1, log = TRUE) +\n  dexp(sigma, rate = 0.7, log = TRUE) + \n  sum(dnorm(x = y, mean = mu, sd = sigma/sqrt(no), log = TRUE))\n}\n\nlogpost_grad &lt;- function(par, data, ...){\n   no &lt;- data$no\n  y &lt;- data$y\n  mu &lt;- par[1]; sigma &lt;- par[2]\n  c(sum(no*(y-mu))/sigma^2 -(mu - 0.01)/0.01,\n    -length(y)/sigma + sum(no*(y-mu)^2)/sigma^3 -0.7\n  )\n}\n\n# Starting values - MAP\nmap &lt;- optim(\n  par = c(mean(qdata$y), 0.5),\n  fn = function(x){-logpost(x, data = qdata)},\n  gr = function(x){-logpost_grad(x, data = qdata)},  \n  hessian = TRUE,\n  method = \"BFGS\")\n# Set initial parameter values\ncurr &lt;- map$par \n# Check convergence \nlogpost_grad(curr, data = qdata)\n\n[1] 7.650733e-03 5.575424e-05\n\n# Compute a mass matrix\nAmat &lt;- solve(map$hessian)\n# Cholesky root - for random number generation\ncholA &lt;- chol(Amat)\n\n\n\n# Create containers for MCMC\nB &lt;- 1e4L # number of iterations\nwarmup &lt;- 1e3L # adaptation period\nnpar &lt;- 2L # number of parameters\nprop_sd &lt;- rep(1, npar) #updating both parameters jointly\nchains &lt;- matrix(nrow = B, ncol = npar)\ndamping &lt;- 0.8 # learning rate\nacceptance &lt;- attempts &lt;- 0 \ncolnames(chains) &lt;- names(curr) &lt;- c(\"mu\",\"sigma\")\nprop_var &lt;- diag(prop_sd) %*% Amat %*% diag(prop_sd)\nfor(i in seq_len(B + warmup)){\n  ind &lt;- pmax(1, i - warmup)\n  # Compute the proposal mean for the Newton step\n  prop_mean &lt;- c(curr + damping * \n     Amat %*% logpost_grad(curr, data = qdata))\n  # prop &lt;- prop_sd * c(rnorm(npar) %*% cholA) + prop_mean\n  prop &lt;- c(mvtnorm::rmvnorm(\n    n = 1,\n    mean = prop_mean, \n    sigma = prop_var))\n  # Compute the reverse step\n  curr_mean &lt;- c(prop + damping * \n     Amat %*% logpost_grad(prop, data = qdata))\n  # log of ratio of bivariate Gaussian densities\n  logmh &lt;- mvtnorm::dmvnorm(\n    x = curr, mean = prop_mean, \n    sigma = prop_var, \n    log = TRUE) - \n    mvtnorm::dmvnorm(\n      x = prop, \n      mean = curr_mean, \n      sigma = prop_var, \n      log = TRUE) + \n  logpost(prop, data = qdata) - \n    logpost(curr, data = qdata)\n  if(logmh &gt; log(runif(1))){\n    curr &lt;- prop\n    acceptance &lt;- acceptance + 1L\n  }\n  attempts &lt;- attempts + 1L\n  # Save current value\n  chains[ind,] &lt;- curr\n  if(i %% 100 & i &lt; warmup){\n    out &lt;- hecbayes::adaptive(\n      attempts = attempts, \n      acceptance = acceptance, \n      sd.p = prop_sd,\n      target = 0.574)\n    prop_sd &lt;- out$sd\n    acceptance &lt;- out$acc\n    attempts &lt;- out$att\n    prop_var &lt;- diag(prop_sd) %*% Amat %*% diag(prop_sd)\n  }\n}\n\nMALA requires critically a good mass matrix, especially if the gradient is very large at the starting values (often the case when the starting value is far from the mode). Given the precision of the original observations, we did not need to modify anything to deal with the parameter constraints \\(0 \\leq \\mu \\leq 1\\) and \\(\\sigma&gt;0\\), outside of encoding them in the log posterior function.\nThe posterior mean for the standard deviation is 0.64, which suggests overdispersion."
  },
  {
    "objectID": "mcmc.html#gibbs-sampling",
    "href": "mcmc.html#gibbs-sampling",
    "title": "3  Simulation-based inference",
    "section": "3.4 Gibbs sampling",
    "text": "3.4 Gibbs sampling\nThe Gibbs sampling algorithm builds a Markov chain by iterating through a sequence of conditional distributions. Consider a model with \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta} \\subseteq \\mathbb{R}^p\\). We consider a single (or \\(m \\leq p\\) blocks of parameters), say \\(\\boldsymbol{\\theta}^{[j]}\\), such that, conditional on the remaining components of the parameter vector \\(\\boldsymbol{\\theta}^{-[j]}\\), the conditional posterior \\(p(\\boldsymbol{\\theta}^{[j]} \\mid \\boldsymbol{\\theta}^{-[j]}, \\boldsymbol{y})\\) is from a known distribution from which we can simulate draws\nAt iteration \\(t\\), we can update each block in turn: note that the \\(k\\)th block uses the partially updated state \\[\\begin{align*}\n\\boldsymbol{\\theta}^{-[k]\\star} = (\\boldsymbol{\\theta}_{t}^{[1]}, \\ldots, \\boldsymbol{\\theta}_{t}^{[k-1]},\\boldsymbol{\\theta}_{t-1}^{[k+1]}, \\boldsymbol{\\theta}_{t-1}^{[m]})\n\\end{align*}\\] which corresponds to the current value of the parameter vector after the updates. To check the validity of the Gibbs sampler, see the methods proposed in Geweke (2004).\nThe Gibbs sampling can be viewed as a special case of Metropolis–Hastings where the proposal distribution \\(q\\) is \\(p(\\boldsymbol{\\theta}^{[j]} \\mid \\boldsymbol{\\theta}^{-[j]\\star}, \\boldsymbol{y})\\). The particularity is that all proposals get accepted because the log posterior of the partial update, equals the proposal distribution, so \\[\\begin{align*}\nR = \\frac{p(\\boldsymbol{\\theta}_t^{[j]\\star} \\mid \\boldsymbol{\\theta}^{-[j]\\star}, \\boldsymbol{y})}{p(\\boldsymbol{\\theta}_{t-1}^{[j]\\star} \\mid \\boldsymbol{\\theta}^{-[j]\\star}, \\boldsymbol{y})}\\frac{p(\\boldsymbol{\\theta}_{t-1}^{[j]\\star} \\mid \\boldsymbol{\\theta}^{-[j]\\star}, \\boldsymbol{y})}{p(\\boldsymbol{\\theta}_t^{[j]\\star} \\mid \\boldsymbol{\\theta}^{-[j]\\star}, \\boldsymbol{y})}=1.\n\\end{align*}\\] Regardless of the order (systematic scan or random scan), the procedure remains valid. The Gibbs sampling is thus an automatic algorithm: we only need to derive the conditional posterior distributions of the parameters and run the sampler, and there are no tuning parameter involved. If the parameters are strongly correlated, the changes for each parameter will be incremental and this will lead to slow mixing and large autocorrelation, even if the values drawn are all different. Figure 3.11 shows 25 steps from a Gibbs algorithm for a bivariate target.\n\n\n\n\n\nFigure 3.11: Sampling trajectory for a bivariate target using Gibbs sampling.\n\n\n\n\n\nAs a toy illustration, we use Gibbs sampling to simulate data from a \\(d\\)-dimensional multivariate Gaussian target with mean \\(\\boldsymbol{\\mu}\\) and equicorrelation covariance matrix \\(\\mathbf{\\Sigma} = (1-\\rho)\\mathbf{I}_d + \\rho\\boldsymbol{1}_{d}\\boldsymbol{1}^\\top_d\\) with inverse \\[\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}=(1-\\rho)^{-1}\\left\\{\\mathbf{I}_d - \\rho \\mathbf{1}_d\\mathbf{1}_d/(1+(d-1)\\rho)\\right\\},\\] for known correlation coefficient \\(\\rho\\). While we can easily sample independent observations, the exercise is insightful to see how well the methods works as the dimension increases, and when the correlation between pairs becomes stronger.\nConsider \\(\\boldsymbol{Y} \\sim \\mathsf{Norm}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) and a partition \\((\\boldsymbol{Y}_1^\\top, \\boldsymbol{Y}_2^\\top)^\\top\\): the conditional distribution of the \\(k\\) subvector \\(\\boldsymbol{Y}_1\\) given the \\(d-k\\) other components \\(\\boldsymbol{Y}_2\\) is, in terms of either the covariance (first line) or the precision (second line), Gaussian where \\[\\begin{align*}\n\\boldsymbol{Y}_1 \\mid \\boldsymbol{Y}_2=\\boldsymbol{y}_2 &\\sim \\mathsf{Norm}_{k}\\left\\{ \\boldsymbol{\\mu}_1 + \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1}(\\boldsymbol{y}_2 - \\boldsymbol{\\mu}_2), \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\boldsymbol{\\Sigma}_{21}\\right\\}\n\\\\&\\sim \\mathsf{Norm}_{k}\\left\\{ \\boldsymbol{\\mu}_1 -\\mathbf{Q}_{11}^{-1}\\mathbf{Q}_{12}(\\boldsymbol{y}_2 - \\boldsymbol{\\mu}_2), \\mathbf{Q}_{11}^{-1}\\right\\}.\n\\end{align*}\\]\n\n# Create a 20 dimensional equicorrelation\nd &lt;- 20\nQ &lt;- hecbayes::equicorrelation(d = d, rho = 0.9, precision = TRUE)\nB &lt;- 1e4\nchains &lt;- matrix(0, nrow = B, ncol = d)\nmu &lt;- rep(2, d)\n# Start far from mode\ncurr &lt;- rep(-3, d)\nfor(i in seq_len(B)){\n  # Random scan, updating one variable at a time\n  for(j in sample(1:d, size = d)){\n    # sample from conditional Gaussian given curr\n    curr[j] &lt;- hecbayes::rcondmvnorm(\n      n = 1, \n      value = curr, \n      ind = j, \n      mean = mu, \n      precision = Q)\n  }\n  chains[i,] &lt;- curr # save values after full round of update\n}\n\nAs the dimension of the parameter space increases, and as the correlation between components becomes larger, the efficiency of the Gibbs sampler degrades: Figure 3.12 shows the first component for updating one-parameter at a time for a multivariate Gaussian target in dimensions \\(d=20\\) and \\(d=3\\), started at four deviation away from the mode. The chain makes smaller steps when there is strong correlation, resulting in an inefficient sampler.\n\n\n\n\n\nFigure 3.12: Trace plots (top) and correlograms (bottom) for the first component of a Gibbs sampler with \\(d=20\\) equicorrelated Gaussian variates with correlation \\(\\rho=0.9\\) (left) and \\(d=3\\) with equicorrelation \\(\\rho=0.5\\) (right).\n\n\n\n\nThe main bottleneck in Gibbs sampling is determining all of the relevant conditional distributions, which often relies on setting conditionally conjugate priors. In large models with multiple layers, full conditionals may only depend on a handful of parameters.\n\n\nExample 3.8 Consider a Gaussian model \\(Y_i \\sim \\mathsf{Norm}(\\mu, \\tau)\\) (\\(i=1, \\ldots, n\\)) are independent, and where we assign priors \\(\\mu \\sim \\mathsf{Norm}(\\nu, \\omega)\\) and \\(\\tau \\sim \\mathsf{InvGamma}(\\alpha, \\beta)\\).\nThe joint posterior is not available in closed form, but the independent priors for the mean and variance of the observations are conditionally conjugate, since the joint posterior \\[\\begin{align*}\np(\\mu, \\tau \\mid \\boldsymbol{y}) \\propto& \\tau^{-n/2}\\exp\\left\\{-\\frac{1}{2\\tau}\\left(\\sum_{i=1}^n y_i^2 - 2\\mu \\sum_{i=1}^n y_i+n\\mu^2 \\right)\\right\\}\\\\& \\times \\exp\\left\\{-\\frac{(\\mu-\\nu)^2}{2\\omega}\\right\\} \\times \\tau^{-\\alpha-1}\\exp(-\\beta/\\tau)\n\\end{align*}\\] gives us \\[\\begin{align*}\np(\\mu \\mid \\tau, \\boldsymbol{y}) &\\propto \\exp\\left\\{-\\frac{1}{2} \\left( \\frac{\\mu^2-2\\mu\\overline{y}}{\\tau/n} + \\frac{\\mu^2-2\\nu \\mu}{\\omega}\\right)\\right\\}\\\\\np(\\tau \\mid \\mu, \\boldsymbol{y}) & \\propto \\tau^{-n/2-\\alpha-1}\\exp\\left[-\\left\\{\\frac{\\sum_{i=1}^n (y_i-\\mu)^2}{2} + \\beta \\right\\}/\\tau\\right]\n\\end{align*}\\] so we can simulate in turn \\[\\begin{align*}\n\\mu_t \\mid \\tau_{t-1}, \\boldsymbol{y} &\\sim \\mathsf{Norm}\\left(\\frac{n\\overline{y}\\omega+\\tau \\nu}{\\tau + n\\omega}, \\frac{\\omega \\tau}{\\tau + n\\omega}\\right)\\\\\n\\tau_t \\mid \\mu_t, \\boldsymbol{y} &\\sim \\mathsf{InvGamma}\\left\\{\\frac{n}{2}+\\alpha, \\frac{\\sum_{i=1}^n (y_i-\\mu)^2}{2} + \\beta\\right\\}.\n\\end{align*}\\]\n\n\nRemark (Gibbs sampler and proper posterior). Gibbs sampling cannot be used to determine if the posterior is improper. If the posterior is not well defined, the Markov chains may seem to stabilize even though there is no proper target.\n\n\nProposition 3.8 (Bayesian linear model) Consider a linear regression model with observation-specific mean \\(\\mu_i = \\mathbf{x}_i\\boldsymbol{\\beta}\\) \\((i=1,\\ldots, n)\\) with \\(\\mathbf{x}_i\\) the \\(i\\)th row of the \\(n \\times p\\) model matrix \\(\\mathbf{X}\\).\nConcatenating records, \\(\\boldsymbol{Y} \\sim \\mathsf{No}_n(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{Q}_y^{-1})\\), for a known precision matrix \\(\\mathbf{Q}_y\\), typically \\(\\mathbf{I}_n\\). To construct a conjugate joint prior for \\(p(\\boldsymbol{\\beta}, \\sigma^2)\\), we consider the sequential formulation \\[\\begin{align*}\n\\boldsymbol{\\beta} \\mid \\sigma^2 \\sim \\mathsf{Norm}_p(\\boldsymbol{\\nu}_\\beta, \\sigma^2 \\mathbf{Q}^{-1}_\\beta), \\qquad \\sigma^2 \\sim \\mathsf{InvGamma}(\\alpha,\\beta)\n\\end{align*}\\] where \\(\\mathsf{InvGamma}\\) denotes the inverse gamma distribution5\n\nThe joint posterior is Gaussian-inverse gamma and can be factorized \\[\\begin{align*}\np(\\boldsymbol{\\beta}, \\sigma^2 \\mid y) = p(\\sigma^2 \\mid y) p(\\boldsymbol{\\beta} \\mid \\sigma^2, y)\n\\end{align*}\\] where \\(p(\\sigma^2 \\mid y) \\sim \\mathsf{InvGamma}(\\alpha^*, \\beta^*)\\) and \\(p(\\boldsymbol{\\beta} \\mid \\sigma^2, y) \\sim \\mathsf{No}_p(\\mathbf{M}\\boldsymbol{m}, \\sigma^2\\mathbf{M})\\) with \\(\\alpha^* = \\alpha + n/2\\), \\(\\beta^*=\\beta + 0.5 \\boldsymbol{\\nu}_\\beta^\\top \\mathbf{Q}_\\beta\\boldsymbol{\\nu}_\\beta + \\boldsymbol{y}^\\top\\boldsymbol{y} - \\boldsymbol{m}^\\top\\mathbf{M}\\boldsymbol{m}\\), \\(\\boldsymbol{m} = \\mathbf{Q}_\\beta \\boldsymbol{\\nu}_\\beta + \\mathbf{X}^\\top \\mathbf{Q}_y\\boldsymbol{y}\\) and \\(\\mathbf{M} = (\\mathbf{Q}_\\beta + \\mathbf{X}^\\top\\mathbf{Q}_y\\mathbf{X})^{-1};\\) the latter can be evaluated efficiently using Shermann–Morrisson–Woodbury identity. Given the conditionally conjugate priors, we can easily sample from the posterior using Gibbs sampling.\n\n\n3.4.1 Data augmentation and auxiliary variables\nIn many problems, the likelihood \\(p(\\boldsymbol{y}; \\boldsymbol{\\theta})\\) is intractable or costly to evaluate and auxiliary variables are introduced to simplify calculations, as in the expectation-maximization algorithm. The Bayesian analog is data augmentation (Tanner and Wong 1987), which we present succinctly: let \\(\\boldsymbol{\\theta} \\in \\Theta\\) be a vector of parameters and consider auxiliary variables \\(\\boldsymbol{u} \\in \\mathbb{R}^k\\) such that \\(\\int_{\\mathbb{R}^k} p(\\boldsymbol{u}, \\boldsymbol{\\theta}; \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{u} = p(\\boldsymbol{\\theta}; \\boldsymbol{y})\\), i.e., the marginal distribution is that of interest, but evaluation of \\(p(\\boldsymbol{u}, \\boldsymbol{\\theta}; \\boldsymbol{y})\\) is cheaper. The data augmentation algorithm consists in running a Markov chain on the augmented state space \\((\\Theta, \\mathbb{R}^k)\\), simulating in turn from the conditionals \\(p(\\boldsymbol{u}; \\boldsymbol{\\theta}, \\boldsymbol{y})\\) and \\(p(\\boldsymbol{\\theta}; \\boldsymbol{u}, \\boldsymbol{y})\\) with new variables chosen to simplify the likelihood. If simulation from the conditionals is straightforward, we can also use data augmentation to speed up calculations or improve mixing. For more details and examples, see Dyk and Meng (2001) and Hobert (2011).\n\nExample 3.9 Consider binary responses \\(\\boldsymbol{Y}_i\\), for which we postulate a probit regression model, \\[\\begin{align*}\np_i = \\Pr(Y_i=1) = \\Phi(\\beta_0 + \\beta_1 \\mathrm{X}_{i1} + \\cdots + \\beta_p\\mathrm{X}_{ip}),\n\\end{align*}\\] where \\(\\Phi\\) is the distribution function of the standard Gaussian distribution. The likelihood of the probit model for a sample of \\(n\\) independent observations is \\[L(\\boldsymbol{\\beta}; \\boldsymbol{y}) = \\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i},\\] and this prevents easy simulation. We can consider a data augmentation scheme where \\(Y_i = \\mathsf{I}(Z_i &gt; 0)\\), where \\(Z_i \\sim \\mathsf{Norm}(\\mathbf{x}_i\\boldsymbol{\\beta}, 1)\\), where \\(\\mathbf{x}_i\\) is the \\(i\\)th row of the design matrix.\nThe augmented data likelihood is \\[\\begin{align*}\np(\\boldsymbol{z}, \\boldsymbol{y} \\mid \\boldsymbol{\\beta}) \\propto \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{z} - \\mathbf{X}\\boldsymbol{\\beta})^\\top(\\boldsymbol{z} - \\mathbf{X}\\boldsymbol{\\beta})\\right\\} \\times \\prod_{i=1}^n \\mathsf{I}(z_i &gt; 0)^{y_i}\\mathsf{I}(z_i \\le 0)^{1-y_i}\n\\end{align*}\\] Given \\(Z_i\\), the coefficients \\(\\boldsymbol{\\beta}\\) are simply the results of ordinary linear regression with unit variance, so \\[\\begin{align*}\n\\boldsymbol{\\beta} \\mid \\boldsymbol{z}, \\boldsymbol{y} &\\sim \\mathsf{Norm}\\left\\{\\widehat{\\boldsymbol{\\beta}}, (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\right\\}\n\\end{align*}\\] with \\(\\widehat{\\boldsymbol{\\beta}}=(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\boldsymbol{z}\\) is the ordinary least square estimator from the regression with model matrix \\(\\mathbf{X}\\) and response vector \\(\\boldsymbol{z}\\). The augmented variables \\(Z_i\\) are conditionally independent and truncated Gaussian with \\[\\begin{align*}\nZ_i \\mid y_i, \\boldsymbol{\\beta} \\sim \\begin{cases}\n\\mathsf{TruncNorm}(\\mathbf{x}_i\\boldsymbol{\\beta}, -\\infty, 0) & y_i =0 \\\\\n\\mathsf{TruncNorm}(\\mathbf{x}_i\\boldsymbol{\\beta}, 0, \\infty) & y_i =1.\n\\end{cases}\n\\end{align*}\\] and we can use the algorithms of Example 3.2 to simulate these.\n\n\nExample 3.10 (Bayesian LASSO) The Laplace distribution with mean \\(\\mu\\) and scale \\(\\sigma\\), which has density \\[\\begin{align*}\nf(x; \\mu, \\sigma) = \\frac{1}{2\\sigma}\\exp\\left(-\\frac{|x-\\mu|}{\\sigma}\\right),\n\\end{align*}\\] can be expressed as a scale mixture of Gaussians, where \\(Y \\sim \\mathsf{La}(\\mu, \\sigma)\\) is equivalent to \\(Z \\mid \\tau \\sim \\mathsf{Norm}(\\mu, \\tau)\\) and \\(\\tau \\sim \\mathsf{Exp}\\{(2\\sigma)^{-1}\\}\\). With the improper prior \\(p(\\mu, \\sigma) \\propto \\sigma^{-1}\\) and with \\(n\\) independent and identically distributed Laplace variates, the joint posterior can be written \\[\\begin{align*}\np(\\boldsymbol{\\tau}, \\mu, \\sigma \\mid \\boldsymbol{y}) &\\propto \\left(\\prod_{i=1}^n \\tau_i\\right)^{-1/2}\\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\tau_i}\\right\\} \\\\&\\quad \\times \\frac{1}{\\sigma^{n+1}}\\exp\\left(-\\frac{1}{2\\sigma}\\sum_{i=1}^n \\tau_i\\right)\n\\end{align*}\\] and \\(\\mu \\mid \\cdots\\) and \\(\\sigma \\mid \\cdots\\) are, as usual, Gaussian and inverse gamma, respectively. The variances, \\(\\tau_j\\), are conditionally independent of one another with \\[\\begin{align*}\np(\\tau_j \\mid \\mu, \\sigma, y_j) &\\propto \\tau_j^{-1/2}\\exp\\left\\{-\\frac{1}{2}\\frac{(y_j-\\mu)^2}{\\tau_j} -\\frac{1}{2} \\frac{\\tau_j}{\\sigma}\\right\\}\n\\end{align*}\\] so with \\(\\xi_j=1/\\tau_j\\), we have \\[\\begin{align*}\np(\\xi_j \\mid \\mu, \\sigma, y_j) &\\propto \\xi_j^{-3/2}\\exp\\left\\{-\\frac{1}{2\\sigma}\\frac{\\xi_j(y_j-\\mu)^2}{\\sigma} -\\frac{1}{2} \\frac{1}{\\xi_j}\\right\\}\\\\\n\\end{align*}\\] and we recognize the latter as a Wald (or inverse Gaussian) distribution, whose density function is \\[\\begin{align*}\nf(y; \\nu, \\lambda) &= \\left(\\frac{\\lambda}{2\\pi y^{3}}\\right)^{1/2} \\exp\\left\\{ - \\frac{\\lambda (y-\\nu)^2}{2\\nu^2y}\\right\\}, \\quad y &gt; 0\n\\\\ &\\stackrel{y}{\\propto} y^{-3/2}\\exp\\left\\{-\\frac{\\lambda}{2} \\left(\\frac{y}{\\nu} + \\frac{1}{y}\\right)\\right\\}\n\\end{align*}\\] for location \\(\\nu &gt;0\\) and shape \\(\\lambda&gt;0\\), where \\(\\xi_i \\sim \\mathsf{Wald}(\\nu_i, \\lambda)\\) with \\(\\nu_i=\\{\\sigma/(y_i-\\mu)^2\\}^{1/2}\\) and \\(\\lambda=\\sigma^{-1}\\).\nPark and Casella (2008) use this hierarchical construction to defined the Bayesian LASSO. With a model matrix \\(\\mathbf{X}\\) whose columns are standardized to have mean zero and unit standard deviation, we may write \\[\\begin{align*}\n\\boldsymbol{Y} \\mid \\mu, \\boldsymbol{\\beta}, \\sigma^2 &\\sim  \\mathsf{Norm}_n(\\mu \\boldsymbol{1}_n + \\mathbf{X}\\boldsymbol{\\beta}, \\sigma \\mathbf{I}_n)\\\\\n\\beta_j \\mid \\sigma, \\tau &\\sim \\mathsf{Norm}(0, \\sigma\\tau)\\\\\n\\tau &\\sim \\mathsf{Exp}(\\lambda/2)\n\\end{align*}\\] If we set an improper prior \\(p(\\mu, \\sigma) \\propto \\sigma^{-1}\\), the resulting conditional distributions are all available and thus the model is amenable to Gibbs sampling.\nThe Bayesian LASSO places a Laplace penalty on the regression coefficients, with lower values of \\(\\lambda\\) yielding more shrinkage. Note that, contrary to the frequentist setting, none of the posterior draws of \\(\\boldsymbol{\\beta}\\) are exactly zero.\nMany elliptical distributions can be cast as scale mixture models of spherical or Gaussian variables; see, e.g., Section 10.2 of Albert (2009) for a similar derivation with a Student-\\(t\\) distribution.\n\n\nExample 3.11 (Mixture models) In clustering problems, we can specify that observations arise from a mixture model with a fixed or unknown number of coefficients: the interest lies then in estimating\nA \\(K\\)-mixture model is a weighted combination of models frequently used in clustering or to model subpopulations with respective densities \\(f_k\\), with density \\[f(x; \\boldsymbol{\\theta}, \\boldsymbol{\\omega}) = \\sum_{k=1}^K \\omega_kf_k(x; \\boldsymbol{\\theta}_k), \\qquad \\omega_1 + \\cdots \\omega_K=1.\\] Since the density involves a sum, numerical optimization is challenging. Let \\(C_i\\) denote the cluster index for observation \\(i\\): if we knew the value of \\(C_i =j\\), the density would involve only \\(f_j\\). We can thus use latent variables representing the group allocation to simplify the problem and run an EM algorithm or use the data augmentation. In an iterative framework, we can consider the complete data as the tuples \\((X_i, Z_i)\\), where \\(Z_i = \\mathsf{I}(C_i=k)\\).\nWith the augmented data, the conditional distribution of \\(Z_i \\mid X_i, \\boldsymbol{\\omega}, \\boldsymbol{\\theta} \\sim \\mathsf{Multinom}(1, \\boldsymbol{\\gamma}_{ik})\\) where \\[\\gamma_{ik} = \\frac{\\omega_k f_k(X_i\\boldsymbol{\\theta}_k)}{\\sum_{j=1}^K f_j(X_i\\boldsymbol{\\theta}_k)}.\\] Given suitable priors for the probabilities \\(\\boldsymbol{\\omega}\\) and \\(\\boldsymbol{\\theta} \\equiv \\{\\boldsymbol{\\theta}_1, \\ldots, \\boldsymbol{\\theta}_k\\}\\), we can use Gibbs sampling updating \\(\\boldsymbol{Z}\\), \\(\\boldsymbol{\\omega}\\) and \\(\\boldsymbol{\\theta}\\) in turn."
  },
  {
    "objectID": "mcmc.html#bayesian-workflow-and-diagnostics-for-markov-chains",
    "href": "mcmc.html#bayesian-workflow-and-diagnostics-for-markov-chains",
    "title": "3  Simulation-based inference",
    "section": "3.5 Bayesian workflow and diagnostics for Markov chains",
    "text": "3.5 Bayesian workflow and diagnostics for Markov chains\nFor a given problem, there are many different Markov chain Monte Carlo algorithms that one can implement: they will typically be distinguished based on the running time and the efficiency (with algorithms providing chains that have low autocorrelation being better). Many visual diagnostics and standard tests can be used to diagnose lack of convergence, or inefficiency. The purpose of this section is to review these in turn.\nThe Bayesian workflow is a coherent framework for model construction, estimation and validation. It typically involves multiple iterations tuning, adapting and modifying both the models and the algorithms in the hope of achieving a model that is useful (Gelman et al. 2020); see also Michael Betancourt for excellent visualizations.\nTo illustrate these, we revisit the model from Example 2.15 with a penalized complexity prior for the individual effect \\(\\alpha_i\\) and vague normal priors. We also fit a simple Poisson model with only the fixed effect, taking \\(Y_{ij} \\sim \\mathsf{Pois}\\{\\exp(\\beta_j)\\}\\) with \\(\\beta_j \\sim \\mathsf{Norm}(0,100)\\) has much too little variability relative to the observations.\n\n3.5.1 Trace plots\nIt is useful to inspect visually the Markov chain, as it may indicate several problems. If the chain drifts around without stabilizing around the posterior mode, then we can suspect that it hasn’t reached it’s stationary distribution (likely due to poor starting values). In such cases, we need to disregard the dubious draws from the chain by discarding the so-called warm up or burn in period. While there are some guarantees of convergence in the long term, silly starting values may translate into tens of thousands of iterations lost wandering around in regions with low posterior mass. Preliminary optimization and plausible starting values help alleviate these problems. Figure 3.13 shows the effect of bad starting values on a toy problem where convergence to the mode is relatively fast. If the proposal is in a flat region of the space, it can wander around for a very long time before converging to the stationary distribution.\nIf we run several chains, as in Figure 3.13, with different starting values, we can monitor convergence by checking whether these chains converge to the same target. A trace rank plots, shown on right panel of Figure 3.13, compares the rank of the values of the different chain at a given iteration: with good mixing, the ranks should switch frequently and be distributed uniformly across integers.\n\n\n\n\n\nFigure 3.13: Traceplots of three Markov chains for the same target with different initial values for the first 500 iterations (left) and trace rank plot after discarding these (right).\n\n\n\n\n\n\n3.5.2 Diagnostics of convergence\nGenerally, one would run a MCMC algorithm. The first iterations, used during the burn in period to tune proposal variances and allow the chains to converge to the stationary distribution, are discarded. If visual inspection of the chains reveal that some of the chains for one or more parameters are not stationary until some iteration, we will discard all of these in addition.\nThe target of inference is functional (i.e., one-dimensional summaries of the chain): we need to have convergence of the latter, but also sufficient effective sample size for our averages to be accurate (at least to two significant digits).\nFor the Poisson example, the effective sample size for the \\(\\boldsymbol{\\beta}\\) for the multilevel model is a bit higher than 1000 with \\(B=5000\\) iterations, whereas we have for the simple naive model is \\(10^{4}\\) for \\(B=10000\\) draws, suggesting superefficient sampling. The dependency between \\(\\boldsymbol{\\alpha}\\) and \\(\\boldsymbol{\\beta}\\) is responsible for the drop in accuracy.\nThe coda (convergence diagnosis and output analysis) R package contains many tests. For example, the Geweke \\(Z\\)-score compares the averages for the beginning and the end of the chain: rejection of the null implies lack of convergence, or poor mixing.\n\nRunning multiple Markov chains can be useful for diagnostics. The Gelman–Rubin diagnostic \\(\\widehat{R}\\), also called potential scale reduction statistic, is obtained by considering the difference between within-chains and between-chains variance. Suppose we run \\(M\\) chains for \\(B\\) iterations, post burn in. Denoting by \\(\\theta_{bm}\\) the \\(b\\)th draw of the \\(m\\)th chain, we compute the global average \\(\\overline{\\theta} = B^{-1}M^{-1}\\sum_{b=1}^B \\sum_{m=1}^m \\theta_{bm}\\) and similarly the chain sample average and variances, respectively \\(\\overline{\\theta}_m\\) and \\(\\widehat{\\sigma}^2_m\\) (\\(m=1, \\ldots, M\\)). The between-chain variance and within-chain variance estimator are \\[\\begin{align*}\n\\mathsf{Va}_{\\text{between}} &= \\frac{B}{M-1}\\sum_{m=1}^M (\\overline{\\theta}_m - \\overline{\\theta})^2\\\\\n\\mathsf{Va}_{\\text{within}} &= \\frac{1}{M}\\sum_{m=1}^m \\widehat{\\sigma}^2_m\n\\end{align*}\\] and we can compute \\[\\begin{align*}\n\\widehat{R} = \\left(\\frac{\\mathsf{Va}_{\\text{within}}(B-1) + \\mathsf{Va}_{\\text{between}}}{B\\mathsf{Va}_{\\text{within}}}\\right)^{1/2}\n\\end{align*}\\] The potential scale reduction statistic must be, by construction, larger than 1 in large sample. Any value larger than this is indicative of problems of convergence. While the Gelman–Rubin diagnostic is frequently reported, and any value larger than 1 deemed problematic, it is not enough to have approximately \\(\\widehat{R}=1\\) to guarantee convergence, but large values are usually indication of something being amiss. Figure 3.14 shows two instances where the chains are visually very far from having the same average and this is reflected by the large values of \\(\\widehat{R}\\).\n\n\n\n\n\nFigure 3.14: Two pairs of Markov chains: the top ones seem stationary, but with different modes. This makes the between chain variance substantial, with a value of \\(\\widehat{R} \\approx 3.4\\), whereas the chains on the right hover around the same values of zero, but do not appear stable with \\(\\widehat{R} \\approx 1.6\\).\n\n\n\n\nMore generally, it is preferable to run a single chain for a longer period than run multiple chains sequentially, as there is a cost to initializing multiple times with different starting values since we must discard initial draws. With parallel computations, multiple chains are more frequent nowadays.\nMCMC algorithms are often run thinning the chain (i.e., keeping only a fraction of the samples drawn, typically every \\(k\\) iteration). This is wasteful as we can of course get more precise estimates by keeping all posterior draws, whether correlated or not. The only argument in favor of thinning is limited storage capacity: if we run very long chains in a model with hundreds of parameters, we may run out of memory.\n\n\n3.5.3 Posterior predictive checks\nPosterior predictive checks can be used to compare models of varying complexity.One of the visual diagnostics, outlined in Gabry et al. (2019), consists in computing a summary statistic of interest from the posterior predictive (whether mean, median, quantile, skewness, etc.) which is relevant for the problem at hand and which we hope our model can adequately capture.\nSuppose we have \\(B\\) draws from the posterior and simulate for each \\(n\\) observations from the posterior predictive \\(p(\\widetilde{\\boldsymbol{y}} \\mid \\boldsymbol{y})\\): we can benchmark summary statistics from our original data \\(\\boldsymbol{y}\\) with the posterior predictive copies \\(\\widetilde{\\boldsymbol{y}}_b\\). Figure 3.15 shows this for the two competing models and highlight the fact that the simpler model is not dispersed enough. Even the more complex model struggles to capture this additional heterogeneity with the additional variables. One could go back to the drawing board and consider a negative binomial model.\n\n\n\n\n\nFigure 3.15: Posterior predictive checks for the standard deviation (top) and density of posterior draws (bottom) for hierarchical Poisson model with individual effects (left) and simpler model with only conditions (right).\n\n\n\n\n\n\n3.5.4 Information criterion\nThe widely applicable information criterion (Watanabe 2010) is a measure of predictive performance that approximates the cross-validation loss. Consider first the log pointwise predictive density, defined as the expected value over the posterior distribution \\(p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\), \\[\\begin{align*}\n\\mathsf{LPPD}_i = \\mathsf{E}_{\\boldsymbol{\\theta} \\mid \\boldsymbol{y}} \\left\\{ \\log p(y_i \\mid \\boldsymbol{\\theta})\\right\\}.\n\\end{align*}\\] The higher the value of the predictive density \\(\\mathsf{LPPD}_i\\), the better the fit for that observation.\nAs in general information criteria, we sum over all observations, adding a penalization factor that approximates the effective number of parameters in the model, with \\[\\begin{align*}\nn\\mathsf{WAIC} = -\\sum_{i=1}^n \\mathsf{LPPD}_i + \\sum_{i=1}^n \\mathsf{Va}_{\\boldsymbol{\\theta} \\mid \\boldsymbol{y}}\\{\\log p(y_i \\mid \\boldsymbol{\\theta})\\}\n\\end{align*}\\] where we use again the empirical variance to compute the rightmost term. When comparing competing models, we can rely on their values of \\(\\mathsf{WAIC}\\) to discriminate about the predictive performance. To compute \\(\\mathsf{WAIC}\\), we need to store the values of the log density of each observation, or at least minimally compute the running mean and variance accurately pointwise at storage cost \\(\\mathrm{O}(n)\\). Note that Section 7.2 of Gelman et al. (2013) define the widely applicable information criterion as \\(2n \\times \\mathsf{WAIC}\\) to make on par with other information criteria, which are defined typically on the deviance scale and so that lower values correspond to higher predictive performance. For the smartwatch model, we get a value of 3.07 for the complex model and 4.51: this suggests an improvement in using individual-specific effects.\nIdeally, one would measure the predictive performance using the leave-one-out predictive distribution for observation \\(i\\) given all the rest, \\(p(y_i \\mid \\boldsymbol{y}_{-i})\\), to avoid double dipping — the latter is computationally intractable because it would require running \\(n\\) Markov chains with \\(n-1\\) observations each, but we can get a good approximation using importance sampling. The loo package uses this with generalized Pareto smoothing to avoid overly large weights.\nOnce we have the collection of estimated \\(p(y_i \\mid \\boldsymbol{y}_{-i})\\), we can assess the probability level of each observation. This gives us a set of values which should be approximately uniform if the model was perfectly calibrated. The probability of seeing an outcome as extreme as \\(y_i\\) can be obtained by simulating draws from the posterior predictive given \\(\\boldsymbol{y}_{-i}\\) and computing the scaled rank of the original observation. Values close to zero or one may indicate outliers.\n\n\n\n\n\nFigure 3.16: Quantile-quantile plots based on leave-one-out cross validation for model for the Poisson hierarchical model with the individual random effects (left) and without (right).\n\n\n\n\n\n\n\n\n\n\n\n\nAlbert, Jim. 2009. Bayesian Computation with R. 2nd ed. New York: springer. https://doi.org/10.1007/978-0-387-92298-0.\n\n\nAndrieu, Christophe, and Johannes Thoms. 2008. “A Tutorial on Adaptive MCMC.” Statistics and Computing 18 (4): 343–73. https://doi.org/10.1007/s11222-008-9110-y.\n\n\nBotev, Zdravko, and Pierre L’Écuyer. 2017. “Simulation from the Normal Distribution Truncated to an Interval in the Tail.” In Proceedings of the 10th EAI International Conference on Performance Evaluation Methodologies and Tools on 10th EAI International Conference on Performance Evaluation Methodologies and Tools, 23–29. https://doi.org/10.4108/eai.25-10-2016.2266879.\n\n\nDevroye, L. 1986. Non-Uniform Random Variate Generation. New York: Springer. http://www.nrbook.com/devroye/.\n\n\nDyk, David A van, and Xiao-Li Meng. 2001. “The Art of Data Augmentation.” Journal of Computational and Graphical Statistics 10 (1): 1–50. https://doi.org/10.1198/10618600152418584.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. “Visualization in Bayesian Workflow.” Journal of the Royal Statistical Society Series A: Statistics in Society 182 (2): 389–402. https://doi.org/10.1111/rssa.12378.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd ed. New York: Chapman; Hall/CRC. https://doi.org/10.1201/b16018.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. “Bayesian Workflow.” arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2011.01808.\n\n\nGeweke, John. 2004. “Getting It Right: Joint Distribution Tests of Posterior Simulators.” Journal of the American Statistical Association 99 (467): 799–804. https://doi.org/10.1198/016214504000001132.\n\n\nGeyer, Charles J. 2011. “Introduction to Markov Chain Monte Carlo.” In Handbook of Markov Chain Monte Carlo, edited by S. Brooks, A. Gelman, G. Jones, and X. L. Meng, 3–48. Boca Raton: CRC Press. https://doi.org/10.1201/b10905.\n\n\nGreen, Peter J. 2001. “A Primer on Markov Chain Monte Carlo.” Monographs on Statistics and Applied Probability 87: 1–62.\n\n\nHastings, W. K. 1970. “Monte Carlo sampling methods using Markov chains and their applications.” Biometrika 57 (1): 97–109. https://doi.org/10.1093/biomet/57.1.97.\n\n\nHobert, James P. 2011. “The Data Augmentation Algorithm: Theory and Methodology.” In Handbook of Markov Chain Monte Carlo, edited by S. Brooks, A. Gelman, G. Jones, and X. L. Meng, 253–94. Boca Raton: CRC Press. https://doi.org/10.1201/b10905.\n\n\nKinderman, Albert J, and John F Monahan. 1977. “Computer Generation of Random Variables Using the Ratio of Uniform Deviates.” ACM Transactions on Mathematical Software (TOMS) 3 (3): 257–60.\n\n\nMetropolis, Nicholas, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. 1953. “Equation of State Calculations by Fast Computing Machines.” The Journal of Chemical Physics 21 (6): 1087–92. https://doi.org/10.1063/1.1699114.\n\n\nPark, Trevor, and George Casella. 2008. “The Bayesian Lasso.” Journal of the American Statistical Association 103 (482): 681–86. https://doi.org/10.1198/016214508000000337.\n\n\nRobert, Christian P., and George Casella. 2004. Monte Carlo Statistical Methods. New York, NY: Springer. https://doi.org/10.1007/978-1-4757-4145-2.\n\n\nRoberts, Gareth O., and Jeffrey S. Rosenthal. 2001. “Optimal Scaling for Various Metropolis–Hastings Algorithms.” Statistical Science 16 (4): 351–67. https://doi.org/10.1214/ss/1015346320.\n\n\nSherlock, Chris. 2013. “Optimal Scaling of the Random Walk Metropolis: General Criteria for the 0.234 Acceptance Rule.” Journal of Applied Probability 50 (1): 1–15. https://doi.org/10.1239/jap/1363784420.\n\n\nTanner, Martin A., and Wing Hung Wong. 1987. “The Calculation of Posterior Distributions by Data Augmentation.” Journal of the American Statistical Association 82 (398): 528–40. https://doi.org/10.1080/01621459.1987.10478458.\n\n\nWakefield, J. C., A. E. Gelfand, and A. F. M. Smith. 1991. “Efficient Generation of Random Variates via the Ratio-of-Uniforms Method.” Statistics and Computing 1 (2): 129–33. https://doi.org/10.1007/BF01889987.\n\n\nWatanabe, Sumio. 2010. “Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory.” Journal of Machine Learning Research 11 (116): 3571–94. http://jmlr.org/papers/v11/watanabe10a.html."
  },
  {
    "objectID": "mcmc.html#footnotes",
    "href": "mcmc.html#footnotes",
    "title": "3  Simulation-based inference",
    "section": "",
    "text": "Meaning \\(\\mathsf{E}\\{g^2(X)\\}&lt;\\infty\\), so the variance of \\(g(X)\\) exists.↩︎\nBy contrasts, if data are identically distributed but not independent, care is needed↩︎\nWhile we won’t focus on the fine prints of the contract, there are conditions for validity and these matter!↩︎\nGeometric ergodicity and existence of moments, among other things.↩︎\nThis simply means that the precision \\(\\sigma^{-2}\\), the reciprocal of the variance, has a gamma distribution with shape \\(\\alpha\\) and rate \\(\\beta\\).↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Albert, Jim. 2009. Bayesian Computation with R.\n2nd ed. New York: springer. https://doi.org/10.1007/978-0-387-92298-0.\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications\nin R. Boca Raton, FL: CRC Press.\n\n\nAndrieu, Christophe, and Johannes Thoms. 2008. “A Tutorial on\nAdaptive MCMC.” Statistics and Computing 18\n(4): 343–73. https://doi.org/10.1007/s11222-008-9110-y.\n\n\nBotev, Zdravko, and Pierre L’Écuyer. 2017. “Simulation from the\nNormal Distribution Truncated to an Interval in the Tail.” In\nProceedings of the 10th EAI International Conference on Performance\nEvaluation Methodologies and Tools on 10th EAI International Conference\non Performance Evaluation Methodologies and Tools, 23–29. https://doi.org/10.4108/eai.25-10-2016.2266879.\n\n\nBrodeur, Mathieu, Perrine Ruer, Pierre-Majorique Léger, and Sylvain\nSénécal. 2021. “Smartwatches Are More Distracting Than Mobile\nPhones While Driving: Results from an Experimental Study.”\nAccident Analysis & Prevention 149: 105846. https://doi.org/10.1016/j.aap.2020.105846.\n\n\nColes, Stuart G., and Jonathan A. Tawn. 1996. “A\nBayesian Analysis of Extreme Rainfall Data.”\nJournal of the Royal Statistical Society. Series C (Applied\nStatistics) 45 (4): 463–78. https://doi.org/10.2307/2986068.\n\n\nDevroye, L. 1986. Non-Uniform Random Variate\nGeneration. New York: Springer. http://www.nrbook.com/devroye/.\n\n\nDyk, David A van, and Xiao-Li Meng. 2001. “The Art of Data\nAugmentation.” Journal of Computational and Graphical\nStatistics 10 (1): 1–50. https://doi.org/10.1198/10618600152418584.\n\n\nFinetti, Bruno de. 1974. Theory of Probability: A Critical\nIntroductory Treatment. Vol. 1. New York: Wiley.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and\nAndrew Gelman. 2019. “Visualization in\nBayesian Workflow.” Journal of the Royal\nStatistical Society Series A: Statistics in Society 182 (2):\n389–402. https://doi.org/10.1111/rssa.12378.\n\n\nGelfand, Alan E., and Adrian F. M. Smith. 1990. “Sampling-Based\nApproaches to Calculating Marginal Densities.” Journal of the\nAmerican Statistical Association 85 (410): 398–409. https://doi.org/10.1080/01621459.1990.10476213.\n\n\nGelman, Andrew. 2006. “Prior Distributions for Variance Parameters\nin Hierarchical Models (Comment on Article by Browne and\nDraper).” Bayesian Analysis 1 (3): 515–34.\nhttps://doi.org/10.1214/06-BA117A.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd\ned. New York: Chapman; Hall/CRC. https://doi.org/10.1201/b16018.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob\nCarpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian\nBürkner, and Martin Modrák. 2020. “Bayesian Workflow.”\narXiv. https://doi.org/https://doi.org/10.48550/arXiv.2011.01808.\n\n\nGeman, Stuart, and Donald Geman. 1984. “Stochastic Relaxation,\nGibbs Distributions, and the Bayesian\nRestoration of Images.” IEEE Transactions on Pattern Analysis\nand Machine Intelligence PAMI-6 (6): 721–41. https://doi.org/10.1109/TPAMI.1984.4767596.\n\n\nGeweke, John. 2004. “Getting It Right: Joint Distribution Tests of\nPosterior Simulators.” Journal of the American Statistical\nAssociation 99 (467): 799–804. https://doi.org/10.1198/016214504000001132.\n\n\nGeyer, Charles J. 2011. “Introduction to Markov Chain\nMonte Carlo.” In Handbook of\nMarkov Chain Monte Carlo,\nedited by S. Brooks, A. Gelman, G. Jones, and X. L. Meng, 3–48. Boca\nRaton: CRC Press. https://doi.org/10.1201/b10905.\n\n\nGreen, Peter J. 2001. “A Primer on Markov Chain\nMonte Carlo.” Monographs on\nStatistics and Applied Probability 87: 1–62.\n\n\nHastings, W. K. 1970. “Monte\nCarlo sampling methods using Markov chains and\ntheir applications.” Biometrika 57 (1): 97–109.\nhttps://doi.org/10.1093/biomet/57.1.97.\n\n\nHobert, James P. 2011. “The Data Augmentation Algorithm: Theory\nand Methodology.” In Handbook of Markov Chain\nMonte Carlo, edited by S. Brooks, A.\nGelman, G. Jones, and X. L. Meng, 253–94. Boca Raton: CRC Press. https://doi.org/10.1201/b10905.\n\n\nKinderman, Albert J, and John F Monahan. 1977. “Computer\nGeneration of Random Variables Using the Ratio of Uniform\nDeviates.” ACM Transactions on Mathematical Software\n(TOMS) 3 (3): 257–60.\n\n\nMatias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles\nEbersole. 2021. “The Upworthy Research\nArchive, a Time Series of 32,487 Experiments in\nU.S. Media.” Scientific Data 8 (195). https://doi.org/10.1038/s41597-021-00934-7.\n\n\nMcNeil, A. J., R. Frey, and P. Embrechts. 2005. Quantitative Risk\nManagement: Concepts, Techniques, and Tools. 1st ed. Princeton, NJ:\nPrinceton University Press.\n\n\nMetropolis, Nicholas, Arianna W. Rosenbluth, Marshall N. Rosenbluth,\nAugusta H. Teller, and Edward Teller. 1953. “Equation of State Calculations by Fast Computing\nMachines.” The Journal of Chemical Physics 21\n(6): 1087–92. https://doi.org/10.1063/1.1699114.\n\n\nPark, Trevor, and George Casella. 2008. “The Bayesian\nLasso.” Journal of the American Statistical\nAssociation 103 (482): 681–86. https://doi.org/10.1198/016214508000000337.\n\n\nRobert, Christian P., and George Casella. 2004. Monte\nCarlo Statistical Methods. New York, NY: Springer. https://doi.org/10.1007/978-1-4757-4145-2.\n\n\nRoberts, Gareth O., and Jeffrey S. Rosenthal. 2001. “Optimal\nScaling for Various Metropolis–Hastings\nAlgorithms.” Statistical Science 16 (4): 351–67. https://doi.org/10.1214/ss/1015346320.\n\n\nSimpson, Daniel, Håvard Rue, Andrea Riebler, Thiago G. Martins, and\nSigrunn H. Sørbye. 2017. “Penalising Model Component Complexity: A\nPrincipled, Practical Approach to Constructing Priors.”\nStatistical Science 32 (1): 1–28. https://doi.org/10.1214/16-STS576.\n\n\nSørbye, Sigrunn Holbek, and Håvard Rue. 2017. “Penalised\nComplexity Priors for Stationary Autoregressive Processes.”\nJournal of Time Series Analysis 38 (6): 923–35. https://doi.org/10.1111/jtsa.12242.\n\n\nTanner, Martin A., and Wing Hung Wong. 1987. “The Calculation of\nPosterior Distributions by Data Augmentation.” Journal of the\nAmerican Statistical Association 82 (398): 528–40. https://doi.org/10.1080/01621459.1987.10478458.\n\n\nWakefield, J. C., A. E. Gelfand, and A. F. M. Smith. 1991.\n“Efficient Generation of Random Variates via the Ratio-of-Uniforms\nMethod.” Statistics and Computing 1 (2): 129–33. https://doi.org/10.1007/BF01889987.\n\n\nWatanabe, Sumio. 2010. “Asymptotic Equivalence of\nBayes Cross Validation and Widely Applicable Information\nCriterion in Singular Learning Theory.” Journal of Machine\nLearning Research 11 (116): 3571–94. http://jmlr.org/papers/v11/watanabe10a.html."
  }
]