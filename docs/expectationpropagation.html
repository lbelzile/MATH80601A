<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Expectation propagation – Bayesian modelling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./variational.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2bb0ec5e928ee8c40b12725cb7836c35.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a51e8ad160b68d9f8f1ac488cc0f242e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./expectationpropagation.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Expectation propagation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/MATH80601A" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH80601A-bayesmod.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./montecarlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Monte Carlo methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Metropolis–Hastings algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gibbs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Gibbs sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Computational strategies and diagnostics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regression models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./laplace.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Deterministic approximations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./variational.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Variational inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./expectationpropagation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Expectation propagation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#newton-smoothing" id="toc-newton-smoothing" class="nav-link active" data-scroll-target="#newton-smoothing"><span class="header-section-number">11.1</span> Newton smoothing</a></li>
  <li><a href="#expectation-propagation" id="toc-expectation-propagation" class="nav-link" data-scroll-target="#expectation-propagation"><span class="header-section-number">11.2</span> Expectation propagation</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/lbelzile/MATH80601A/edit/main/expectationpropagation.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><h1 class="title display-7"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Expectation propagation</span></h1></header>

<header id="title-block-header">


</header>


<section id="newton-smoothing" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="newton-smoothing"><span class="header-section-number">11.1</span> Newton smoothing</h2>
<p>This section revisits Newton method for calculation of the maximum a posteriori and sheds a new light on the technique by viewing it as a sequence of Gaussian approximations to the target. For simplicity, write the Gaussian distribution in terms of canonical parameters <span class="math display">\[\begin{align*}
q(\boldsymbol{\theta}) \propto \exp \left( - \frac{1}{2} \boldsymbol{\theta}^\top \mathbf{Q}\boldsymbol{\theta} + \boldsymbol{\theta}^\top \boldsymbol{r}\right)
\end{align*}\]</span> where <span class="math inline">\(\mathbf{Q}\)</span> is the precision matrix and <span class="math inline">\(\boldsymbol{r}=\mathbf{Q}\boldsymbol{\mu},\)</span> the linear shift.</p>
<p>Let <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y})=\exp\{-\psi(\boldsymbol{\theta})\}\)</span> denote the posterior density. Since logarithm is a monotonic transform, we can equivalent minimize <span class="math inline">\(\psi(\boldsymbol{\theta})\)</span> to find the posterior mode. Denote the gradient <span class="math inline">\(\nabla_{\boldsymbol{\theta}} \psi(\boldsymbol{\theta}) = \partial \psi/\partial \boldsymbol{\theta}\)</span> and the Hessian matrix <span class="math inline">\(\mathbf{H}(\boldsymbol{\theta}) = \partial^2 \psi/(\partial \boldsymbol{\theta}\partial \boldsymbol{\theta}^\top).\)</span> Starting from an initial value <span class="math inline">\(\boldsymbol{\theta}_{(0)},\)</span> we consider at step <span class="math inline">\(i\)</span>, a second order Taylor series expansion of <span class="math inline">\(\psi(\boldsymbol{\theta})\)</span> around <span class="math inline">\(\boldsymbol{\theta}_{(i)},\)</span> which gives <span class="math display">\[\begin{align*}
\psi(\boldsymbol{\theta}) \approx \psi(\boldsymbol{\theta}_{(i)}) + \nabla_{\boldsymbol{\theta}} \psi(\boldsymbol{\theta}_{(i)})(\boldsymbol{\theta}-\boldsymbol{\theta}_{(i)}) + (\boldsymbol{\theta}-\boldsymbol{\theta}_{(i)})^\top\mathbf{H}(\boldsymbol{\theta}_{(i)})(\boldsymbol{\theta}-\boldsymbol{\theta}_{(i)})
\end{align*}\]</span> The term <span class="math inline">\(\psi(\boldsymbol{\theta}_{(i)})\)</span> is constant, so if we plug-in this inside the exponential, we obtain <span class="math display">\[\begin{align*}
q_{(i+1)}(\boldsymbol{\theta}) &amp;\propto \exp \left\{ - \frac{1}{2} \boldsymbol{\theta}^\top\mathbf{H}(\boldsymbol{\theta}_{(i)}) \boldsymbol{\theta} + \boldsymbol{\theta}_{(i+1)}^\top\mathbf{H}(\boldsymbol{\theta}_{(i)})\boldsymbol{\theta}\right\}
\end{align*}\]</span> where the mean of the approximation is <span class="math display">\[\begin{align*}
\boldsymbol{\theta}_{(i+1)} = \boldsymbol{\theta}_{(i)} - \mathbf{H}^{-1}(\boldsymbol{\theta}_{(i)}) \nabla_{\boldsymbol{\theta}} \psi(\boldsymbol{\theta}_{(i)}).
\end{align*}\]</span> The iterations perform gradient descent, with a correction that adjusts for the curvature locally. This scheme works provided that <span class="math inline">\(\mathbf{H}(\boldsymbol{\theta}_{(i)})\)</span> is positive definite (all of it’s eigenvalues are positive); this may fail for non-convex targets, in which case we could perturb the scheme by adding a ridge-penalty (large diagonal matrix of positive terms) to ensure convergence until we reach a neighborhood of the mode. The new mean vector <span class="math inline">\(\boldsymbol{\theta}_{(i+1)}\)</span> corresponds to a Newton update, and at the same time we have defined a sequence of Gaussian updating approximations. The fixed point to which the algorithm converges is the Laplace approximation.</p>
<p>Suppose for simplicity that the domain <span class="math inline">\(\boldsymbol{\Theta}=\mathbb{R}^p\)</span>, so that no prior transformation is necessary. For location-scale family, we have seen in the section on ADVI that the variational Bayes with a Gaussian approximation on the target <span class="math inline">\(\boldsymbol{\theta} = \boldsymbol{\mu} + \mathbf{L}\boldsymbol{Z}\)</span> with <span class="math inline">\(\mathbf{LL}^\top=\boldsymbol{\Sigma}\)</span> and <span class="math inline">\(\boldsymbol{Z} \sim \mathsf{Gauss}_p(\boldsymbol{0}_p, \mathbf{I}_p)\)</span> that the gradient satisfies <span class="math display">\[\begin{align*}
\nabla_{\boldsymbol{\mu}}\mathsf{ELBO}(q)&amp;= -\mathsf{E}_{\boldsymbol{Z}}\{\nabla_{\boldsymbol{\theta}}\psi(\boldsymbol{\theta})\} \\
\nabla_{\mathbf{L}}\mathsf{ELBO}(q)&amp;= -\mathsf{E}_{\boldsymbol{Z}}\{\nabla_{\boldsymbol{\theta}}\psi(\boldsymbol{\theta})\boldsymbol{Z}^\top\} + \mathbf{L}^{-\top}
\end{align*}\]</span> If we apply integration by parts (Stein’s lemma, see <a href="#prp-stein" class="quarto-xref">Proposition&nbsp;<span>11.1</span></a>) using the fact that the integral is with respect to a standard Gaussian density <span class="math inline">\(\phi_p(\boldsymbol{z}),\)</span> we can rewrite the second term as <span class="math display">\[\begin{align*}
  \nabla_{\mathbf{L}}\mathsf{ELBO}(q)&amp;= -\mathsf{E}_{\boldsymbol{Z}}\left\{ \frac{\partial^2 \psi(\boldsymbol{\theta})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\top}\right\}\mathbf{L} + \mathbf{L}^{-\top}.
\end{align*}\]</span> At a critical point, both of these derivatives must be zero, whence <span class="math display">\[\begin{align*}
\mathsf{E}_{\boldsymbol{Z}}\{\nabla_{\boldsymbol{\theta}}\psi(\boldsymbol{\theta})\} &amp;= \boldsymbol{0}_p. \\
\mathsf{E}_{\boldsymbol{Z}}\left\{ \frac{\partial^2 \psi(\boldsymbol{\theta})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\top}\right\} &amp;= \boldsymbol{\Sigma}^{-1}.
\end{align*}\]</span> Compared to the Laplace approximation, the variational Gaussian approximation returns a vector <span class="math inline">\(\boldsymbol{\mu}\)</span> around which the expected value of the gradient is zero and similarly <span class="math inline">\(\boldsymbol{\Sigma}\)</span> for which the expected value of the curvature (Hessian) is equal to the precision. The averaging step is what distinguishes the Laplace and variational approximations.</p>
<div id="prp-stein" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 11.1 (Stein’s lemma)</strong></span> Consider <span class="math inline">\(h: \mathbb{R}^d \to \mathbb{R}\)</span> a differentiable function and integration with respect to <span class="math inline">\(\boldsymbol{X} \sim \mathsf{Gauss}_d(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span> such that the gradient is absolutely integrable, <span class="math inline">\(\mathsf{E}_{\boldsymbol{X}}\{|\nabla_i h(\boldsymbol{X})|\} &lt; \infty\)</span> for <span class="math inline">\(i=1, \ldots, d.\)</span> Then <span class="citation" data-cites="Liu:1994">(<a href="references.html#ref-Liu:1994" role="doc-biblioref">Liu 1994</a>)</span>, <span class="math display">\[\begin{align*}
\mathsf{E}_{\boldsymbol{X}}\left\{h(\boldsymbol{X})(\boldsymbol{X}-\boldsymbol{\mu})\right\} = \boldsymbol{\Sigma}\mathsf{E}_{\boldsymbol{X}}\left\{\nabla h(\boldsymbol{X})\right\}
\end{align*}\]</span></p>
</div>
</section>
<section id="expectation-propagation" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="expectation-propagation"><span class="header-section-number">11.2</span> Expectation propagation</h2>
<p>Variational inference minimizes the reverse Kullback–Leibler divergence between some approximation <span class="math inline">\(q\)</span> and the target <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y});\)</span> it thus places significant mass in areas of the <span class="math inline">\(\boldsymbol{\Theta}\)</span> where <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span> is small or near zero (due to the log term) unless the approximation vanishes there. Qualitatively, the approximation will be very different from minimizing the Kullback–Leibler divergence <span class="math inline">\(\mathsf{KL}\{p(\boldsymbol{\theta} \mid \boldsymbol{y}) \parallel q\}\)</span> <span class="math display">\[\begin{align*}
\mathrm{argmin}_{\boldsymbol{\psi}}  \int_{\boldsymbol{\Theta}}\log \left(\frac{p(\boldsymbol{\theta} \mid \boldsymbol{y}) }{q(\boldsymbol{\theta}; \boldsymbol{\psi})}\right) p(\boldsymbol{\theta} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\theta}.
\end{align*}\]</span> One can show that, if we approximate the posterior <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span> by a Gaussian approximation <span class="math inline">\(q(; \boldsymbol{\psi})\)</span>, the parameters that minimize the Kullback–Leibler divergence are the posterior mean and posterior variance of the <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y}).\)</span> Solving this problem exactly typically won’t be feasible, as this is the main reason to consider approximations in the first place.</p>
<p>Expectation propagation is an approximation algorithm proposed by <span class="citation" data-cites="Minka:2001">Minka (<a href="references.html#ref-Minka:2001" role="doc-biblioref">2001</a>)</span> that tries to tackle the problem of minimizing the Kullback–Leibler divergence with distributions from exponential family; we will restrict attention here to Gaussian approximating functions. Expectation propagation is more accurate, but generally slower than variational Bayes, although it is quite fast when implemented properly and it is parallelizable. EP builds on a decomposition of the posterior as a product of terms; typically, we have likelihood contributions <span class="math inline">\(L_i(\boldsymbol{\theta})\)</span> (called “factors” or “sites” in the EP lingo) for independent data, where <span class="math display">\[\begin{align*}
p(\boldsymbol{\theta} \mid \boldsymbol{y}) \propto p(\boldsymbol{\theta}) \prod_{i=1}^n L_i(\boldsymbol{\theta}) = \prod_{i=0}^n L_i(\boldsymbol{\theta})
\end{align*}\]</span> with the convention that <span class="math inline">\(L_0(\boldsymbol{\theta})\)</span> equals the prior density. Such factorization is also feasible in graphical models (e.g., autoregressive processes, Markov fields), but needs not be unique. Note that it is not equivalent to the factorization of the posterior (mean-field approximation) for variational Bayes; every term in the EP approximation is a function of the whole vector <span class="math inline">\(\boldsymbol{\theta}.\)</span></p>
<p>The expectation propagation considers a factor structure approximation, in which each <span class="math inline">\(q_i\)</span> is Gaussian with precision <span class="math inline">\(\mathbf{Q}_i\)</span> and linear shift <span class="math inline">\(\boldsymbol{r}_i\)</span>, <span class="math display">\[\begin{align*}
q(\boldsymbol{\theta}) &amp;\propto \prod_{i=1}^n q_i(\boldsymbol{\theta})
\\&amp; \propto \prod_{i=0}^n \exp \left(-\frac{1}{2} \boldsymbol{\theta}^\top\mathbf{Q}_i\boldsymbol{\theta} + \boldsymbol{\theta}^\top\boldsymbol{r}_i\right)
\\ &amp;= \exp \left( - \frac{1}{2} \boldsymbol{\theta}^\top \sum_{i=0}^n\mathbf{Q}_i\boldsymbol{\theta} + \boldsymbol{\theta}^\top \sum_{i=0}^n\boldsymbol{r}_i\right)
\end{align*}\]</span> and so the global approximation is also Gaussian; the last line also holds for distributions in the exponential family, where <span class="math inline">\(-0.5\boldsymbol{\Sigma}^{-1}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu} = \boldsymbol{r}\)</span> are the canonical parameters of the Gaussian distribution.</p>
<p>Expectation propagation works by starting with the joint approximation and removing one site to get the cavity <span class="math display">\[q_{-j}(\boldsymbol{\theta}) = \prod_{\substack{i = 0\\i \neq j}}^n q_{i}(\boldsymbol{\theta});\]</span> in practice, this is most easily done by subtracting the term from the approximation of the canonical parameters. We replace then the missing term by <span class="math inline">\(L_j(\boldsymbol{\theta})\)</span> to construct the so-called hybrid density <span class="math inline">\(h_j(\boldsymbol{\theta}) = L_j(\boldsymbol{\theta})q_{-j}(\boldsymbol{\theta}).\)</span> The resulting density is unnormalized, but closer to a sense to the target posterior than is <span class="math inline">\(q(\boldsymbol{\theta}).\)</span> We can start from this and compute a Gaussian approximation to minimize the Kullback–Leibler distance with <span class="math inline">\(\mathsf{KL}(h_j \parallel q^*_j)\)</span> by matching moments, where <span class="math display">\[\begin{align*}
c_j &amp;= \int h_j(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} \\
\boldsymbol{\mu}_j &amp;= c_{j}^{-1} \int \boldsymbol{\theta} h_j(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
\\ \boldsymbol{\Sigma}_j &amp;= c_j^{-1} (\boldsymbol{\theta} - \boldsymbol{\mu}_j)(\boldsymbol{\theta} - \boldsymbol{\mu}_j)^\top h_j(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
\end{align*}\]</span> The normalizing constant, mean and variance in the above are written in terms of <span class="math inline">\(p\)</span>-dimensional integrals but could be easily obtained by Monte Carlo by drawing from the cavity prior. We then go back from moments to canonical parameters <span class="math inline">\(\mathbf{Q}_j\)</span> and <span class="math inline">\(\boldsymbol{r}_j\)</span> and update the global approximation <span class="math inline">\(q.\)</span> These updates can be performed sequentially or in parallel, which can lead to massive gains in efficiency.</p>
<div id="exm-logistic-ep" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.1 (Logistic regression)</strong></span> Consider a logistic regression model where we code successes <span class="math inline">\(Y=1\)</span> and failures <span class="math inline">\(Y=-1;\)</span> for a given row vector <span class="math inline">\(\mathbf{x}\)</span> of length <span class="math inline">\(p\)</span> and the vector <span class="math inline">\(\boldsymbol{\beta} \in \mathbb{R}^p\)</span> of coefficients, the probability of success is <span class="math display">\[\begin{align*}
\Pr(Y=1 \mid \mathbf{x}, \boldsymbol{\beta}) = \left\{1+\exp(-\mathbf{x}\boldsymbol{\beta})\right\}^{-1} = \mathrm{expit}(\mathbf{x}\boldsymbol{\beta}).
\end{align*}\]</span> The term on the right is the logistic function. This reparametrization makes it easier to write the likelihood contribution of observation <span class="math inline">\(i\)</span> as <span class="math display">\[L_i(\boldsymbol{\beta}) = \mathrm{expit}(y_i \mathbf{x}_i\boldsymbol{\beta}).\]</span> If we have the approximation for <span class="math inline">\(\boldsymbol{\beta} \sim \mathsf{Gauss}_p(\boldsymbol{\mu}, \boldsymbol{\Sigma}),\)</span> then the integrals for the normalizing constant, etc. depend only on the linear combination <span class="math inline">\(\mathbf{x}_i^\top\boldsymbol{\beta} \sim \mathsf{Gauss}(y_i \mathbf{x}_i^\top\boldsymbol{\mu}, y_i \mathbf{x}_i^\top\boldsymbol{\Sigma}\mathbf{x}_i).\)</span> This means all integrals for the tilting problem are unidimensional and can be obtained by Gaussian quadrature or similar numerical integration schemes.</p>
<p>Given the mean and variance of the cavity, denoted <span class="math inline">\(\boldsymbol{\mu}_{-j}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_{-j},\)</span> and with <span class="math inline">\(s_j = \mathbf{x}_j\boldsymbol{\Sigma}_{-j}\mathbf{x}_j^\top\)</span>, the updated mean and variance are <span class="math display">\[\begin{align*}
\boldsymbol{\mu}_j^* &amp;= \boldsymbol{\mu}_{-j} s_j^{-1}\boldsymbol{\Sigma}_{-j}\mathbf{x}^\top \{\mathbf{E}(Z_j) -\mathbf{x}_j\boldsymbol{\mu}_{-j}\} \\
\boldsymbol{\Sigma}_{j} &amp;=\boldsymbol{\Sigma}_{-j} + s_j^{-2}\boldsymbol{\Sigma}_{-j}\mathbf{x}^\top_j\left\{\mathsf{Var}(Z_j) - s_j\right\}\mathbf{x}_j\boldsymbol{\Sigma}_{-j}
\end{align*}\]</span> where <span class="math inline">\(Z_j\)</span> has distribution proportional to <span class="math inline">\(L_j(z) \times \phi(z; \mathbf{x}_j\boldsymbol{\mu}_j, \mathbf{x}_j\boldsymbol{\Sigma}_{-j}\mathbf{x}_j^\top),\)</span> where <span class="math inline">\(\phi(\cdot; \mu, \sigma^2)\)</span> denotes the density of a Gaussian random variable. The updates to the parameters for more general exponential families are found in page 23 of <span class="citation" data-cites="Cseke.Heskes:2011">Cseke and Heskes (<a href="references.html#ref-Cseke.Heskes:2011" role="doc-biblioref">2011</a>)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param mu_lc mean of the linear combination</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param sd_lc std. dev of the linear combination</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>ep_update <span class="ot">&lt;-</span> <span class="cf">function</span>(y, mu_lc, sd_lc){</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate outside of the loop the cavity</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  fn <span class="ot">&lt;-</span> <span class="cf">function</span>(x){ <span class="fu">dnorm</span>(x, <span class="at">mean =</span> mu_lc, <span class="at">sd =</span> sd_lc)<span class="sc">*</span><span class="fu">plogis</span>(y<span class="sc">*</span>x)}</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute normalizing constant</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  cst <span class="ot">&lt;-</span> <span class="fu">integrate</span>(<span class="at">f =</span> fn, <span class="at">lower =</span> <span class="sc">-</span><span class="cn">Inf</span>, <span class="at">upper =</span> <span class="cn">Inf</span>)<span class="sc">$</span>value</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> <span class="fu">integrate</span>(<span class="at">f =</span> <span class="cf">function</span>(x){<span class="fu">fn</span>(x)<span class="sc">*</span>x}, <span class="sc">-</span><span class="cn">Inf</span>, <span class="cn">Inf</span>)<span class="sc">$</span>value<span class="sc">/</span>cst</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  va <span class="ot">&lt;-</span> <span class="fu">integrate</span>(<span class="at">f =</span> <span class="cf">function</span>(x){<span class="fu">fn</span>(x)<span class="sc">*</span>(x<span class="sc">-</span>mu)<span class="sc">^</span><span class="dv">2</span>}, <span class="sc">-</span><span class="cn">Inf</span>, <span class="cn">Inf</span>)<span class="sc">$</span>value<span class="sc">/</span>cst</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In generalized linear models, the linear predictor will always be one-dimensional, and more generally, in exponential families, we can get a similar dimension reduction.</p>
</div>
<p>The EP algorithm is particularly well suited to latent Gaussian models <span class="citation" data-cites="Cseke.Heskes:2011">(<a href="references.html#ref-Cseke.Heskes:2011" role="doc-biblioref">Cseke and Heskes 2011</a>)</span> and generalized linear models with Gaussian priors for the coefficients.</p>
<p>The EP algorithm iterates the steps until convergence:</p>
<ol type="1">
<li>Initialize the site-specific parameters</li>
<li>Loop over each observation of the likelihood factorization: 2.1 form the cavity and the hybrid distribution 2.2 compute the moments of the hybrid <span class="math inline">\(\boldsymbol{\mu}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> 2.3 transform back to canonical parameters <span class="math inline">\(\mathbf{Q}\)</span> and <span class="math inline">\(\boldsymbol{r}\)</span> 2.3 update the global approximation</li>
<li>Declare convergence when change in parameters is less than tolerance.</li>
</ol>
<p>We can monitor convergence of EP by looking at changes in the parameters: it is a fixed point algorithm. However, the EP does not have guarantees of convergence and may diverge; <span class="citation" data-cites="Dehaene.Barthelme:2018">Dehaene and Barthelmé (<a href="references.html#ref-Dehaene.Barthelme:2018" role="doc-biblioref">2018</a>)</span> explain some heuristics for these by making analogies with Newton’s method, which only converges in the neighbourhood of fixed points and can diverge. The idea is to update the global approximation with a damping term for the canonical parameters. It is also useful to perform updates using suitable numerical linear algebra routines, for example by competing the Cholesky root of the covariance and back-solving to get the precision.</p>
<p>The expensive steps are related to the inversion of the moments to get the canonical parameters from the moments; the matrix inversion has complexity <span class="math inline">\(\mathrm{O}(np^3)\)</span> for a single pass, but the algorithm typically converges quicky. The other bottleneck is calculation of the moments.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Cseke.Heskes:2011" class="csl-entry" role="listitem">
Cseke, Botond, and Tom Heskes. 2011. <span>“Approximate Marginals in Latent <span>G</span>aussian Models.”</span> <em>Journal of Machine Learning Research</em> 12 (13): 417–54. <a href="http://jmlr.org/papers/v12/cseke11a.html">http://jmlr.org/papers/v12/cseke11a.html</a>.
</div>
<div id="ref-Dehaene.Barthelme:2018" class="csl-entry" role="listitem">
Dehaene, Guillaume, and Simon Barthelmé. 2018. <span>“Expectation Propagation in the Large Data Limit.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 80 (1): 199–217. <a href="https://doi.org/10.1111/rssb.12241">https://doi.org/10.1111/rssb.12241</a>.
</div>
<div id="ref-Liu:1994" class="csl-entry" role="listitem">
Liu, Jun S. 1994. <span>“<span>S</span>iegel’s Formula via <span>S</span>tein’s Identities.”</span> <em>Statistics &amp; Probability Letters</em> 21 (3): 247–51. <a href="https://doi.org/10.1016/0167-7152(94)90121-X">https://doi.org/10.1016/0167-7152(94)90121-X</a>.
</div>
<div id="ref-Minka:2001" class="csl-entry" role="listitem">
Minka, Thomas P. 2001. <span>“A Family of Algorithms for Approximate <span>B</span>ayesian Inference.”</span> PhD thesis, Massachusetts Institute of Technology. <a href="http://hdl.handle.net/1721.1/86583">http://hdl.handle.net/1721.1/86583</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lbelzile\.github\.io\/MATH80601A\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./variational.html" class="pagination-link" aria-label="Variational inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Variational inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">References</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2024-2025, Léo Belzile</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/lbelzile/MATH80601A/edit/main/expectationpropagation.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>