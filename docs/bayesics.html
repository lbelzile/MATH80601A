<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Bayesics – Bayesian modelling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./priors.html" rel="next">
<link href="./introduction.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2bb0ec5e928ee8c40b12725cb7836c35.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a51e8ad160b68d9f8f1ac488cc0f242e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./bayesics.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/MATH80601A" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH80601A-bayesmod.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesics.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./montecarlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Monte Carlo methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Metropolis–Hastings algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gibbs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Gibbs sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Computational strategies and diagnostics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regression models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./laplace.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Deterministic approximations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./variational.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Variational inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./expectationpropagation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Expectation propagation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#probability-and-frequency" id="toc-probability-and-frequency" class="nav-link active" data-scroll-target="#probability-and-frequency"><span class="header-section-number">2.1</span> Probability and frequency</a></li>
  <li><a href="#posterior-distribution" id="toc-posterior-distribution" class="nav-link" data-scroll-target="#posterior-distribution"><span class="header-section-number">2.2</span> Posterior distribution</a></li>
  <li><a href="#posterior-predictive-distribution" id="toc-posterior-predictive-distribution" class="nav-link" data-scroll-target="#posterior-predictive-distribution"><span class="header-section-number">2.3</span> Posterior predictive distribution</a></li>
  <li><a href="#summarizing-posterior-distributions" id="toc-summarizing-posterior-distributions" class="nav-link" data-scroll-target="#summarizing-posterior-distributions"><span class="header-section-number">2.4</span> Summarizing posterior distributions</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/lbelzile/MATH80601A/edit/main/bayesics.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><h1 class="title display-7"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesics</span></h1></header>

<header id="title-block-header">


</header>


<p>The Bayesian paradigm is an inferential framework that is widely used in data science. <!--It's popularity increased dramatically thanks to the Markov chain Monte Carlo in the 1990's, which allowed for numerical estimation of models that were. 
--> It builds on likelihood-based inference, offers a natural framework for prediction and for uncertainty quantification. The interpretation is more natural than that of classical (i.e., frequentist) paradigm, and it is more easy to generalized models to complex settings, notably through hierarchical constructions. The main source of controversy is the role of the prior distribution, which allows one to incorporate subject-matter expertise but leads to different inferences being drawn by different practitioners; this subjectivity is not to the taste of many and has been the subject of many controversies.</p>
<p>The Bayesian paradigm includes multiples notions that are not covered in undergraduate introductory courses. The purpose of this chapter is to introduce these concepts and put them in perspective; the reader is assumed to be familiar with basics of likelihood-based inference. We begin with a discussion of the notion of probability, then define priors, posterior distributions, marginal likelihood and posterior predictive distributions. We focus on the interpretation of posterior distributions and explain how to summarize the posterior, leading leading to definitions of high posterior density region, credible intervals, posterior mode for cases where we either have a (correlated) sample from the posterior, or else have access to the whole distribution. Several notions, including sequentiality, prior elicitation and estimation of the marginal likelihood, are mentioned in passing. A brief discussion of Bayesian hypothesis testing (and alternatives) is presented.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<strong>Learning objectives</strong>:
</div>
</div>
<div class="callout-body-container callout-body">
<p>At the end of the chapter, students should be able to</p>
<ul>
<li>define key notions (prior, marginal likelihood, Bayes factor, credible intervals, etc.) of Bayesian inference.</li>
<li>distinguish between questions that relate to the posterior distribution versus the posterior predictive.</li>
<li>calculate and compute numerically summaries of posterior distributions (point estimators and credible intervals) given a posterior sample.</li>
<li>explain some of the conceptual differences between frequentist and Bayesian inference.</li>
</ul>
</div>
</div>
<section id="probability-and-frequency" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="probability-and-frequency"><span class="header-section-number">2.1</span> Probability and frequency</h2>
<p>In classical (frequentist) parametric statistic, we treat observations <span class="math inline">\(\boldsymbol{Y}\)</span> as realizations of a distribution whose parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> are unknown. All of the information about parameters is encoded by the likelihood function.</p>
<p>The interpretation of probability in the classical statistic is in terms of long run frequency, which is why we term this approach frequentist statistic. Think of a fair die: when we state that values <span class="math inline">\(\{1, \ldots, 6\}\)</span> are equiprobable, we mean that repeatedly tossing the die should result, in large sample, in each outcome being realized roughly <span class="math inline">\(1/6\)</span> of the time (the symmetry of the object also implies that each facet should be equally likely to lie face up). This interpretation also carries over to confidence intervals: a <span class="math inline">\((1-\alpha)\)</span> confidence interval either contains the true parameter value or it doesn’t, so the probability level <span class="math inline">\((1-\alpha)\)</span> is only the long-run proportion of intervals created by the procedure that should contain the true fixed value, not the probability that a single interval contains the true value. This is counter-intuitive to most.</p>
<p>In practice, the true value of the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> vector is unknown to the practitioner, thus uncertain: Bayesians would argue that we should treat the latter as a random quantity rather than a fixed constant. Since different people may have different knowledge about these potential values, the prior knowledge is a form of <strong>subjective probability</strong>. For example, if you play cards, one person may have recorded the previous cards that were played, whereas other may not. They thus assign different probability of certain cards being played. In Bayesian inference, we consider <span class="math inline">\(\boldsymbol{\theta}\)</span> as random variables to reflect our lack of knowledge about potential values taken. Italian scientist Bruno de Finetti, who is famous for the claim ``Probability does not exist’’, stated in the preface of <span class="citation" data-cites="deFinetti:1974">Finetti (<a href="references.html#ref-deFinetti:1974" role="doc-biblioref">1974</a>)</span>:</p>
<blockquote class="blockquote">
<p>Probabilistic reasoning — always to be understood as subjective — merely stems from our being uncertain about something. It makes no difference whether the uncertainty relates to an unforseeable future, or to an unnoticed past, or to a past doubtfully reported or forgotten: it may even relate to something more or less knowable (by means of a computation, a logical deduction, etc.) but for which we are not willing or able tho make the effort; and so on […] The only relevant thing is uncertainty — the extent of our knowledge and ignorance. The actual fact of whether or not the events considered are in some sense <em>determined</em>, or known by other people, and so on, is of no consequence.</p>
</blockquote>
<p>On page 3, de Finetti continues <span class="citation" data-cites="deFinetti:1974">(<a href="references.html#ref-deFinetti:1974" role="doc-biblioref">Finetti 1974</a>)</span></p>
<blockquote class="blockquote">
<p>only subjective probabilities exist — i.e., the degree of belief in the occurrence of an event attributed by a given person at a given instant and with a given set of information.</p>
</blockquote>
</section>
<section id="posterior-distribution" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="posterior-distribution"><span class="header-section-number">2.2</span> Posterior distribution</h2>
<p>We consider a parametric model with parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> defined on <span class="math inline">\(\boldsymbol{\Theta} \subseteq \mathbb{R}^p.\)</span> In Bayesian learning, we adjoin to the likelihood <span class="math inline">\(\mathcal{L}(\boldsymbol{\theta}; \boldsymbol{y}) \equiv p(\boldsymbol{y} \mid \boldsymbol{\theta})\)</span> a <strong>prior</strong> function <span class="math inline">\(p(\boldsymbol{\theta})\)</span> that reflects the prior knowledge about potential values taken by the <span class="math inline">\(p\)</span>-dimensional parameter vector, before observing the data <span class="math inline">\(\boldsymbol{y}.\)</span> The prior makes <span class="math inline">\(\boldsymbol{\theta}\)</span> random and the distribution of the parameter reflects our uncertainty about the true value of the model parameters.</p>
<p>In a Bayesian analysis, observations are random variables but inference is performed conditional on the observed sample values. By Bayes’ theorem, our target is therefore the posterior density <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y}),\)</span> defined as</p>
<p><span id="eq-posterior"><span class="math display">\[
\underbracket[0.25pt]{p(\boldsymbol{\theta} \mid \boldsymbol{y})}_{\text{posterior}} = \frac{\overbracket[0.25pt]{p(\boldsymbol{y} \mid \boldsymbol{\theta})}^{\text{likelihood}} \times  \overbracket[0.25pt]{p(\boldsymbol{\theta})}^{\text{prior}}}{\underbracket[0.25pt]{\int p(\boldsymbol{y} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}}_{\text{marginal likelihood }p(\boldsymbol{y})}}.
\tag{2.1}\]</span></span></p>
<p>The posterior <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span> is proportional, as a function of <span class="math inline">\(\theta,\)</span> to the product of the likelihood and the prior function.</p>
<p>For the posterior to be <strong>proper</strong>, we need the product of the prior and the likelihood on the right hand side to be integrable as a function of <span class="math inline">\(\boldsymbol{\theta}\)</span> over the parameter domain <span class="math inline">\(\boldsymbol{\Theta}.\)</span> The integral in the denominator, termed marginal likelihood or prior predictive distribution and denoted <span class="math inline">\(p(\boldsymbol{y}) = \mathsf{E}_{\boldsymbol{\theta}}\{p(\boldsymbol{y} \mid \boldsymbol{\theta})\}.\)</span> It represents the distribution of the data before data collection, the respective weights being governed by the prior probability of different parameters values. The denominator of <a href="#eq-posterior" class="quarto-xref">Equation&nbsp;<span>2.1</span></a> is a normalizing constant, making the posterior density integrate to unity. The marginal likelihood plays a central role in Bayesian testing.</p>
<p>If <span class="math inline">\(\boldsymbol{\theta}\)</span> is low dimensional, numerical integration such as quadrature methods can be used to compute the marginal likelihood.</p>
<p>To fix ideas, we consider next a simple one-parameter model where the marginal likelihood can be computed explicitly.</p>
<div id="exm-betabinomconjugate" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Binomial model with beta prior)</strong></span> Consider a binomial likelihood with probability of success <span class="math inline">\(\theta \in [0,1]\)</span> and <span class="math inline">\(n\)</span> trials, <span class="math inline">\(Y \sim \mathsf{binom}(n, \theta).\)</span> If we take a beta prior, <span class="math inline">\(\theta \sim \mathsf{beta}(\alpha, \beta)\)</span> and observe <span class="math inline">\(y\)</span> successes, the posterior is <span class="math display">\[\begin{align*}
p(\theta \mid y = y) &amp;\propto \binom{n}{y} \theta^y (1-\theta)^{n-y} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}\theta^{\alpha-1} (1-\theta)^{\beta-1}
\\&amp;\stackrel{\theta}{\propto} \theta^{y+\alpha-1}(1-\theta)^{n-y+\beta-1}
\end{align*}\]</span> and is <span class="math display">\[\int_{0}^{1} \theta^{y+\alpha-1}(1-\theta)^{n-y+\beta-1}\mathrm{d} \theta = \frac{\Gamma(y+\alpha)\Gamma(n-y+\beta)}{\Gamma(n+\alpha+\beta)},\]</span> a beta function. Since we need only to keep track of the terms that are function of the parameter <span class="math inline">\(\theta,\)</span> we could recognize directly that the posterior distribution is <span class="math inline">\(\mathsf{beta}(y+\alpha, n-y+\beta)\)</span> and deduce the normalizing constant from there.</p>
<p>If <span class="math inline">\(Y \sim \mathsf{binom}(n, \theta),\)</span> the expected number of success is <span class="math inline">\(n\theta\)</span> and the expected number of failures <span class="math inline">\(n(1-\theta)\)</span> and so the likelihood contribution, relative to the prior, will dominate as the sample size <span class="math inline">\(n\)</span> grows.</p>
<p>Another way to see this is to track moments (expectation, variance, etc.) From <a href="introduction.html#def-beta" class="quarto-xref">Definition&nbsp;<span>1.3</span></a>, the posterior mean is <span class="math display">\[\begin{align*}
\mathsf{E}(\theta \mid y) = w\frac{y}{n} + (1-w) \frac{\alpha}{\alpha+\beta},
\qquad w = \frac{n}{n+\alpha + \beta},
\end{align*}\]</span> a weighted average of the maximum likelihood estimator and the prior mean. We can think of the parameter <span class="math inline">\(\alpha\)</span> (respectively <span class="math inline">\(\beta\)</span>) as representing the fixed prior number of success (resp. failures). The variance term is <span class="math inline">\(\mathrm{O}(n^{-1})\)</span> and, as the sample size increases, the likelihood weight <span class="math inline">\(w\)</span> dominates.</p>
<p><a href="#fig-betabinom" class="quarto-xref">Figure&nbsp;<span>2.1</span></a> shows three different posterior distributions with different beta priors: the first prior, which favors values closer to 1/2, leads to a more peaked posterior density, contrary to the second which is symmetric, but concentrated toward more extreme values near endpoints of the support. The rightmost panel is truncated: as such, the posterior is zero for any value of <span class="math inline">\(\theta\)</span> beyond 1/2 and so the posterior mode may be close to the endpoint of the prior. The influence of such a prior will not necessarily vanish as sample size and should be avoided, unless there are compelling reasons for restricting the domain.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-betabinom" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-betabinom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesics_files/figure-html/fig-betabinom-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-betabinom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: Scaled binomial likelihood for six successes out of 14 trials, with <span class="math inline">\(\mathsf{beta}(3/2, 3/2)\)</span> prior (left), <span class="math inline">\(\mathsf{beta}(1/4, 1/4)\)</span> (middle) and truncated uniform on <span class="math inline">\([0,1/2]\)</span> (right), with the corresponding posterior distributions.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em> (Proportionality). </span>Any term appearing in the likelihood times prior function that does not depend on parameters can be omitted since they will be absorbed by the normalizing constant. This makes it useful to compute normalizing constants or likelihood ratios.</p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>An alternative parametrization for the beta distribution sets <span class="math inline">\(\alpha=\mu \kappa,\)</span> <span class="math inline">\(\beta = (1-\mu)\kappa\)</span> for <span class="math inline">\(\mu \in (0,1)\)</span> and <span class="math inline">\(\kappa&gt;0,\)</span> so that the model is parametrized directly in terms of mean <span class="math inline">\(\mu,\)</span> with <span class="math inline">\(\kappa\)</span> capturing the dispersion.</p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>A density integrates to 1 over the range of possible outcomes, but there is no guarantee that the likelihood function, as a function of <span class="math inline">\(\boldsymbol{\theta},\)</span> integrates to one over the parameter domain <span class="math inline">\(\boldsymbol{\Theta}.\)</span></p>
<p>For example, the binomial likelihood with <span class="math inline">\(n\)</span> trials and <span class="math inline">\(y\)</span> successes satisfies <span class="math display">\[\int_0^1 \binom{n}{y}\theta^y(1-\theta)^{n-y} \mathrm{d} \theta = \frac{1}{n+1}.\]</span></p>
<p>Moreover, the binomial distribution is discrete with support <span class="math inline">\(0, \ldots, n,\)</span> whereas the likelihood is continuous as a function of the probability of success, as evidenced by <a href="#fig-binom-massvslik" class="quarto-xref">Figure&nbsp;<span>2.2</span></a></p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-binom-massvslik" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-binom-massvslik-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesics_files/figure-html/fig-binom-massvslik-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-binom-massvslik-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Binomial mass function (left) and scaled likelihood function (right).
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="def-bayes-factor" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 (Bayes factor and model comparison)</strong></span> The marginal likelihood enters in the comparison of different models. Suppose that we have models <span class="math inline">\(\mathcal{M}_m\)</span> <span class="math inline">\((m=1, \ldots, M)\)</span> to be compared, with parameter vectors <span class="math inline">\(\boldsymbol{\theta}^{(m)}\)</span> and data vector <span class="math inline">\(\boldsymbol{y}.\)</span> Consider <span class="math inline">\(p_m =\Pr(\mathcal{M}_m)\)</span> the prior probability of the different models under consideration, with <span class="math inline">\(p_1 + \cdots + p_M = 1.\)</span> The posterior odds for Models <span class="math inline">\(\mathcal{M}_i\)</span> vs <span class="math inline">\(\mathcal{M}_j\)</span> are <span class="math display">\[\begin{align*}
\underbracket[0.25pt]{\frac{\Pr(\mathcal{M}_i \mid \boldsymbol{y})}{\Pr(\mathcal{M}_j \mid \boldsymbol{y})}}_{\text{posterior odds}} = \underbracket[0.25pt]{\frac{p(\boldsymbol{y} \mid \mathcal{M}_i)}{p(\boldsymbol{y} \mid \mathcal{M}_j)}}_{\text{Bayes factor}} \underbracket[0.25pt]{\frac{\Pr(\mathcal{M}_i)}{\Pr(\mathcal{M}_j)}}_{\text{prior odds}}
\end{align*}\]</span> where the first term on the right hand side is the Bayes factor for model <span class="math inline">\(i\)</span> vs <span class="math inline">\(j,\)</span> denoted <span class="math inline">\(\mathsf{BF}_{ij}.\)</span> The Bayes factor is the ratio of marginal likelihoods, as <span class="math display">\[\begin{align*}
p(\boldsymbol{y} \mid \mathcal{M}_i) = \int p(y \mid \boldsymbol{\theta}^{(i)}, \mathcal{M}_i) p( \boldsymbol{\theta}^{(i)} \mid \mathcal{M}_i) \mathrm{d}  \boldsymbol{\theta}^{(i)}.
\end{align*}\]</span> Values of <span class="math inline">\(\mathsf{BF}_{ij}&gt;1\)</span> correspond to model <span class="math inline">\(\mathcal{M}_i\)</span> being more likely than <span class="math inline">\(\mathcal{M}_j.\)</span></p>
<p>While Bayes factors are used for model comparison, the answers depend very strongly on the prior <span class="math inline">\(p( \boldsymbol{\theta}^{(i)} \mid \mathcal{M}_i)\)</span> specified and the latter must be proper as a general rule for the ratio to be well-defined.</p>
<p>The Bayes factor require that we compare the same data, but both likelihood and priors could be different from one model to the next.</p>
</div>
<div id="exm-bayesfactor" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Bayes factor for the binomial model)</strong></span> The marginal likelihood for the <span class="math inline">\(Y \mid P=p \sim \mathsf{binom}(n,p)\)</span> model with prior <span class="math inline">\(P \sim \mathsf{beta}(\alpha, \beta)\)</span> is <span class="math display">\[\begin{align*}
p_{Y}(y) = \binom{n}{y} \frac{\mathrm{beta}(\alpha + y, \beta + n - y)}{\mathrm{beta}(\alpha, \beta)}.
\end{align*}\]</span> where <span class="math inline">\(\mathrm{beta}(\alpha, \beta) = \Gamma(\alpha)\Gamma(\beta)/\Gamma(\alpha+\beta)\)</span> is the beta function, expressed in terms of gamma functions.</p>
<p>Consider three models with <span class="math inline">\(Y \mid P^{(i)}=p, \mathcal{M}_i \sim \mathsf{binom}(n, p)\)</span> for <span class="math inline">\(i=1, 2, 3\)</span> and uniform, point mass and beta priors <span class="math inline">\(P^{(1)}\sim \mathsf{unif}(0,1),\)</span> <span class="math inline">\(P^{(2)} \sim \mathsf{beta}(3/2, 3/2)\)</span> and <span class="math inline">\(P^{(3)}\sim \mathsf{1}_{p=0.5}.\)</span> For <span class="math inline">\(\mathcal{M}_3,\)</span> the marginal likelihood is simply equal to the binomial distribution with <span class="math inline">\(p=0.5.\)</span></p>
<p>If <span class="math inline">\(n=14,\)</span> but we let instead the number of success varies, the models that put more mass closer to the ratio <span class="math inline">\(y/n\)</span> will be favored. The uniform prior in model <span class="math inline">\(\mathcal{M}_1\)</span> will have a higher Bayes factor than model <span class="math inline">\(\mathcal{M}_2\)</span> or <span class="math inline">\(\mathcal{M}_3\)</span> for values closer to <span class="math inline">\(p=0\)</span> or <span class="math inline">\(p=1,\)</span> but there is mild evidence as shown in <a href="#fig-bayesfactor" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Log of marginal posterior for binom with beta prior (default is uniform)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>log_marg_post_beta <span class="ot">&lt;-</span> <span class="cf">function</span>(n, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">beta =</span> <span class="dv">1</span>){</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lchoose</span>(n, y) <span class="sc">+</span> <span class="fu">lbeta</span>(alpha <span class="sc">+</span> y, beta <span class="sc">+</span> n <span class="sc">-</span> y) <span class="sc">-</span> <span class="fu">lbeta</span>(alpha, beta)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Log of Bayes factor</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>logBF2vs3 <span class="ot">&lt;-</span> <span class="cf">function</span>(y, n){ <span class="co"># model 2 (beta(1.5,1.5) vs 3 (point mass at 0.5)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">log_marg_post_beta</span>(<span class="at">n =</span> n, <span class="at">y =</span> y, <span class="at">alpha =</span> <span class="fl">1.5</span>, <span class="at">beta =</span> <span class="fl">1.5</span>) <span class="sc">-</span> <span class="fu">dbinom</span>(<span class="at">x =</span> y, <span class="at">size =</span> n, <span class="at">prob =</span> <span class="fl">0.5</span>, <span class="at">log =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-bayesfactor" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bayesfactor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesics_files/figure-html/fig-bayesfactor-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bayesfactor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Log of Bayes factors for comparison of binomial models with <span class="math inline">\(n=14\)</span> trials as a function of the number of successes <span class="math inline">\(n.\)</span> Values larger than zero (on log scale) indicate preference for Model 2.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="prp-sequentiality" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.1 (Sequentiality and Bayesian updating)</strong></span> The likelihood is invariant to the order of the observations if they are independent. Thus, if we consider two blocks of observations <span class="math inline">\(\boldsymbol{y}_1\)</span> and <span class="math inline">\(\boldsymbol{y}_2\)</span> <span class="math display">\[p(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \boldsymbol{y}_2) = p(\boldsymbol{\theta} \mid \boldsymbol{y}_1) p(\boldsymbol{\theta} \mid \boldsymbol{y}_2),\]</span> so it makes no difference if we treat data all at once or in blocks. More generally, for data exhibiting spatial or serial dependence, it makes sense to consider rather the conditional (sequential) decomposition <span class="math display">\[f(\boldsymbol{y}; \boldsymbol{\theta}) = f(\boldsymbol{y}_1; \boldsymbol{\theta}) f(\boldsymbol{y}_2; \boldsymbol{\theta}, \boldsymbol{y}_1) \cdots f(\boldsymbol{y}_n; \boldsymbol{\theta}, \boldsymbol{y}_1, \ldots, \boldsymbol{y}_{n-1})\]</span> where <span class="math inline">\(f(\boldsymbol{y}_k; \boldsymbol{y}_1, \ldots, \boldsymbol{y}_{k-1})\)</span> denotes the conditional density function given observations <span class="math inline">\(\boldsymbol{y}_1, \ldots, \boldsymbol{y}_{k-1}.\)</span></p>
<p>By Bayes’ rule, we can consider <em>updating</em> the posterior by adding terms to the likelihood, noting that <span class="math display">\[\begin{align*}
p(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \boldsymbol{y}_2) \propto p(\boldsymbol{y}_2 \mid \boldsymbol{y}_1, \boldsymbol{\theta}) p(\boldsymbol{\theta} \mid \boldsymbol{y}_1)
\end{align*}\]</span> which amounts to treating the posterior <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y}_1)\)</span> as a prior. If data are exchangeable, the order in which observations are collected and the order of the belief updating is irrelevant to the full posterior. <a href="#fig-sequential" class="quarto-xref">Figure&nbsp;<span>2.4</span></a> shows how the posterior becomes gradually closer to the scaled likelihood as we increase the sample size, and the posterior mode moves towards the true value of the parameter (here 0.3).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-sequential" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sequential-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesics_files/figure-html/fig-sequential-1.png" class="img-fluid figure-img" width="1152">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sequential-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: Beta posterior and binomial likelihood with a uniform prior for increasing number of observations (from left to right) out of a total of 100 trials.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-numericalintegration" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Numerical integration)</strong></span> While we can calculate analytically the value of the normalizing constant for the beta-binomial model, we could also for arbitrary priors use numerical integration or Monte Carlo methods in the event the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> is low-dimensional.</p>
<p>While estimation of the normalizing constant is possible in simple models, the following highlights some challenges that are worth keeping in mind. In a model for discrete data (that is, assigning probability mass to a countable set of outcomes), the terms in the likelihood are probabilities and thus the likelihood becomes smaller as we gather more observations (since we multiply terms between zero or one). The marginal likelihood term becomes smaller and smaller, so it’s reciprocal is big and this can lead to arithmetic underflow.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">6</span>L <span class="co"># number of successes </span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">14</span>L <span class="co"># number of trials</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> beta <span class="ot">&lt;-</span> <span class="fl">1.5</span> <span class="co"># prior parameters</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>unnormalized_posterior <span class="ot">&lt;-</span> <span class="cf">function</span>(theta){</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  theta<span class="sc">^</span>(y<span class="sc">+</span>alpha<span class="dv">-1</span>) <span class="sc">*</span> (<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span>(n<span class="sc">-</span>y <span class="sc">+</span> beta <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="at">f =</span> unnormalized_posterior,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>          <span class="at">lower =</span> <span class="dv">0</span>,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">upper =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.066906e-05 with absolute error &lt; 1e-12</code></pre>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare with known constant</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">beta</span>(y <span class="sc">+</span> alpha, n <span class="sc">-</span> y <span class="sc">+</span> beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.066906e-05</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo integration</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">unnormalized_posterior</span>(<span class="fu">runif</span>(<span class="fl">1e5</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.064067e-05</code></pre>
</div>
</div>
</div>
<p>When <span class="math inline">\(\boldsymbol{\theta}\)</span> is high-dimensional, the marginal likelihood is intractable. This is one of the main challenges of Bayesian statistics and the popularity and applicability has grown drastically with the development and popularity of numerical algorithms, following the publication of <span class="citation" data-cites="Geman.Geman:1984">Geman and Geman (<a href="references.html#ref-Geman.Geman:1984" role="doc-biblioref">1984</a>)</span> and <span class="citation" data-cites="Gelfand.Smith:1990">Gelfand and Smith (<a href="references.html#ref-Gelfand.Smith:1990" role="doc-biblioref">1990</a>)</span>. Markov chain Monte Carlo methods circumvent the calculation of the denominator by drawing approximate samples from the posterior.</p>
<div id="exm-Duke-Amir" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (Importance of selling format)</strong></span> <span class="citation" data-cites="Duke.Amir:2023">Duke and Amir (<a href="references.html#ref-Duke.Amir:2023" role="doc-biblioref">2023</a>)</span> consider the difference between integrated and sequential format for sales. The <code>sellingformat</code> dataset contains <span class="math inline">\(n=397\)</span> observations split into two groups: quantity-integrated decision (decide the amount to buy) and quantity-sequential (first select buy, then select the amount). Participants of the study were randomly allocated to either of these two format and their decision, either buy, <code>1</code>, or do not buy <code>0</code>, is recorded.</p>
<div class="cell">
<div id="tbl-contingency-sellingformat" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-contingency-sellingformat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.1: Aggregated data from Duke and Amir (2023), experiment 1. Number of participants who did not (<code>0</code>) or did buy (<code>1</code>) products as a function of experimental condition.
</figcaption>
<div aria-describedby="tbl-contingency-sellingformat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">0</th>
<th style="text-align: right;">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">quantity-integrated</td>
<td style="text-align: right;">152</td>
<td style="text-align: right;">46</td>
</tr>
<tr class="even">
<td style="text-align: left;">quantity-sequential</td>
<td style="text-align: right;">176</td>
<td style="text-align: right;">23</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>We consider the number of purchased out of the total, treating records as independent Bernoulli observations with a flat (uniform prior).</p>
<p>With a beta-binomial model, the posterior for the probability of buying is <span class="math inline">\(\mathsf{beta}(47, 153)\)</span> for quantity-integrated and <span class="math inline">\(\mathsf{beta}(24, 177)\)</span> for quantity-sequential. We can compute the posterior of the odds ratio, <span class="math display">\[O = \frac{\Pr(Y=1 \mid \texttt{integrated})}{\Pr(Y=0 \mid \texttt{integrated})}\frac{\Pr(Y=0 \mid \texttt{sequential})}{\Pr(Y=1 \mid \texttt{sequential})},\]</span> by simulating independent draws from the posteriors of each condition and computing the odds ratio.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(sellingformat, <span class="at">package =</span> <span class="st">"hecbayes"</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>contingency <span class="ot">&lt;-</span> <span class="fu">with</span>(sellingformat, <span class="fu">table</span>(format, purchased))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior draws of the parameters</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>post_p_int <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="at">n =</span> <span class="fl">1e4</span>, <span class="at">shape1 =</span> <span class="dv">47</span>, <span class="at">shape2 =</span> <span class="dv">153</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>post_p_seq <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="at">n =</span> <span class="fl">1e4</span>, <span class="at">shape1 =</span> <span class="dv">24</span>, <span class="at">shape2 =</span> <span class="dv">177</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Reparametrization</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>post_odds_int <span class="ot">&lt;-</span> (post_p_int <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> post_p_int))</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>post_odds_seq <span class="ot">&lt;-</span> (post_p_seq <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> post_p_seq))</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>post_oddsratio <span class="ot">&lt;-</span> post_odds_int <span class="sc">/</span> post_odds_seq</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><a href="#fig-post-Duke_Amir" class="quarto-xref">Figure&nbsp;<span>2.5</span></a> shows the posterior of the probability of buying for each group, and the odds. It is clear that the integrated format leads to much more sales in the experiment, with a posterior ratio exceeding 1 with probability <span class="math inline">\(99.89\%.\)</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-post-Duke_Amir" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post-Duke_Amir-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesics_files/figure-html/fig-post-Duke_Amir-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post-Duke_Amir-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: Posterior curves per group (left) and odds ratio (right)
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="posterior-predictive-distribution" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="posterior-predictive-distribution"><span class="header-section-number">2.3</span> Posterior predictive distribution</h2>
<p>Prediction in the Bayesian paradigm is obtained by considering the <em>posterior predictive distribution</em>, <span class="math display">\[\begin{align*}
p(y_{\text{new}} \mid \boldsymbol{y}) =
\int_{\Theta} p(y_{\text{new}}  \mid \boldsymbol{\theta}) p(\boldsymbol{\theta} \mid  \boldsymbol{y}) \mathrm{d} \boldsymbol{\theta}
\end{align*}\]</span> In a simulation context, we generate one new dataset (or observation) for each posterior draw <span class="math inline">\(\boldsymbol{\theta}\)</span> from the posterior <span class="math inline">\(p(\boldsymbol{\theta} \mid  \boldsymbol{y}).\)</span></p>
<p>Given draws from the posterior distribution, say <span class="math inline">\(\boldsymbol{\theta}_b\)</span> <span class="math inline">\((b=1, \ldots, B),\)</span> we sample from each a new realization from the distribution appearing in the likelihood <span class="math inline">\(p(y_{\text{new}}  \mid \boldsymbol{\theta}_b).\)</span> This is different from the frequentist setting, which fixes the value of the parameter to some estimate <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span>; by contrast, the posterior predictive, here a beta-binomial distribution <span class="math inline">\(\mathsf{beta binom}(n, \alpha + y, n - y + \beta),\)</span> carries over the uncertainty so will typically be wider and overdispersed relative to the corresponding binomial model. This can be easily seen from the left-panel of <a href="#fig-betabinompostpred" class="quarto-xref">Figure&nbsp;<span>2.6</span></a>, which contrasts the binomial mass function evaluated at the maximum likelihood estimator <span class="math inline">\(\widehat{\theta}=6/14\)</span> with the posterior predictive.</p>
<div class="exm-betabinompred">
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>npost <span class="ot">&lt;-</span> <span class="fl">1e4</span>L</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample draws from the posterior distribution</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>post_samp <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="at">n =</span> npost, y <span class="sc">+</span> alpha, n <span class="sc">-</span> y <span class="sc">+</span> beta)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># For each draw, sample new observation</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>post_pred <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="at">n =</span> npost, <span class="at">size =</span> n, <span class="at">prob =</span> post_samp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-betabinompostpred" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-betabinompostpred-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesics_files/figure-html/fig-betabinompostpred-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-betabinompostpred-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: Beta-binomial posterior predictive distribution with corresponding binomial mass function evaluated at the maximum likelihood estimator.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Given the <span class="math inline">\(\mathsf{beta}(a, b)\)</span> posterior with <span class="math inline">\(a=y + \alpha\)</span> and <span class="math inline">\(b=n-y + \beta,\)</span> the predictive distribution of <span class="math inline">\(Y_{\text{new}}\)</span> for fixed <span class="math inline">\(n_{\text{new}}\)</span> number of trials is beta-binomial with mass function <span class="math display">\[\begin{align*}
p(y_{\text{new}}\mid y) &amp;= \int_0^1 \binom{n_{\text{new}}}{y_{\text{new}}} \frac{\theta^{a + y_{\text{new}}-1}(1-\theta)^{b + n_{\text{new}} - y_{\text{new}}-1}}{
\mathrm{beta}(a, b)}\mathrm{d} \theta
\\&amp;= \binom{n_{\text{new}}}{y_{\text{new}}} \frac{\mathrm{beta}(a + y_{\text{new}}, b + n_{\text{new}} - y_{\text{new}})}{\mathrm{beta}(a, b)}
\end{align*}\]</span></p>
</div>
<div id="exm-normal-post-pred" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5 (Posterior predictive distribution of univariate Gaussian with known mean)</strong></span> Consider an <span class="math inline">\(n\)</span> sample of independent and identically distributed Gaussian, <span class="math inline">\(Y_i \sim \mathsf{Gauss}(0, \tau^{-1})\)</span> (<span class="math inline">\(i=1, \ldots, n\)</span>), where we assign a gamma prior on the precision <span class="math inline">\(\tau \sim \mathsf{gamma}(\alpha, \beta).\)</span> The posterior is <span class="math display">\[\begin{align*}
p(\tau \mid \boldsymbol{y}) \stackrel{\tau}{\propto} \prod_{i=1}^n \tau^{n/2}\exp\left(-\tau \frac{\sum_{i=1}^n{y_i^2}}{2}\right) \times \tau^{\alpha-1} \exp(-\beta \tau)
\end{align*}\]</span> and rearranging the terms to collect powers of <span class="math inline">\(\tau,\)</span> etc. we find that the posterior for <span class="math inline">\(\tau\)</span> must also be gamma, with shape parameter <span class="math inline">\(\alpha^* = \alpha + n/2\)</span> and rate <span class="math inline">\(\beta^* = \beta + \sum_{i=1}^n y_i^2/2.\)</span></p>
<p>The posterior predictive is <span class="math display">\[\begin{align*}
p(y_{\text{new}} \mid \boldsymbol{y}) &amp;= \int_0^\infty \frac{\tau^{1/2}}{(2\pi)^{1/2}}\exp(-\tau y_{\text{new}}^2/2) \frac{\beta^{*\alpha^*}}{\Gamma(\alpha^*)}\tau^{\alpha^*-1}\exp(-\beta^* \tau) \mathrm{d} \tau
\\&amp;= (2\pi)^{-1/2} \frac{\beta^{*\alpha^*}}{\Gamma(\alpha^*)} \int_0^\infty\tau^{\alpha^*-1/2} \exp\left\{- \tau (y_{\text{new}}^2/2 + \beta^*)\right\} \mathrm{d} \tau
\\&amp;= (2\pi)^{-1/2} \frac{\beta^{*\alpha^*}}{\Gamma(\alpha^*)} \frac{\Gamma(\alpha^* + 1/2)}{(y_{\text{new}}^2/2 + \beta^*)^{\alpha^*+1/2}}
\\&amp;= \frac{\Gamma\left(\frac{2\alpha^* + 1}{2}\right)}{\sqrt{2\pi}\Gamma\left(\frac{2\alpha^*}{2}\right)\beta^{*1/2}} \left( 1+ \frac{y_{\text{new}}^2}{2\beta^*}\right)^{-\alpha^*-1/2}
\\&amp;= \frac{\Gamma\left(\frac{2\alpha^* + 1}{2}\right)}{\sqrt{\pi}\sqrt{ 2\alpha^*}\Gamma\left(\frac{2\alpha^*}{2}\right)(\beta^*/\alpha^*)^{1/2}} \left( 1+ \frac{1}{2\alpha^*}\frac{y_{\text{new}}^2}{(\beta^*/\alpha^*)}\right)^{-\alpha^*-1/2}
\end{align*}\]</span> which entails that <span class="math inline">\(Y_{\text{new}}\)</span> is a scaled Student-<span class="math inline">\(t\)</span> distribution with scale <span class="math inline">\((\beta^*/\alpha^*)^{1/2}\)</span> and <span class="math inline">\(2\alpha+n\)</span> degrees of freedom. This example also exemplifies the additional variability relative to the distribution generating the data: indeed, the Student-<span class="math inline">\(t\)</span> distribution is more heavy-tailed than the Gaussian, but since the degrees of freedom increase linearly with <span class="math inline">\(n,\)</span> the distribution converges to a Gaussian as <span class="math inline">\(n \to \infty,\)</span> reflecting the added information as we collect more and more data points and the variance gets better estimated through <span class="math inline">\(\sum_{i=1}^n y_i^2/n.\)</span></p>
</div>
</section>
<section id="summarizing-posterior-distributions" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="summarizing-posterior-distributions"><span class="header-section-number">2.4</span> Summarizing posterior distributions</h2>
<p>The output of the Bayesian learning problem will be either of:</p>
<ol type="1">
<li>a fully characterized distribution</li>
<li>a numerical approximation to the posterior distribution (pointwise)</li>
<li>an exact or approximate sample drawn from the posterior distribution</li>
</ol>
<p>In the first case, we will be able to directly evaluate quantities of interest if there are closed-form expressions for the latter, or else we could draw samples from the distribution and evaluate them via Monte-Carlo. In case of numerical approximations, we will need to resort to numerical integration or otherwise to get our answers.</p>
<p>Often, we will also be interested in the marginal posterior distribution of each component <span class="math inline">\(\theta_j\)</span> in turn (<span class="math inline">\(j=1, \ldots, J\)</span>). To get these, we carry out additional integration steps, <span class="math display">\[p(\theta_j \mid \boldsymbol{y}) = \int p(\boldsymbol{\theta} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\theta}_{-j}.\]</span> With a posterior sample, this is trivial: it suffices to keep the column corresponding to <span class="math inline">\(\theta_j\)</span> and discard the others.</p>
<p>Most of the field of Bayesian statistics revolves around the creation of algorithms that either circumvent the calculation of the normalizing constant (notably using Monte Carlo and Markov chain Monte Carlo methods) or else provide accurate numerical approximation of the posterior pointwise, including for marginalizing out all but one parameters (integrated nested Laplace approximations, variational inference, etc.) The target of inference is the whole posterior distribution, a potentially high-dimensional object which may be difficult to summarize or visualize. We can thus report only characteristics of the the latter.</p>
<p>The choice of point summary to keep has it’s root in decision theory.</p>
<div id="def-lossfunction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 (Loss function)</strong></span> A loss function <span class="math inline">\(c(\boldsymbol{\theta}, \boldsymbol{\upsilon})\)</span> is a mapping from <span class="math inline">\(\mathbb{R}^p \to \mathbb{R}^k\)</span> that assigns a weight to each value of <span class="math inline">\(\boldsymbol{\theta},\)</span> corresponding to the regret or loss arising from choosing this value. The corresponding point estimator <span class="math inline">\(\widehat{\boldsymbol{\upsilon}}\)</span> is the minimizer of the expected loss,</p>
<p><span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\upsilon}} &amp;= \mathop{\mathrm{argmin}}_{\boldsymbol{\upsilon}}\mathsf{E}_{\boldsymbol{\Theta} \mid \boldsymbol{Y}}\{c(\boldsymbol{\theta}, \boldsymbol{v})\} \\&amp;=\mathop{\mathrm{argmin}}_{\boldsymbol{\upsilon}} \int_{\mathbb{R}^d} c(\boldsymbol{\theta}, \boldsymbol{\upsilon})p(\boldsymbol{\theta} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\theta}
\end{align*}\]</span></p>
</div>
<p>For example, in a univariate setting, the quadratic loss <span class="math inline">\(c(\theta, \upsilon) = (\theta-\upsilon)^2\)</span> returns the posterior mean, the absolute loss <span class="math inline">\(c(\theta, \upsilon)=|\theta - \upsilon|\)</span> returns the posterior median and the 0-1 loss <span class="math inline">\(c(\theta, \upsilon) = \mathrm{I}(\upsilon \neq \theta)\)</span> returns the posterior mode.</p>
<p>For example consider the quadratic loss function which is differentiable. Provided we can interchange differential operator and integral sign, <span class="math display">\[\begin{align*}
0&amp;= \int_{\mathbb{R}} \frac{\partial (\upsilon-\theta)^2}{\partial \upsilon} p(\theta \mid \boldsymbol{y}) \mathrm{d} \theta
\\&amp;= \int_{\mathbb{R}} \frac{\partial 2(\upsilon-\theta)}{\partial \upsilon} p(\theta \mid \boldsymbol{y}) \mathrm{d} \theta
\\&amp; = 2\upsilon - 2 \mathsf{E}(\theta)
\end{align*}\]</span> which is minimized when <span class="math inline">\(\widehat{\upsilon}=\mathsf{E}_{\Theta \mid \boldsymbol{Y}}(\theta)\)</span>.</p>
<p>All of these point estimators are central tendency measures, but some may be more adequate depending on the setting as they can correspond to potentially different values, as shown in the left-panel of <a href="#fig-central-moments" class="quarto-xref">Figure&nbsp;<span>2.7</span></a>. The choice is application specific: for multimodal distributions, the mode is likely a better choice.</p>
<p>If we know how to evaluate the distribution numerically, we can optimize to find the mode or else return the value for the pointwise evaluation on a grid at which the density achieves it’s maximum. The mean and median would have to be evaluated by numerical integration if there is no closed-form expression for the latter.</p>
<p>If we have rather a sample from the posterior with associated posterior density values, then we can obtain the mode as the parameter combination with the highest posterior, the median from the value at rank <span class="math inline">\(\lfloor n/2\rfloor\)</span> and the mean through the sample mean of posterior draws.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-central-moments" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-central-moments-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesics_files/figure-html/fig-central-moments-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-central-moments-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: Point estimators from a right-skewed distribution (left) and from a multimodal distribution (right).
</figcaption>
</figure>
</div>
</div>
</div>
<p>The loss function is often a functional (meaning a one-dimensional summary) from the posterior. The following example shows how it reduces a three-dimensional problem into a single risk measure.</p>
<div id="exm-loss-extremes" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6 (Value-at-risk for Danish insurance losses)</strong></span> In extreme value, we are often interested in assessing the risk of events that are rare enough that they lie beyond the range of observed data. To provide a scientific extrapolation, it often is justified to fit a generalized Pareto distribution to exceedances of <span class="math inline">\(Z=Y-u,\)</span> for some user-specified threshold <span class="math inline">\(u\)</span> which is often taken as a large quantile of the distribution of <span class="math inline">\(Z \sim \mathsf{gen. Pareto}(\tau, \xi);\)</span> see <a href="introduction.html#def-gen-pareto" class="quarto-xref">Definition&nbsp;<span>1.11</span></a></p>
<p>Insurance companies provide coverage in exchange for premiums, but need to safeguard themselves against very high claims by buying reinsurance products. These risks are often communicated through the value-at-risk (VaR), a high quantile exceeded with probability <span class="math inline">\(p.\)</span> We model Danish fire insurance claim amounts for inflation-adjusted data collected from January 1980 until December 1990 that are in excess of a million Danish kroner, found in the <code>evir</code> package and analyzed in Example 7.23 of <span class="citation" data-cites="McNeil.Frey.Embrechts:2005">McNeil, Frey, and Embrechts (<a href="references.html#ref-McNeil.Frey.Embrechts:2005" role="doc-biblioref">2005</a>)</span>. These claims are denoted <span class="math inline">\(Y\)</span> and there are 2167 observations.</p>
<p>We fit a generalized Pareto distribution to exceedances above 10 millions krones, keeping 109 observations or roughly the largest 5% of the original sample. Preliminary analysis shows that we can treat data as roughly independent and identically distributed and goodness-of-fit diagnostics (not shown) for the generalized Pareto suggest that the fit is adequate for all but the three largest observations, which are (somewhat severely) underestimated by the model.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-danish" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-danish-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesics_files/figure-html/fig-danish-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-danish-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.8: Time series of Danish fire claims exceeding a million krone (left) and posterior samples from the scale <span class="math inline">\(\tau\)</span> and shape <span class="math inline">\(\xi\)</span> of the generalized Pareto model fitted to exceedances above 10 million krone (right).
</figcaption>
</figure>
</div>
</div>
</div>
<p>The generalized Pareto model only describes the <span class="math inline">\(n_u\)</span> exceedances above <span class="math inline">\(u=10,\)</span> so we need to incorporate in the likelihood a binomial contribution for the probability <span class="math inline">\(\zeta_u\)</span> of exceeding the threshold <span class="math inline">\(u.\)</span> The log likelihood for the full model for <span class="math inline">\(y_i&gt;u\)</span> is <span class="math display">\[\begin{align*}
\ell(\tau, \xi, \zeta_u) &amp;\propto -109 \log \tau + \sum_{i=1}^{109} (1+1/\xi)\log\left(1+\xi\frac{y_i-10}{\tau}\right)_{+} + \\&amp; \quad   109\log \zeta_u + 2058 \log(1-\zeta_u),
\end{align*}\]</span> <!-- and, p --> Provided that the priors for <span class="math inline">\((\tau, \xi)\)</span> are independent of those for <span class="math inline">\(\zeta_u,\)</span> the posterior also factorizes as a product, so <span class="math inline">\(\zeta_u\)</span> and <span class="math inline">\((\tau, \xi)\)</span> are a posteriori independent.</p>
<p>Suppose for now that we set a <span class="math inline">\(\mathsf{beta}(0.5, 0.5)\)</span> prior for <span class="math inline">\(\zeta_u\)</span> and a non-informative prior for the generalized Pareto parameters.</p>
<p>We consider the modelling of insurance losses exceeding <span class="math inline">\(u=10\)</span> millions krones using a generalized Pareto distribution to the <code>danish</code> fire insurance data with some prior; see <a href="introduction.html#def-gen-pareto" class="quarto-xref">Definition&nbsp;<span>1.11</span></a> for the model. The model has three parameters: the scale <span class="math inline">\(\tau\)</span>, the shape <span class="math inline">\(\xi\)</span> and the probability of exceeding the threshold <span class="math inline">\(\zeta_u\)</span>.</p>
<p>Our aim is to evaluate the posterior distribution for the value-at-risk, the <span class="math inline">\(\alpha\)</span> quantile of <span class="math inline">\(Y\)</span> for high values of <span class="math inline">\(\alpha\)</span> and see what point estimator one would obtain depending on our choice of loss function. For any <span class="math inline">\(\alpha &gt; 1-\zeta_u,\)</span> the <span class="math inline">\(q_{\alpha}\)</span> can be written in terms of the generalized Pareto survival function times the probability of exceedance above the threshold, <span class="math display">\[\begin{align*}
1- \alpha  &amp;= \Pr(Y &gt; q_\alpha \mid Y &gt; u) \Pr(Y &gt; u)
\\ &amp;= \left(1+\xi \frac{q_{\alpha}-u}{\tau}\right)_{+}^{-1/\xi}\zeta_u
\end{align*}\]</span> and solving for <span class="math inline">\(q_{\alpha}\)</span> gives <span class="math display">\[\begin{align*}
q_{\alpha} = u+ \frac{\tau}{\xi} \left\{\left(\frac{\zeta_u}{1-\alpha}\right)^\xi-1\right\}.
\end{align*}\]</span> We obtained, using tools that will be discussed in <a href="montecarlo.html#exm-rust" class="quarto-xref">Example&nbsp;<span>4.3</span></a>, a matrix <code>post_samp</code> that contains exact samples from the posterior distribution of <span class="math inline">\((\tau, \xi, \zeta_u).\)</span> To obtain the posterior distribution of the <span class="math inline">\(\alpha\)</span> quantile, <span class="math inline">\(q_{\alpha},\)</span> it thus suffices to plug in each posterior sample and evaluate the function: the uncertainty is carried over from the simulated values of the parameters to those of the quantile <span class="math inline">\(q_{\alpha}.\)</span> The left panel of <a href="#fig-lossfn" class="quarto-xref">Figure&nbsp;<span>2.9</span></a> shows the posterior density estimate of the <span class="math inline">\(\mathsf{VaR}(0.99)\)</span> along with the maximum a posteriori (mode) of the latter.</p>
<p>Suppose that we prefer to under-estimate the value-at-risk rather than overestimate: this could be captured by the custom loss function <span class="math display">\[\begin{align*}
c(q, q_0) =
\begin{cases}
0.5(0.99q - q_0), &amp; q &gt; q_0 \\
0.75(q_0 - 1.01q), &amp; q &lt; q_0.
\end{cases}
\end{align*}\]</span> For a given value of the value-at-risk <span class="math inline">\(q_0\)</span> evaluated on a grid, we thus compute <span class="math display">\[\begin{align*}
r(q_0) = \int_{\boldsymbol{\Theta}}c(q(\boldsymbol{\theta}), q_0) p (\boldsymbol{\theta} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\theta}
\end{align*}\]</span> and we seek to minimize the risk, <span class="math inline">\(\widehat{q} =\mathrm{argmin}_{q_0 \in \mathbb{R}_{+}} r(q_0).\)</span> The value returned that minimizes the loss, shown in <a href="#fig-lossfn" class="quarto-xref">Figure&nbsp;<span>2.9</span></a>, is to the left of the posterior mean for <span class="math inline">\(q_\alpha.\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute value at risk from generalized Pareto distribution quantile fn</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>VaR_post <span class="ot">&lt;-</span> <span class="fu">with</span>(post_samp,   <span class="co"># data frame of posterior draws</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>            revdbayes<span class="sc">::</span><span class="fu">qgp</span>(   <span class="co"># with columns 'probexc', 'scale', 'shape'</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> <span class="fl">0.01</span><span class="sc">/</span>probexc, </span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">loc =</span> <span class="dv">10</span>, </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">scale =</span> scale, </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">shape =</span> shape, </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower.tail =</span> <span class="cn">FALSE</span>))</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss function</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>loss <span class="ot">&lt;-</span> <span class="cf">function</span>(qhat, q){</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mean</span>(<span class="fu">ifelse</span>(q <span class="sc">&gt;</span> qhat,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>           <span class="fl">0.5</span><span class="sc">*</span>(<span class="fl">0.99</span><span class="sc">*</span>q<span class="sc">-</span>qhat),</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>           <span class="fl">0.75</span><span class="sc">*</span>(qhat<span class="fl">-1.01</span><span class="sc">*</span>q)))</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a grid of values over which to estimate the loss for VaR</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>nvals <span class="ot">&lt;-</span> <span class="dv">101</span>L</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>VaR_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">from =</span> <span class="fu">quantile</span>(VaR_post, <span class="fl">0.01</span>),</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">to =</span> <span class="fu">quantile</span>(VaR_post, <span class="fl">0.99</span>), </span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">length.out =</span> nvals)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a container to store results</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>risk <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="at">length =</span> nvals)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq_len</span>(nvals)){</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute integral (Monte Carlo average over draws)</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a> risk[i] <span class="ot">&lt;-</span> <span class="fu">loss</span>(<span class="at">q =</span> VaR_post, <span class="at">qhat =</span> VaR_grid[i])</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-lossfn" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lossfn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesics_files/figure-html/fig-lossfn-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lossfn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.9: Posterior density (left) and losses functions for the 0.99 value-at-risk for the Danish fire insurance data. The vertical lines denote point estimates of the quantiles that minimize the loss functions.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>To communicate uncertainty, we may resort to credible regions and intervals.</p>
<div id="def-credible-region" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3</strong></span> A <span class="math inline">\((1-\alpha)\)</span> <strong>credible region</strong> (or credible interval in the univariate setting) is a set <span class="math inline">\(\mathcal{S}_\alpha\)</span> such that, with probability level <span class="math inline">\(\alpha,\)</span> <span class="math display">\[\begin{align*}
\Pr(\boldsymbol{\theta} \in \mathcal{S}_\alpha \mid \boldsymbol{Y}=\boldsymbol{y}) = 1-\alpha
\end{align*}\]</span></p>
</div>
<p>These intervals are not unique, as are confidence sets. In the univariate setting, the central or equitailed interval are the most popular, and easily obtained by considering the <span class="math inline">\(\alpha/2, 1-\alpha/2\)</span> quantiles. These are easily obtained from samples by simply taking empirical quantiles. An alternative, highest posterior density credible sets, which may be a set of disjoint intervals obtained by considering the parts of the posterior with the highest density, may be more informative. The top panel <a href="#fig-credible-intervals" class="quarto-xref">Figure&nbsp;<span>2.10</span></a> shows two extreme cases in which these intervals differ: the distinction for a bimodal mixture distribution, and a even more striking difference for 50% credible intervals for a symmetric beta distribution whose mass lie near the endpoints of the distribution, leading to no overlap between the two intervals.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2023</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>postsamp <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="at">n =</span> <span class="dv">1000</span>, <span class="at">shape1 =</span> <span class="fl">0.5</span>, <span class="at">shape2 =</span> <span class="fl">0.2</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.11</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute equitailed interval bounds</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(postsamp, <span class="at">probs =</span> <span class="fu">c</span>(alpha<span class="sc">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     5.5%     94.5% 
0.0246807 0.9999980 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qbeta</span>(<span class="at">p =</span> <span class="fu">c</span>(alpha<span class="sc">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>), <span class="at">shape1 =</span> <span class="fl">0.5</span>, <span class="at">shape2 =</span> <span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.02925205 0.99999844</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Highest posterior density intervals</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>hdiD <span class="ot">&lt;-</span> HDInterval<span class="sc">::</span><span class="fu">hdi</span>(<span class="fu">density</span>(postsamp), <span class="at">credMass =</span> <span class="dv">1</span><span class="sc">-</span>alpha, <span class="at">allowSplit =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The equitailed intervals for a known posterior can be obtained directly from the quantile function or via Monte Carlo simply by querying sample quantiles. The HPD region is more complicated to obtain and requires dedicated software, which in the above case may fail to account for the support of the posterior!</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-credible-intervals" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-credible-intervals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bayesics_files/figure-html/fig-credible-intervals-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-credible-intervals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.10: Density plots with 89% (top) and 50% (bottom) equitailed or central credible (left) and highest posterior density (right) regions for two data sets, highlighted in grey. The horizontal lign gives the posterior density value determining the cutoff for the HDP region.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<strong>Summary</strong>:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Bayesians treat both parameter and observations as random, the former due to uncertainty about the true value. Inference is performed conditional on observed data, and summarized in the posterior.</li>
<li>Bayesians specify both a prior for the parameter, and a likelihood specifying the data generating mechanism. It is thus an extension of likelihood-based inference.</li>
<li>Information from the prior is updated in light of new data, which is encoded by the likelihood. Sequential updating leads.</li>
<li>Under weak conditions on the prior, large-sample behaviour of Bayesian and frequentist.</li>
<li>Bayesian inference is complicated by the fact that there is more often than not no closed-form expression for the posterior distribution. Evaluation of the normalizing constant, the so-called marginal likelihood, is challenging, especially in high dimensional settings.</li>
<li>Rather than hypothesis testing, Bayesian methods rely on the posterior distribution of parameters, or on Bayes factor for model comparisons.</li>
<li>The posterior predictive distribution, used for model assessment and prediction, and it has a higher variance than the data generating distribution from the likelihood, due to parameter uncertainty.</li>
<li>Bayesians typically will have approximations to the posterior distribution, or samples drawn from it.</li>
<li>Loss functions can be used to summarize a posterior distribution into a numerical summary of interest, which may vary depending on the objective.</li>
<li>Uncertainty is reflected by credible sets or credible intervals, which encode the posterior probability that the true value <span class="math inline">\(\boldsymbol{\theta}\)</span> belongs to the set.</li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Duke.Amir:2023" class="csl-entry" role="listitem">
Duke, Kristen E., and On Amir. 2023. <span>“The Importance of Selling Formats: When Integrating Purchase and Quantity Decisions Increases Sales.”</span> <em>Marketing Science</em> 42 (1): 87–109. <a href="https://doi.org/10.1287/mksc.2022.1364">https://doi.org/10.1287/mksc.2022.1364</a>.
</div>
<div id="ref-deFinetti:1974" class="csl-entry" role="listitem">
Finetti, Bruno de. 1974. <em>Theory of Probability: A Critical Introductory Treatment</em>. Vol. 1. New York: Wiley.
</div>
<div id="ref-Gelfand.Smith:1990" class="csl-entry" role="listitem">
Gelfand, Alan E., and Adrian F. M. Smith. 1990. <span>“Sampling-Based Approaches to Calculating Marginal Densities.”</span> <em>Journal of the American Statistical Association</em> 85 (410): 398–409. <a href="https://doi.org/10.1080/01621459.1990.10476213">https://doi.org/10.1080/01621459.1990.10476213</a>.
</div>
<div id="ref-Geman.Geman:1984" class="csl-entry" role="listitem">
Geman, Stuart, and Donald Geman. 1984. <span>“Stochastic Relaxation, <span>G</span>ibbs Distributions, and the <span>B</span>ayesian Restoration of Images.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> Pami-6 (6): 721–41. <a href="https://doi.org/10.1109/tpami.1984.4767596">https://doi.org/10.1109/tpami.1984.4767596</a>.
</div>
<div id="ref-McNeil.Frey.Embrechts:2005" class="csl-entry" role="listitem">
McNeil, A. J., R. Frey, and P. Embrechts. 2005. <em>Quantitative Risk Management: Concepts, Techniques, and Tools</em>. 1st ed. Princeton, NJ: Princeton University Press.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lbelzile\.github\.io\/MATH80601A\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./introduction.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./priors.html" class="pagination-link" aria-label="Priors">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Priors</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2024-2025, Léo Belzile</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/lbelzile/MATH80601A/edit/main/bayesics.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>